{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DV7fmwYwVX_",
        "outputId": "15506f7a-adcb-45c0-a93c-9396df2c80aa"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2nZg8-K24xf",
        "outputId": "6ab6ad20-9212-4192-d90a-a600f99ba46d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "C5LvCIPR2eVx"
      },
      "outputs": [],
      "source": [
        "import pickle as pkl\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import re\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import scipy.sparse\n",
        "# from gensim.models import KeyedVectors\n",
        "# from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import os\n",
        "\n",
        "# import stanfordnlp\n",
        "\n",
        "# Download the Arabic models for the neural pipeline\n",
        "# stanfordnlp.download('ar', force=True)\n",
        "# Build a neural pipeline using the Arabic models\n",
        "# nlp = stanfordnlp.Pipeline(lang='ar')\n",
        "\n",
        "# def split_arabic_sentences_with_stanfordnlp(corpus_text):\n",
        "#     # Process the text\n",
        "#     doc = nlp(corpus_text)\n",
        "\n",
        "#     # Extract sentences from the doc\n",
        "#     sentences = [sentence.text for sentence in doc.sentences]\n",
        "\n",
        "#     return sentences\n",
        "# file_path = '../SG_300_3_400/w2v_SG_300_3_400_10.model'\n",
        "# word_embed_model = Word2Vec.load(file_path)\n",
        "\n",
        "max_len=300\n",
        "\n",
        "with open('files/arabic_letters.pickle', 'rb') as file:\n",
        "            ARABIC_LETTERS_LIST = pkl.load(file)\n",
        "with open('files/diacritics.pickle', 'rb') as file:\n",
        "            DIACRITICS_LIST = pkl.load(file)\n",
        "with open('files/diacritic2id.pickle', 'rb') as file:\n",
        "            DIACRITICS_LIST2ID = pkl.load(file)\n",
        "\n",
        "arabic_letters=[]\n",
        "for letter in ARABIC_LETTERS_LIST:\n",
        "    arabic_letters.append(letter[0])\n",
        "arabic_letters.append(\" \")\n",
        "\n",
        "\n",
        "dicritics=[]\n",
        "for letter in DIACRITICS_LIST:\n",
        "    dicritics.append(letter[0])\n",
        "classes={k:i for i,k in enumerate(DIACRITICS_LIST2ID)}\n",
        "classes[\" \"]=15\n",
        "\n",
        "\n",
        "\n",
        "inverted_classes = {v: k for k, v in classes.items()}\n",
        "\n",
        "##################### to delete #####################\n",
        "def read_text(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        return file.read()\n",
        "\n",
        "def write_to_file_second(dirctory,file_path, text):\n",
        "    if not os.path.exists(dirctory):\n",
        "        os.makedirs(dirctory)\n",
        "\n",
        "    with open(dirctory+\"/\"+file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        lines=text\n",
        "        for l in lines:\n",
        "            file.write(l)\n",
        "        file.write('\\n')\n",
        "def write_to_file_labels(dirctory,file_path, text):\n",
        "    if not os.path.exists(dirctory):\n",
        "        os.makedirs(dirctory)\n",
        "\n",
        "    with open(dirctory+\"/\"+file_path, \"a\", encoding=\"utf-8\") as file:\n",
        "        lines=text\n",
        "        for l in lines:\n",
        "            file.write(inverted_classes[l])\n",
        "        file.write('\\n')\n",
        "\n",
        "\n",
        "def write_to_file_string(dirctory,file_path, text):\n",
        "    if not os.path.exists(dirctory):\n",
        "        os.makedirs(dirctory)\n",
        "\n",
        "    with open(dirctory+\"/\"+file_path, \"a\", encoding=\"utf-8\") as file:\n",
        "        lines=text\n",
        "        file.write(lines)\n",
        "        file.write('\\n')\n",
        "        file.write('\\n')\n",
        "#####################################################\n",
        "\n",
        "def preprocess(text):\n",
        "        # Remove URLs\n",
        "        text = re.sub(r\"http[s|S]\\S+\", \"\", text,flags=re.MULTILINE)\n",
        "        text = re.sub(r\"www\\S+\", \"\", text,flags=re.MULTILINE)\n",
        "        # Remove English letters\n",
        "        text = re.sub(r\"[A-Za-z]+\", \"\", text,flags=re.MULTILINE)\n",
        "        # Remove Kashida Arabic character\n",
        "        text = re.sub(r\"\\u0640\", \"\", text,flags=re.MULTILINE)\n",
        "        # Add space before and after the numbers\n",
        "        text = re.sub(r\"(\\d+)\", r\" \\1 \", text,flags=re.MULTILINE)\n",
        "        # removes SHIFT+J Arabic character\n",
        "        text = re.sub(r\"\\u0691\", \"\", text,flags=re.MULTILINE)\n",
        "        # remove english numbers\n",
        "        text = re.sub(r\"[0-9]+\", \"\", text,flags=re.MULTILINE)\n",
        "        # remove arabic numbers\n",
        "        text = re.sub(r\"[٠-٩]+\", \"\", text,flags=re.MULTILINE)\n",
        "         # remove brackets\n",
        "        # text = re.sub(r\"\\[.*?\\]\", \"\", text)\n",
        "        # text = re.sub(r\"\\(.*?\\)\", \"\", text)\n",
        "        return text\n",
        "\n",
        "\n",
        "# text=read_text('Dataset/train.txt')\n",
        "# text=preprocess(text)\n",
        "# print(len(text))\n",
        "# # delete except arabic letters and dicritics and punctuation\n",
        "# # remove multiple spaces\n",
        "\n",
        "# write_to_file_string(\"test\",\"data.txt\",text)\n",
        "\n",
        "def split_text(text):\n",
        "    text=text.split('.')\n",
        "    # split text to sentences on all arabic sparatators\n",
        "\n",
        "    data=[]\n",
        "    for t in text:\n",
        "        if(len(t)==0): continue\n",
        "        if(len(t)<max_len):\n",
        "            while(len(t)<max_len):\n",
        "                 t+=\" \"\n",
        "            data.append(t)\n",
        "        if(len(t)>max_len):\n",
        "            data.append(t[:max_len])\n",
        "            supdata=t[max_len:]\n",
        "            while(len(supdata)>max_len):\n",
        "                data.append(supdata[:max_len])\n",
        "                supdata=supdata[max_len:]\n",
        "            if(len(supdata)<max_len):\n",
        "                while(len(supdata)<max_len):\n",
        "                    supdata+=\" \"\n",
        "                data.append(supdata)\n",
        "    return data\n",
        "\n",
        "\n",
        "# [\".\", \"،\", \":\", \"؛\", \"-\", \"؟\"]\n",
        "VALID_ARABIC = dicritics + arabic_letters + ['.']\n",
        "\n",
        "import re\n",
        "\n",
        "_whitespace_re = re.compile(r\"\\s+\")\n",
        "\n",
        "def remove_spaces(text):\n",
        "    text = re.sub(_whitespace_re, \" \", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def preprocessing(text):\n",
        "    text = filter(lambda char: char in VALID_ARABIC, text)\n",
        "    text = remove_spaces(''.join(list(text)))\n",
        "    text=text.strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "def get_data_labels(text):\n",
        "    data=\"\"\n",
        "    labels=[]\n",
        "    for i in range(len(text)):\n",
        "        if(text[i] in arabic_letters):\n",
        "            data+=text[i]\n",
        "            if(text[i]==\" \"):\n",
        "                labels.append(15)\n",
        "            elif(i+1<len(text) and text[i+1] in dicritics):\n",
        "                if(i+2<len(text) and text[i+2] in dicritics and (classes.get(text[i+1])==7 or classes.get(text[i+2])==7)):\n",
        "                    if(text[i+1]==7 ):\n",
        "                        labels.append(classes[text[i+2]+text[i+1]])\n",
        "                        i+=2\n",
        "                    else:\n",
        "                        labels.append(classes[text[i+1]+text[i+2]])\n",
        "                        i+=2\n",
        "                else:\n",
        "                    labels.append(classes[text[i+1]])\n",
        "                    i+=1\n",
        "            else:\n",
        "                labels.append(14)\n",
        "    return data,labels\n",
        "\n",
        "def encoding(text):\n",
        "    idx=arabic_letters.index(text)\n",
        "    encode=np.zeros(len(arabic_letters))\n",
        "    encode[idx]=1\n",
        "    return torch.tensor(encode,dtype=torch.float32).to(device)\n",
        "\n",
        "\n",
        "def get_dataloader(encoded_data, encoding_labels,batch_size=1):\n",
        "    # Create TensorDataset\n",
        "    dataset = TensorDataset(encoded_data.to(device), encoding_labels.to(device))\n",
        "    # Create DataLoader\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "def get_validation():\n",
        "    text=read_text('Dataset/val.txt')\n",
        "    text=preprocess(text)\n",
        "    # size=int(0.02*len(text))\n",
        "    # text=text[:size]\n",
        "    # text=split_text(text)\n",
        "    # write_to_file_second(\"test\",\"data.txt\",text)\n",
        "    # text=split_text(text)\n",
        "    data=[]\n",
        "    labels=[]\n",
        "    for i in range (len(text)):\n",
        "        if(text[i] in arabic_letters):\n",
        "                data+=(text[i])\n",
        "                if(i+1<len(text) and text[i+1] in dicritics):\n",
        "                    if(i+2<len(text) and classes[text[i+1]]==4 and text[i+2] in dicritics):\n",
        "                        labels.append(classes[text[i+1]+text[i+2]])\n",
        "                        i+=2\n",
        "                    else:\n",
        "                        labels.append(classes[text[i+1]])\n",
        "                        i+=1\n",
        "                else:\n",
        "                    labels.append(15)\n",
        "        # else:\n",
        "        #     data.append(text[i])\n",
        "        #     labels.append(15)\n",
        "    encoded_data = torch.empty(0, len(arabic_letters),dtype=torch.float32)\n",
        "\n",
        "    for letter in data:\n",
        "        if letter in arabic_letters:\n",
        "            x = encoding(letter).unsqueeze(0)\n",
        "        else:\n",
        "            x=np.zeros((1,len(arabic_letters)))\n",
        "            x=torch.tensor(x,dtype=torch.float32)\n",
        "        encoded_data = torch.cat((encoded_data, x), 0)\n",
        "    labels=torch.tensor(labels,dtype=torch.long)\n",
        "    # print(encoded_data.shape)\n",
        "    # print(labels.shape)\n",
        "    dataloader=get_dataloader(encoded_data,labels)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "def get_test(path):\n",
        "    text=read_text(path)\n",
        "    text=preprocess(text)\n",
        "    # size=int(0.01*len(text))\n",
        "    # text=text[:size]\n",
        "    # print(len(text))\n",
        "    text = preprocessing(text)\n",
        "    text=\"\".join(text)\n",
        "    text=text.split('.')\n",
        "    data=[]\n",
        "    labels=[]\n",
        "    for t in text:\n",
        "        t=t.strip()\n",
        "        d=\"\"\n",
        "        l=[]\n",
        "        if len(t)>max_len:\n",
        "            continue\n",
        "        else:\n",
        "            d,l=get_data_labels(t)\n",
        "            if(len(d)==0): continue\n",
        "            elif(len(d)<max_len):\n",
        "                data.append(d)\n",
        "                labels.append(l)\n",
        "                continue\n",
        "            elif(len(d)>max_len):\n",
        "                data.append(d[:max_len])\n",
        "                labels.append(l[:max_len])\n",
        "                supdata=d[max_len:]\n",
        "                suplabels=l[max_len:]\n",
        "                while(len(supdata)>max_len):\n",
        "                    data.append(supdata[:max_len])\n",
        "                    labels.append(suplabels[:max_len])\n",
        "                    supdata=supdata[max_len:]\n",
        "                    suplabels=suplabels[max_len:]\n",
        "                if(len(supdata)<max_len):\n",
        "                    data.append(supdata)\n",
        "                    labels.append(suplabels)\n",
        "    return data,labels\n",
        "\n",
        "def get_data(path):\n",
        "    text=read_text(path)\n",
        "    text=preprocess(text)\n",
        "    size=int(0.01*len(text))\n",
        "    text=text[:size]\n",
        "    text = preprocessing(text)\n",
        "    text=\"\".join(text)\n",
        "    # write_to_file_string(\"test\",\"data.txt\",text)\n",
        "    # text=split_text(text)\n",
        "    text=text.split('.')\n",
        "    print(len(text))\n",
        "\n",
        "\n",
        "    # write_to_file_second(\"test\",\"data.txt\",text)\n",
        "    # # get max length of sentence in text\n",
        "    # maxdata=text.split('\\n')\n",
        "\n",
        "    # max_len=0\n",
        "    # for t in maxdata:\n",
        "    #     if(len(t)>max_len):\n",
        "    #         max_len=len(t)\n",
        "    # print(max_len)\n",
        "\n",
        "    # split text to sentences on all arabic sparatators\n",
        "    # text = split_arabic_sentences_with_stanfordnlp(text)\n",
        "\n",
        "    # Filter out empty strings or whitespace-only sentences\n",
        "    # text = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "    data=[]\n",
        "    labels=[]\n",
        "    for t in text:\n",
        "        d=\"\"\n",
        "        l=[]\n",
        "        d,l=get_data_labels(t)\n",
        "        if(len(d)==0): continue\n",
        "        if(len(d)<max_len):\n",
        "            while(len(d)<max_len):\n",
        "                 d+=\" \"\n",
        "                 l.append(15)\n",
        "            data.append(d)\n",
        "            labels.append(l)\n",
        "            continue\n",
        "        if(len(d)>max_len):\n",
        "            data.append(d[:max_len])\n",
        "            labels.append(l[:max_len])\n",
        "            supdata=d[max_len:]\n",
        "            suplabels=l[max_len:]\n",
        "            while(len(supdata)>max_len):\n",
        "                data.append(supdata[:max_len])\n",
        "                labels.append(suplabels[:max_len])\n",
        "                supdata=supdata[max_len:]\n",
        "                suplabels=suplabels[max_len:]\n",
        "            if(len(supdata)<max_len):\n",
        "                while(len(supdata)<max_len):\n",
        "                    supdata+=\" \"\n",
        "                    suplabels.append(15)\n",
        "                data.append(supdata)\n",
        "                labels.append(suplabels)\n",
        "        # data.append(d)\n",
        "        # labels.append(l)\n",
        "\n",
        "    # for d in data:\n",
        "    #     write_to_file_string(\"test\",\"data.txt\",d)\n",
        "    return data,labels\n",
        "\n",
        "def get_features(data,labels):\n",
        "    encoded_data = torch.empty(0, max_len, len(arabic_letters),dtype=torch.float32).to(device)\n",
        "    for d in data:\n",
        "        enc = torch.empty(0, len(arabic_letters),dtype=torch.float32).to(device)\n",
        "        for letter in d:\n",
        "            x = encoding(letter).unsqueeze(0).to(device)\n",
        "            enc = torch.cat((enc, x), 0)\n",
        "        encoded_data = torch.cat((encoded_data, enc.unsqueeze(0)), 0)\n",
        "    # print(encoded_data.shape)\n",
        "    encoding_labels=torch.tensor(labels,dtype=torch.long).to(device)\n",
        "    # print(encoding_labels.shape)\n",
        "    return encoded_data,encoding_labels\n",
        "\n",
        "def get_word2vec_features(data, labels, model):\n",
        "    max_seq_length = 300\n",
        "    encoded_data = torch.empty(0, max_len, max_seq_length, dtype=torch.float32)\n",
        "\n",
        "    for sentence in data:\n",
        "        encoded_sentence = torch.empty(0, max_seq_length, dtype=torch.float32)\n",
        "        for letter in sentence:\n",
        "            if letter in model.wv:\n",
        "                # Convert NumPy array to PyTorch tensor\n",
        "                 embedding  = torch.tensor(model.wv[letter], dtype=torch.float32).unsqueeze(0)\n",
        "            else:\n",
        "                # If the word is not in the model's vocabulary, fill with zeros\n",
        "                embedding  = torch.zeros((1, max_seq_length), dtype=torch.float32)\n",
        "\n",
        "            encoded_sentence = torch.cat((encoded_sentence, embedding), 0)\n",
        "\n",
        "        encoded_data = torch.cat((encoded_data, encoded_sentence.unsqueeze(0)), 0)\n",
        "\n",
        "    encoding_labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return encoded_data, encoding_labels\n",
        "\n",
        "\n",
        "def get_tf_idf_features(data, labels):\n",
        "    # Create a one-hot encoding tensor with the same dimensions as Word2Vec\n",
        "    encoded_data = torch.empty(0, max_len, len(arabic_letters), dtype=torch.float32)\n",
        "\n",
        "    # Create a mapping of words to indices for one-hot encoding\n",
        "    word_to_idx = {word: idx for idx, word in enumerate(arabic_letters)}\n",
        "\n",
        "    for sentence in data:\n",
        "        encoded_sentence = torch.empty(0, len(arabic_letters), dtype=torch.float32)\n",
        "\n",
        "        for letter in sentence:\n",
        "            # Use one-hot encoding for each letter\n",
        "            if letter in word_to_idx:\n",
        "                idx = word_to_idx[letter]\n",
        "                one_hot = torch.zeros(len(arabic_letters), dtype=torch.float32)\n",
        "                one_hot[idx] = 1\n",
        "                one_hot = one_hot.unsqueeze(0)\n",
        "            else:\n",
        "                # If the word is not in the vocabulary, fill with zeros\n",
        "                one_hot = torch.zeros(len(arabic_letters), dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "            encoded_sentence = torch.cat((encoded_sentence, one_hot), 0)\n",
        "\n",
        "        encoded_data = torch.cat((encoded_data, encoded_sentence.unsqueeze(0)), 0)\n",
        "\n",
        "    encoding_labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return encoded_data, encoding_labels\n",
        "\n",
        "\n",
        "\n",
        "def get_bow_features(data, labels, max_seq_length=400):\n",
        "    # Create a CountVectorizer\n",
        "    vectorizer = CountVectorizer(analyzer='char', vocabulary=arabic_letters)\n",
        "\n",
        "    # Fit and transform the data\n",
        "    encoded_data = vectorizer.transform(data)\n",
        "\n",
        "    # Convert to a PyTorch sparse tensor with the specified max_seq_length\n",
        "    encoded_data = torch.sparse.FloatTensor(\n",
        "        torch.tensor(encoded_data.nonzero()).t(),\n",
        "        torch.ones(encoded_data.nnz),\n",
        "        (len(data), max_seq_length, len(arabic_letters))\n",
        "    )\n",
        "\n",
        "    encoding_labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return encoded_data, encoding_labels\n",
        "\n",
        "class DataSet():\n",
        "\n",
        "    def __init__(self,path,batch_size=1,test=False) :\n",
        "        print(\"Loading data...\")\n",
        "        if(test):\n",
        "            data1,labels1=get_test(path)\n",
        "        else:\n",
        "            data1,labels1=get_data(path)\n",
        "        # now labels is list of list [[1,2,3,4,5,15,15,0],[1,2,3,4,5,15,15,0]]\n",
        "        # data is list of string ['احمد','محمد']\n",
        "        print(\"Extracting features...\")\n",
        "        data,labels=get_features(data1,labels1)\n",
        "        # data,labels=get_word2vec_features(data1,labels1,word_embed_model)\n",
        "        print(data.shape)\n",
        "        # now the data and labels are tensor\n",
        "        # data is tensor of shape (number of sentences,max_len,37)\n",
        "        # labels is tensor of shape (number of sentences,max_len)\n",
        "        print(\"Creating dataloader...\")\n",
        "\n",
        "        dataloader=get_dataloader(data,labels,batch_size)\n",
        "        self.x=data1\n",
        "        self.y=labels1\n",
        "        self.dataloader=dataloader\n",
        "        print(\"Done data creation !\")\n",
        "\n",
        "    def __len__(self):\n",
        "         return len(self.y)\n",
        "    def item(self,idx):\n",
        "         return self.x[idx],self.y[idx]\n",
        "    def getdata(self):\n",
        "        return self.dataloader\n",
        "    def getx(self):\n",
        "        return self.x\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "qhy617nk2VPI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch import optim\n",
        "from numpy import vstack\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "def write_to_file_string(dirctory,file_path, text):\n",
        "    if not os.path.exists(dirctory):\n",
        "        os.makedirs(dirctory)\n",
        "\n",
        "    with open(dirctory+\"/\"+file_path, \"a\", encoding=\"utf-8\") as file:\n",
        "        lines=text\n",
        "        file.write(lines)\n",
        "        file.write('\\n')\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, inp_vocab_size: int, hidden_dim: int = 256, seq_len: int = 600, num_classes: int = 16):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(inp_vocab_size, hidden_dim,num_layers=3, batch_first=True, bidirectional=True)\n",
        "        # self.lstm = nn.LSTM(inp_vocab_size, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # Output layer for 0 to 16 integers\n",
        "\n",
        "    def forward(self, input_sequence: torch.Tensor):\n",
        "        output, _ = self.lstm(input_sequence)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "class GRU(nn.Module):\n",
        "    def __init__(self, inp_vocab_size: int, hidden_dim: int = 256, seq_len: int = 600, num_classes: int = 16):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(inp_vocab_size, hidden_dim,num_layers=3, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # Output layer for 0 to 16 integers\n",
        "\n",
        "    def forward(self, input_sequence: torch.Tensor):\n",
        "        output, _ = self.gru(input_sequence)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "def train(train_dl, model):\n",
        "    # define the optimization\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "    model.to(device)\n",
        "    # enumerate epochs\n",
        "    for epoch in range(1):\n",
        "        for i, (inputs, targets) in enumerate(train_dl):\n",
        "            # convert the input and target to tensor\n",
        "            # clear the gradients\n",
        "            optimizer.zero_grad()\n",
        "            # compute the model output\n",
        "            yhat = model(inputs.to(device))\n",
        "            yhat = yhat.view(-1, yhat.size(2))  # Reshape model output to [batch_size * sequence_length, num_classes]\n",
        "            targets = targets.view(-1)  # Reshape targets to [batch_size * sequence_length]\n",
        "            # calculate loss\n",
        "            loss = criterion(yhat, targets.to(device))\n",
        "            loss.backward()\n",
        "            # update model weights\n",
        "            optimizer.step()\n",
        "            print(f'epoch {epoch} batch {i} loss {loss.item()}')\n",
        "            # break\n",
        "    # save model to file after training\n",
        "    torch.save(model.state_dict(), 'model.pth')\n",
        "\n",
        "\n",
        "def calculate_DER(actual_labels, predicted_labels):\n",
        "    # Convert lists to PyTorch tensors if they are not already\n",
        "    if not isinstance(actual_labels, torch.Tensor):\n",
        "        actual_labels = torch.tensor(actual_labels)\n",
        "    if not isinstance(predicted_labels, torch.Tensor):\n",
        "        predicted_labels = torch.tensor(predicted_labels)\n",
        "\n",
        "    # Check if the lengths of both label sequences match\n",
        "    if len(actual_labels) != len(predicted_labels):\n",
        "        raise ValueError(\"Lengths of actual and predicted labels should match.\")\n",
        "\n",
        "    total_errors = torch.sum(actual_labels != predicted_labels)\n",
        "    total_frames = len(actual_labels)\n",
        "\n",
        "    # DER calculation\n",
        "    DER = (1-(total_errors / total_frames)) * 100.0\n",
        "    return DER.item()  # Convert PyTorch scalar to Python float\n",
        "\n",
        "\n",
        "def evaluate_model(test_dl, model):\n",
        "    predictions, actuals = [], []\n",
        "    model.to(device)\n",
        "    for i, (inputs, targets) in enumerate(test_dl):\n",
        "        yhat = model(inputs.to(device))\n",
        "        yhat = yhat.detach().cpu().numpy()\n",
        "        # reshape the outputs to [batch_size * sequence_length, num_classes]\n",
        "        yhat = yhat.reshape(-1, yhat.shape[-1])\n",
        "        # get predicted classes\n",
        "        predicted_classes = np.argmax(yhat, axis=1)\n",
        "        # convert targets to numpy array and reshape\n",
        "        targets = targets.cpu().numpy().reshape(-1)\n",
        "        # store predictions and actuals\n",
        "        predictions.extend(predicted_classes.tolist())\n",
        "        actuals.extend(targets.tolist())\n",
        "        # break\n",
        "    # calculate accuracy\n",
        "    acc = calculate_DER(np.array(actuals), np.array(predictions))\n",
        "\n",
        "    return acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqgXGly72RLu",
        "outputId": "7d755813-cea3-4a6b-9c5e-a075df6c8510"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "378\n",
            "Extracting features...\n",
            "torch.Size([565, 300, 37])\n",
            "Creating dataloader...\n",
            "Done data creation !\n",
            "-------------------start training-------------------\n",
            "epoch 0 batch 0 loss 2.7780938148498535\n",
            "epoch 0 batch 1 loss 2.1769192218780518\n",
            "epoch 0 batch 2 loss 2.3285586833953857\n",
            "epoch 0 batch 3 loss 2.0479254722595215\n",
            "epoch 0 batch 4 loss 1.9563891887664795\n",
            "epoch 0 batch 5 loss 2.2386105060577393\n",
            "epoch 0 batch 6 loss 1.0062251091003418\n",
            "epoch 0 batch 7 loss 1.2962751388549805\n",
            "epoch 0 batch 8 loss 3.2190186977386475\n",
            "epoch 0 batch 9 loss 2.96653413772583\n",
            "epoch 0 batch 10 loss 2.5203816890716553\n",
            "epoch 0 batch 11 loss 1.036270260810852\n",
            "epoch 0 batch 12 loss 1.815755009651184\n",
            "epoch 0 batch 13 loss 1.9802144765853882\n",
            "epoch 0 batch 14 loss 1.4450982809066772\n",
            "epoch 0 batch 15 loss 1.8555344343185425\n",
            "epoch 0 batch 16 loss 1.5407874584197998\n",
            "epoch 0 batch 17 loss 1.9989835023880005\n",
            "epoch 0 batch 18 loss 1.4290448427200317\n",
            "epoch 0 batch 19 loss 1.4406636953353882\n",
            "epoch 0 batch 20 loss 1.9905816316604614\n",
            "epoch 0 batch 21 loss 1.6003962755203247\n",
            "epoch 0 batch 22 loss 0.6417617201805115\n",
            "epoch 0 batch 23 loss 1.1478909254074097\n",
            "epoch 0 batch 24 loss 2.5380096435546875\n",
            "epoch 0 batch 25 loss 1.4167304039001465\n",
            "epoch 0 batch 26 loss 2.0790302753448486\n",
            "epoch 0 batch 27 loss 0.6921759843826294\n",
            "epoch 0 batch 28 loss 1.0442603826522827\n",
            "epoch 0 batch 29 loss 1.5221866369247437\n",
            "epoch 0 batch 30 loss 2.3547463417053223\n",
            "epoch 0 batch 31 loss 2.225008249282837\n",
            "epoch 0 batch 32 loss 2.0844714641571045\n",
            "epoch 0 batch 33 loss 1.548986792564392\n",
            "epoch 0 batch 34 loss 1.9443776607513428\n",
            "epoch 0 batch 35 loss 1.775262713432312\n",
            "epoch 0 batch 36 loss 1.6545010805130005\n",
            "epoch 0 batch 37 loss 1.955485463142395\n",
            "epoch 0 batch 38 loss 1.8948168754577637\n",
            "epoch 0 batch 39 loss 1.278406023979187\n",
            "epoch 0 batch 40 loss 1.6326096057891846\n",
            "epoch 0 batch 41 loss 2.1377789974212646\n",
            "epoch 0 batch 42 loss 0.7399880886077881\n",
            "epoch 0 batch 43 loss 0.5296925902366638\n",
            "epoch 0 batch 44 loss 2.687117338180542\n",
            "epoch 0 batch 45 loss 2.6709296703338623\n",
            "epoch 0 batch 46 loss 1.6208961009979248\n",
            "epoch 0 batch 47 loss 2.278611421585083\n",
            "epoch 0 batch 48 loss 0.8185728192329407\n",
            "epoch 0 batch 49 loss 2.1813318729400635\n",
            "epoch 0 batch 50 loss 2.1403887271881104\n",
            "epoch 0 batch 51 loss 1.6148591041564941\n",
            "epoch 0 batch 52 loss 2.0214171409606934\n",
            "epoch 0 batch 53 loss 1.7586264610290527\n",
            "epoch 0 batch 54 loss 1.7043477296829224\n",
            "epoch 0 batch 55 loss 2.0069613456726074\n",
            "epoch 0 batch 56 loss 1.855077862739563\n",
            "epoch 0 batch 57 loss 2.3199949264526367\n",
            "epoch 0 batch 58 loss 2.024812698364258\n",
            "epoch 0 batch 59 loss 1.7040612697601318\n",
            "epoch 0 batch 60 loss 1.6927260160446167\n",
            "epoch 0 batch 61 loss 1.9421271085739136\n",
            "epoch 0 batch 62 loss 1.2492213249206543\n",
            "epoch 0 batch 63 loss 2.024169683456421\n",
            "epoch 0 batch 64 loss 1.2580240964889526\n",
            "epoch 0 batch 65 loss 1.939198613166809\n",
            "epoch 0 batch 66 loss 0.6578350067138672\n",
            "epoch 0 batch 67 loss 2.4465315341949463\n",
            "epoch 0 batch 68 loss 2.495814323425293\n",
            "epoch 0 batch 69 loss 2.278752088546753\n",
            "epoch 0 batch 70 loss 1.110453486442566\n",
            "epoch 0 batch 71 loss 1.5506746768951416\n",
            "epoch 0 batch 72 loss 2.0815718173980713\n",
            "epoch 0 batch 73 loss 1.9693621397018433\n",
            "epoch 0 batch 74 loss 1.71208918094635\n",
            "epoch 0 batch 75 loss 1.7634285688400269\n",
            "epoch 0 batch 76 loss 1.577268362045288\n",
            "epoch 0 batch 77 loss 1.9054332971572876\n",
            "epoch 0 batch 78 loss 1.4072957038879395\n",
            "epoch 0 batch 79 loss 2.0709705352783203\n",
            "epoch 0 batch 80 loss 2.2435874938964844\n",
            "epoch 0 batch 81 loss 2.23000431060791\n",
            "epoch 0 batch 82 loss 1.804591417312622\n",
            "epoch 0 batch 83 loss 2.098365306854248\n",
            "epoch 0 batch 84 loss 1.3716224431991577\n",
            "epoch 0 batch 85 loss 1.615787386894226\n",
            "epoch 0 batch 86 loss 1.1019693613052368\n",
            "epoch 0 batch 87 loss 1.1511666774749756\n",
            "epoch 0 batch 88 loss 0.8641959428787231\n",
            "epoch 0 batch 89 loss 2.471297264099121\n",
            "epoch 0 batch 90 loss 1.315023422241211\n",
            "epoch 0 batch 91 loss 2.630445957183838\n",
            "epoch 0 batch 92 loss 2.621368169784546\n",
            "epoch 0 batch 93 loss 0.9101553559303284\n",
            "epoch 0 batch 94 loss 0.6075703501701355\n",
            "epoch 0 batch 95 loss 1.3784266710281372\n",
            "epoch 0 batch 96 loss 1.4621020555496216\n",
            "epoch 0 batch 97 loss 2.1644482612609863\n",
            "epoch 0 batch 98 loss 1.324819564819336\n",
            "epoch 0 batch 99 loss 1.5231742858886719\n",
            "epoch 0 batch 100 loss 1.8430722951889038\n",
            "epoch 0 batch 101 loss 1.3295111656188965\n",
            "epoch 0 batch 102 loss 1.3977303504943848\n",
            "epoch 0 batch 103 loss 1.0912044048309326\n",
            "epoch 0 batch 104 loss 1.0440564155578613\n",
            "epoch 0 batch 105 loss 2.351961374282837\n",
            "epoch 0 batch 106 loss 0.7437934279441833\n",
            "epoch 0 batch 107 loss 2.530715227127075\n",
            "epoch 0 batch 108 loss 2.370728015899658\n",
            "epoch 0 batch 109 loss 0.7705091834068298\n",
            "epoch 0 batch 110 loss 1.1048762798309326\n",
            "epoch 0 batch 111 loss 0.7550853490829468\n",
            "epoch 0 batch 112 loss 1.6137877702713013\n",
            "epoch 0 batch 113 loss 2.3472936153411865\n",
            "epoch 0 batch 114 loss 1.1135766506195068\n",
            "epoch 0 batch 115 loss 2.031754970550537\n",
            "epoch 0 batch 116 loss 2.1008918285369873\n",
            "epoch 0 batch 117 loss 1.772264838218689\n",
            "epoch 0 batch 118 loss 1.511073112487793\n",
            "epoch 0 batch 119 loss 1.8885966539382935\n",
            "epoch 0 batch 120 loss 1.3566224575042725\n",
            "epoch 0 batch 121 loss 2.08562970161438\n",
            "epoch 0 batch 122 loss 0.980185329914093\n",
            "epoch 0 batch 123 loss 2.106843948364258\n",
            "epoch 0 batch 124 loss 1.2942938804626465\n",
            "epoch 0 batch 125 loss 0.603428840637207\n",
            "epoch 0 batch 126 loss 2.466097116470337\n",
            "epoch 0 batch 127 loss 2.553719997406006\n",
            "epoch 0 batch 128 loss 1.3959158658981323\n",
            "epoch 0 batch 129 loss 2.524726629257202\n",
            "epoch 0 batch 130 loss 0.6836411356925964\n",
            "epoch 0 batch 131 loss 1.3395953178405762\n",
            "epoch 0 batch 132 loss 0.8709468841552734\n",
            "epoch 0 batch 133 loss 1.114698886871338\n",
            "epoch 0 batch 134 loss 1.567150592803955\n",
            "epoch 0 batch 135 loss 1.7524882555007935\n",
            "epoch 0 batch 136 loss 1.4641718864440918\n",
            "epoch 0 batch 137 loss 2.330410957336426\n",
            "epoch 0 batch 138 loss 0.7489693760871887\n",
            "epoch 0 batch 139 loss 2.2802326679229736\n",
            "epoch 0 batch 140 loss 1.432132601737976\n",
            "epoch 0 batch 141 loss 2.1455118656158447\n",
            "epoch 0 batch 142 loss 1.8294881582260132\n",
            "epoch 0 batch 143 loss 1.3085232973098755\n",
            "epoch 0 batch 144 loss 1.629459023475647\n",
            "epoch 0 batch 145 loss 1.2402212619781494\n",
            "epoch 0 batch 146 loss 1.0506112575531006\n",
            "epoch 0 batch 147 loss 2.1656956672668457\n",
            "epoch 0 batch 148 loss 1.11471426486969\n",
            "epoch 0 batch 149 loss 2.3000779151916504\n",
            "epoch 0 batch 150 loss 2.3144495487213135\n",
            "epoch 0 batch 151 loss 1.0936049222946167\n",
            "epoch 0 batch 152 loss 0.8671018481254578\n",
            "epoch 0 batch 153 loss 1.374537467956543\n",
            "epoch 0 batch 154 loss 1.0729637145996094\n",
            "epoch 0 batch 155 loss 1.655024528503418\n",
            "epoch 0 batch 156 loss 1.3328535556793213\n",
            "epoch 0 batch 157 loss 1.0972559452056885\n",
            "epoch 0 batch 158 loss 2.0197157859802246\n",
            "epoch 0 batch 159 loss 0.7719517350196838\n",
            "epoch 0 batch 160 loss 2.115619421005249\n",
            "epoch 0 batch 161 loss 0.7096994519233704\n",
            "epoch 0 batch 162 loss 2.132288932800293\n",
            "epoch 0 batch 163 loss 2.144127368927002\n",
            "epoch 0 batch 164 loss 2.107473850250244\n",
            "epoch 0 batch 165 loss 1.6766564846038818\n",
            "epoch 0 batch 166 loss 1.9856979846954346\n",
            "epoch 0 batch 167 loss 1.986535906791687\n",
            "epoch 0 batch 168 loss 1.832564115524292\n",
            "epoch 0 batch 169 loss 1.901489496231079\n",
            "epoch 0 batch 170 loss 1.9689866304397583\n",
            "epoch 0 batch 171 loss 1.9440549612045288\n",
            "epoch 0 batch 172 loss 1.8487672805786133\n",
            "epoch 0 batch 173 loss 1.9721193313598633\n",
            "epoch 0 batch 174 loss 1.9607449769973755\n",
            "epoch 0 batch 175 loss 2.0601279735565186\n",
            "epoch 0 batch 176 loss 2.000995397567749\n",
            "epoch 0 batch 177 loss 1.540355920791626\n",
            "epoch 0 batch 178 loss 1.2161972522735596\n",
            "epoch 0 batch 179 loss 1.4485336542129517\n",
            "epoch 0 batch 180 loss 2.0988101959228516\n",
            "epoch 0 batch 181 loss 1.5157259702682495\n",
            "epoch 0 batch 182 loss 1.4414851665496826\n",
            "epoch 0 batch 183 loss 1.7822844982147217\n",
            "epoch 0 batch 184 loss 0.7323967218399048\n",
            "epoch 0 batch 185 loss 1.7473781108856201\n",
            "epoch 0 batch 186 loss 1.4902071952819824\n",
            "epoch 0 batch 187 loss 0.895556628704071\n",
            "epoch 0 batch 188 loss 1.4466511011123657\n",
            "epoch 0 batch 189 loss 0.48665618896484375\n",
            "epoch 0 batch 190 loss 2.489881753921509\n",
            "epoch 0 batch 191 loss 0.7094771265983582\n",
            "epoch 0 batch 192 loss 0.8422145247459412\n",
            "epoch 0 batch 193 loss 0.4503830373287201\n",
            "epoch 0 batch 194 loss 0.6624107360839844\n",
            "epoch 0 batch 195 loss 1.752122402191162\n",
            "epoch 0 batch 196 loss 1.7504554986953735\n",
            "epoch 0 batch 197 loss 1.961418867111206\n",
            "epoch 0 batch 198 loss 0.6897508502006531\n",
            "epoch 0 batch 199 loss 2.568899393081665\n",
            "epoch 0 batch 200 loss 2.3844408988952637\n",
            "epoch 0 batch 201 loss 2.109856128692627\n",
            "epoch 0 batch 202 loss 2.013674020767212\n",
            "epoch 0 batch 203 loss 1.6794058084487915\n",
            "epoch 0 batch 204 loss 1.923882246017456\n",
            "epoch 0 batch 205 loss 2.0094330310821533\n",
            "epoch 0 batch 206 loss 2.005488157272339\n",
            "epoch 0 batch 207 loss 1.9517574310302734\n",
            "epoch 0 batch 208 loss 1.6225672960281372\n",
            "epoch 0 batch 209 loss 1.8114176988601685\n",
            "epoch 0 batch 210 loss 1.1214128732681274\n",
            "epoch 0 batch 211 loss 1.0175083875656128\n",
            "epoch 0 batch 212 loss 1.0140094757080078\n",
            "epoch 0 batch 213 loss 2.8248789310455322\n",
            "epoch 0 batch 214 loss 2.859495162963867\n",
            "epoch 0 batch 215 loss 0.2883369028568268\n",
            "epoch 0 batch 216 loss 2.727769374847412\n",
            "epoch 0 batch 217 loss 1.0407556295394897\n",
            "epoch 0 batch 218 loss 1.3927074670791626\n",
            "epoch 0 batch 219 loss 2.506157398223877\n",
            "epoch 0 batch 220 loss 2.2629435062408447\n",
            "epoch 0 batch 221 loss 0.8832240700721741\n",
            "epoch 0 batch 222 loss 2.02537202835083\n",
            "epoch 0 batch 223 loss 1.995486855506897\n",
            "epoch 0 batch 224 loss 1.8140995502471924\n",
            "epoch 0 batch 225 loss 1.6573621034622192\n",
            "epoch 0 batch 226 loss 1.9564378261566162\n",
            "epoch 0 batch 227 loss 1.8345097303390503\n",
            "epoch 0 batch 228 loss 1.8599324226379395\n",
            "epoch 0 batch 229 loss 1.2996022701263428\n",
            "epoch 0 batch 230 loss 1.0962159633636475\n",
            "epoch 0 batch 231 loss 2.3994483947753906\n",
            "epoch 0 batch 232 loss 2.3391027450561523\n",
            "epoch 0 batch 233 loss 1.7850427627563477\n",
            "epoch 0 batch 234 loss 2.1872527599334717\n",
            "epoch 0 batch 235 loss 2.103984832763672\n",
            "epoch 0 batch 236 loss 2.146092414855957\n",
            "epoch 0 batch 237 loss 1.3766916990280151\n",
            "epoch 0 batch 238 loss 1.7969648838043213\n",
            "epoch 0 batch 239 loss 1.806732416152954\n",
            "epoch 0 batch 240 loss 1.3651134967803955\n",
            "epoch 0 batch 241 loss 1.2451916933059692\n",
            "epoch 0 batch 242 loss 1.2020912170410156\n",
            "epoch 0 batch 243 loss 0.7535437941551208\n",
            "epoch 0 batch 244 loss 0.8276757597923279\n",
            "epoch 0 batch 245 loss 2.699458360671997\n",
            "epoch 0 batch 246 loss 2.757901906967163\n",
            "epoch 0 batch 247 loss 0.4133669137954712\n",
            "epoch 0 batch 248 loss 1.7274631261825562\n",
            "epoch 0 batch 249 loss 1.254508137702942\n",
            "epoch 0 batch 250 loss 0.7667521238327026\n",
            "epoch 0 batch 251 loss 1.1200321912765503\n",
            "epoch 0 batch 252 loss 0.4266479015350342\n",
            "epoch 0 batch 253 loss 0.9360963702201843\n",
            "epoch 0 batch 254 loss 0.4194992184638977\n",
            "epoch 0 batch 255 loss 2.0861144065856934\n",
            "epoch 0 batch 256 loss 2.769217014312744\n",
            "epoch 0 batch 257 loss 2.5784614086151123\n",
            "epoch 0 batch 258 loss 1.7278180122375488\n",
            "epoch 0 batch 259 loss 0.7930587530136108\n",
            "epoch 0 batch 260 loss 2.1691951751708984\n",
            "epoch 0 batch 261 loss 2.0308868885040283\n",
            "epoch 0 batch 262 loss 2.014734983444214\n",
            "epoch 0 batch 263 loss 1.822041392326355\n",
            "epoch 0 batch 264 loss 1.8511995077133179\n",
            "epoch 0 batch 265 loss 1.8803006410598755\n",
            "epoch 0 batch 266 loss 1.811607003211975\n",
            "epoch 0 batch 267 loss 1.7692384719848633\n",
            "epoch 0 batch 268 loss 1.8553414344787598\n",
            "epoch 0 batch 269 loss 1.7673169374465942\n",
            "epoch 0 batch 270 loss 2.1044938564300537\n",
            "epoch 0 batch 271 loss 1.0582021474838257\n",
            "epoch 0 batch 272 loss 0.9681020379066467\n",
            "epoch 0 batch 273 loss 2.4429025650024414\n",
            "epoch 0 batch 274 loss 0.7027345299720764\n",
            "epoch 0 batch 275 loss 2.474524974822998\n",
            "epoch 0 batch 276 loss 0.8252865672111511\n",
            "epoch 0 batch 277 loss 2.5445175170898438\n",
            "epoch 0 batch 278 loss 0.6633171439170837\n",
            "epoch 0 batch 279 loss 0.9691259860992432\n",
            "epoch 0 batch 280 loss 2.2038636207580566\n",
            "epoch 0 batch 281 loss 2.6125755310058594\n",
            "epoch 0 batch 282 loss 2.1408393383026123\n",
            "epoch 0 batch 283 loss 1.0283808708190918\n",
            "epoch 0 batch 284 loss 1.2374625205993652\n",
            "epoch 0 batch 285 loss 1.6991212368011475\n",
            "epoch 0 batch 286 loss 1.3216700553894043\n",
            "epoch 0 batch 287 loss 2.0316452980041504\n",
            "epoch 0 batch 288 loss 2.0445775985717773\n",
            "epoch 0 batch 289 loss 2.0908119678497314\n",
            "epoch 0 batch 290 loss 1.7935117483139038\n",
            "epoch 0 batch 291 loss 1.925407886505127\n",
            "epoch 0 batch 292 loss 1.80406653881073\n",
            "epoch 0 batch 293 loss 1.552965521812439\n",
            "epoch 0 batch 294 loss 1.9127418994903564\n",
            "epoch 0 batch 295 loss 1.6972297430038452\n",
            "epoch 0 batch 296 loss 1.9069132804870605\n",
            "epoch 0 batch 297 loss 1.9051802158355713\n",
            "epoch 0 batch 298 loss 2.0761029720306396\n",
            "epoch 0 batch 299 loss 1.9953422546386719\n",
            "epoch 0 batch 300 loss 1.4380333423614502\n",
            "epoch 0 batch 301 loss 1.6622188091278076\n",
            "epoch 0 batch 302 loss 1.8283199071884155\n",
            "epoch 0 batch 303 loss 1.6881957054138184\n",
            "epoch 0 batch 304 loss 1.4978141784667969\n",
            "epoch 0 batch 305 loss 0.9291161894798279\n",
            "epoch 0 batch 306 loss 0.9273268580436707\n",
            "epoch 0 batch 307 loss 2.04874587059021\n",
            "epoch 0 batch 308 loss 1.3440393209457397\n",
            "epoch 0 batch 309 loss 2.138345241546631\n",
            "epoch 0 batch 310 loss 0.8605205416679382\n",
            "epoch 0 batch 311 loss 2.138979434967041\n",
            "epoch 0 batch 312 loss 1.0053656101226807\n",
            "epoch 0 batch 313 loss 2.29781436920166\n",
            "epoch 0 batch 314 loss 0.8347365856170654\n",
            "epoch 0 batch 315 loss 2.246309518814087\n",
            "epoch 0 batch 316 loss 2.1263515949249268\n",
            "epoch 0 batch 317 loss 0.7706438899040222\n",
            "epoch 0 batch 318 loss 2.1260855197906494\n",
            "epoch 0 batch 319 loss 1.4532444477081299\n",
            "epoch 0 batch 320 loss 1.1379704475402832\n",
            "epoch 0 batch 321 loss 0.9623088836669922\n",
            "epoch 0 batch 322 loss 1.2199586629867554\n",
            "epoch 0 batch 323 loss 2.364132881164551\n",
            "epoch 0 batch 324 loss 4.215691566467285\n",
            "epoch 0 batch 325 loss 1.0972989797592163\n",
            "epoch 0 batch 326 loss 0.7899362444877625\n",
            "epoch 0 batch 327 loss 0.8702026605606079\n",
            "epoch 0 batch 328 loss 1.8330625295639038\n",
            "epoch 0 batch 329 loss 0.8861982226371765\n",
            "epoch 0 batch 330 loss 2.4679532051086426\n",
            "epoch 0 batch 331 loss 0.6646308302879333\n",
            "epoch 0 batch 332 loss 2.3014464378356934\n",
            "epoch 0 batch 333 loss 2.267221450805664\n",
            "epoch 0 batch 334 loss 1.3443669080734253\n",
            "epoch 0 batch 335 loss 2.3684604167938232\n",
            "epoch 0 batch 336 loss 1.7228702306747437\n",
            "epoch 0 batch 337 loss 1.9563230276107788\n",
            "epoch 0 batch 338 loss 1.627840280532837\n",
            "epoch 0 batch 339 loss 1.5822112560272217\n",
            "epoch 0 batch 340 loss 1.9662846326828003\n",
            "epoch 0 batch 341 loss 1.359731912612915\n",
            "epoch 0 batch 342 loss 1.516315221786499\n",
            "epoch 0 batch 343 loss 1.0372291803359985\n",
            "epoch 0 batch 344 loss 2.1490566730499268\n",
            "epoch 0 batch 345 loss 1.3262237310409546\n",
            "epoch 0 batch 346 loss 1.5268263816833496\n",
            "epoch 0 batch 347 loss 1.3884711265563965\n",
            "epoch 0 batch 348 loss 2.1687254905700684\n",
            "epoch 0 batch 349 loss 0.5374466180801392\n",
            "epoch 0 batch 350 loss 1.9131884574890137\n",
            "epoch 0 batch 351 loss 2.4191012382507324\n",
            "epoch 0 batch 352 loss 0.5083489418029785\n",
            "epoch 0 batch 353 loss 2.3392696380615234\n",
            "epoch 0 batch 354 loss 2.2225050926208496\n",
            "epoch 0 batch 355 loss 1.4815568923950195\n",
            "epoch 0 batch 356 loss 1.2311429977416992\n",
            "epoch 0 batch 357 loss 1.0039981603622437\n",
            "epoch 0 batch 358 loss 1.0093817710876465\n",
            "epoch 0 batch 359 loss 1.4674948453903198\n",
            "epoch 0 batch 360 loss 0.8704407811164856\n",
            "epoch 0 batch 361 loss 2.3552534580230713\n",
            "epoch 0 batch 362 loss 2.353774070739746\n",
            "epoch 0 batch 363 loss 0.4940151572227478\n",
            "epoch 0 batch 364 loss 2.4002766609191895\n",
            "epoch 0 batch 365 loss 2.344578504562378\n",
            "epoch 0 batch 366 loss 0.9580340385437012\n",
            "epoch 0 batch 367 loss 0.549414873123169\n",
            "epoch 0 batch 368 loss 0.6880476474761963\n",
            "epoch 0 batch 369 loss 2.2485859394073486\n",
            "epoch 0 batch 370 loss 0.6096271872520447\n",
            "epoch 0 batch 371 loss 1.3221843242645264\n",
            "epoch 0 batch 372 loss 2.289609432220459\n",
            "epoch 0 batch 373 loss 0.7271437048912048\n",
            "epoch 0 batch 374 loss 0.928941547870636\n",
            "epoch 0 batch 375 loss 2.220651388168335\n",
            "epoch 0 batch 376 loss 1.5467859506607056\n",
            "epoch 0 batch 377 loss 2.1432933807373047\n",
            "epoch 0 batch 378 loss 2.089008092880249\n",
            "epoch 0 batch 379 loss 1.4256459474563599\n",
            "epoch 0 batch 380 loss 1.420373558998108\n",
            "epoch 0 batch 381 loss 1.9713356494903564\n",
            "epoch 0 batch 382 loss 1.39497971534729\n",
            "epoch 0 batch 383 loss 1.8116549253463745\n",
            "epoch 0 batch 384 loss 1.933373212814331\n",
            "epoch 0 batch 385 loss 1.2394657135009766\n",
            "epoch 0 batch 386 loss 1.957689642906189\n",
            "epoch 0 batch 387 loss 1.656955361366272\n",
            "epoch 0 batch 388 loss 1.188719630241394\n",
            "epoch 0 batch 389 loss 1.215248942375183\n",
            "epoch 0 batch 390 loss 2.155207395553589\n",
            "epoch 0 batch 391 loss 1.4790582656860352\n",
            "epoch 0 batch 392 loss 0.609889805316925\n",
            "epoch 0 batch 393 loss 2.0291526317596436\n",
            "epoch 0 batch 394 loss 2.3569304943084717\n",
            "epoch 0 batch 395 loss 1.3216701745986938\n",
            "epoch 0 batch 396 loss 2.0565288066864014\n",
            "epoch 0 batch 397 loss 1.287500262260437\n",
            "epoch 0 batch 398 loss 1.0745023488998413\n",
            "epoch 0 batch 399 loss 2.1132354736328125\n",
            "epoch 0 batch 400 loss 1.980552315711975\n",
            "epoch 0 batch 401 loss 1.9241422414779663\n",
            "epoch 0 batch 402 loss 1.4454820156097412\n",
            "epoch 0 batch 403 loss 1.9295896291732788\n",
            "epoch 0 batch 404 loss 1.3722723722457886\n",
            "epoch 0 batch 405 loss 2.005153179168701\n",
            "epoch 0 batch 406 loss 1.155342698097229\n",
            "epoch 0 batch 407 loss 1.8031575679779053\n",
            "epoch 0 batch 408 loss 1.4670556783676147\n",
            "epoch 0 batch 409 loss 0.844043493270874\n",
            "epoch 0 batch 410 loss 2.090433120727539\n",
            "epoch 0 batch 411 loss 0.7153995633125305\n",
            "epoch 0 batch 412 loss 1.3230998516082764\n",
            "epoch 0 batch 413 loss 2.493398904800415\n",
            "epoch 0 batch 414 loss 1.2854208946228027\n",
            "epoch 0 batch 415 loss 2.182969570159912\n",
            "epoch 0 batch 416 loss 2.2247724533081055\n",
            "epoch 0 batch 417 loss 0.8972055912017822\n",
            "epoch 0 batch 418 loss 0.9963566064834595\n",
            "epoch 0 batch 419 loss 1.944972276687622\n",
            "epoch 0 batch 420 loss 2.2910027503967285\n",
            "epoch 0 batch 421 loss 0.8321220278739929\n",
            "epoch 0 batch 422 loss 0.81050705909729\n",
            "epoch 0 batch 423 loss 1.6330337524414062\n",
            "epoch 0 batch 424 loss 0.9715779423713684\n",
            "epoch 0 batch 425 loss 2.464160680770874\n",
            "epoch 0 batch 426 loss 1.7660953998565674\n",
            "epoch 0 batch 427 loss 0.8050839900970459\n",
            "epoch 0 batch 428 loss 2.083538770675659\n",
            "epoch 0 batch 429 loss 2.0288245677948\n",
            "epoch 0 batch 430 loss 1.1166902780532837\n",
            "epoch 0 batch 431 loss 1.9516123533248901\n",
            "epoch 0 batch 432 loss 1.9726046323776245\n",
            "epoch 0 batch 433 loss 2.0011491775512695\n",
            "epoch 0 batch 434 loss 0.828636884689331\n",
            "epoch 0 batch 435 loss 2.013181447982788\n",
            "epoch 0 batch 436 loss 1.2832109928131104\n",
            "epoch 0 batch 437 loss 2.064798355102539\n",
            "epoch 0 batch 438 loss 1.9221014976501465\n",
            "epoch 0 batch 439 loss 1.9143564701080322\n",
            "epoch 0 batch 440 loss 1.647797703742981\n",
            "epoch 0 batch 441 loss 1.9506179094314575\n",
            "epoch 0 batch 442 loss 1.924142837524414\n",
            "epoch 0 batch 443 loss 2.1334593296051025\n",
            "epoch 0 batch 444 loss 1.7333952188491821\n",
            "epoch 0 batch 445 loss 1.5520843267440796\n",
            "epoch 0 batch 446 loss 1.4974349737167358\n",
            "epoch 0 batch 447 loss 1.9016245603561401\n",
            "epoch 0 batch 448 loss 1.8474782705307007\n",
            "epoch 0 batch 449 loss 0.9591114521026611\n",
            "epoch 0 batch 450 loss 2.021660566329956\n",
            "epoch 0 batch 451 loss 2.015564441680908\n",
            "epoch 0 batch 452 loss 0.5352146625518799\n",
            "epoch 0 batch 453 loss 2.3100790977478027\n",
            "epoch 0 batch 454 loss 1.8386057615280151\n",
            "epoch 0 batch 455 loss 1.267005205154419\n",
            "epoch 0 batch 456 loss 1.3616467714309692\n",
            "epoch 0 batch 457 loss 2.0404269695281982\n",
            "epoch 0 batch 458 loss 1.354439377784729\n",
            "epoch 0 batch 459 loss 1.9845629930496216\n",
            "epoch 0 batch 460 loss 2.038672924041748\n",
            "epoch 0 batch 461 loss 0.9844826459884644\n",
            "epoch 0 batch 462 loss 1.916015625\n",
            "epoch 0 batch 463 loss 2.2815332412719727\n",
            "epoch 0 batch 464 loss 1.9051157236099243\n",
            "epoch 0 batch 465 loss 1.5404282808303833\n",
            "epoch 0 batch 466 loss 1.9835786819458008\n",
            "epoch 0 batch 467 loss 1.1949567794799805\n",
            "epoch 0 batch 468 loss 1.9632796049118042\n",
            "epoch 0 batch 469 loss 1.000592589378357\n",
            "epoch 0 batch 470 loss 0.6904106736183167\n",
            "epoch 0 batch 471 loss 1.3617258071899414\n",
            "epoch 0 batch 472 loss 0.6752837300300598\n",
            "epoch 0 batch 473 loss 1.9854137897491455\n",
            "epoch 0 batch 474 loss 2.2191667556762695\n",
            "epoch 0 batch 475 loss 1.9325472116470337\n",
            "epoch 0 batch 476 loss 2.223480701446533\n",
            "epoch 0 batch 477 loss 1.9677616357803345\n",
            "epoch 0 batch 478 loss 1.9838149547576904\n",
            "epoch 0 batch 479 loss 1.859717607498169\n",
            "epoch 0 batch 480 loss 1.578324556350708\n",
            "epoch 0 batch 481 loss 1.8064807653427124\n",
            "epoch 0 batch 482 loss 1.3769360780715942\n",
            "epoch 0 batch 483 loss 1.330222725868225\n",
            "epoch 0 batch 484 loss 1.2208011150360107\n",
            "epoch 0 batch 485 loss 2.0994811058044434\n",
            "epoch 0 batch 486 loss 2.095928430557251\n",
            "epoch 0 batch 487 loss 2.183124542236328\n",
            "epoch 0 batch 488 loss 1.1850508451461792\n",
            "epoch 0 batch 489 loss 0.5916284918785095\n",
            "epoch 0 batch 490 loss 0.5016692280769348\n",
            "epoch 0 batch 491 loss 1.523301362991333\n",
            "epoch 0 batch 492 loss 2.233394145965576\n",
            "epoch 0 batch 493 loss 1.9329982995986938\n",
            "epoch 0 batch 494 loss 1.7680771350860596\n",
            "epoch 0 batch 495 loss 1.5791289806365967\n",
            "epoch 0 batch 496 loss 1.0212202072143555\n",
            "epoch 0 batch 497 loss 1.958377718925476\n",
            "epoch 0 batch 498 loss 0.8879456520080566\n",
            "epoch 0 batch 499 loss 2.0358726978302\n",
            "epoch 0 batch 500 loss 2.2622060775756836\n",
            "epoch 0 batch 501 loss 1.357077717781067\n",
            "epoch 0 batch 502 loss 1.8478195667266846\n",
            "epoch 0 batch 503 loss 1.5648308992385864\n",
            "epoch 0 batch 504 loss 1.1168341636657715\n",
            "epoch 0 batch 505 loss 1.177607774734497\n",
            "epoch 0 batch 506 loss 2.201970100402832\n",
            "epoch 0 batch 507 loss 0.8398631811141968\n",
            "epoch 0 batch 508 loss 1.3135517835617065\n",
            "epoch 0 batch 509 loss 0.9366826415061951\n",
            "epoch 0 batch 510 loss 1.8118587732315063\n",
            "epoch 0 batch 511 loss 0.5108048915863037\n",
            "epoch 0 batch 512 loss 2.2533342838287354\n",
            "epoch 0 batch 513 loss 0.7409718036651611\n",
            "epoch 0 batch 514 loss 2.840742826461792\n",
            "epoch 0 batch 515 loss 2.246126174926758\n",
            "epoch 0 batch 516 loss 2.1533701419830322\n",
            "epoch 0 batch 517 loss 2.115929126739502\n",
            "epoch 0 batch 518 loss 0.7820929884910583\n",
            "epoch 0 batch 519 loss 1.8760557174682617\n",
            "epoch 0 batch 520 loss 1.9760082960128784\n",
            "epoch 0 batch 521 loss 1.959757685661316\n",
            "epoch 0 batch 522 loss 1.9732859134674072\n",
            "epoch 0 batch 523 loss 1.7433961629867554\n",
            "epoch 0 batch 524 loss 1.3719500303268433\n",
            "epoch 0 batch 525 loss 1.3748886585235596\n",
            "epoch 0 batch 526 loss 1.2645214796066284\n",
            "epoch 0 batch 527 loss 1.9598302841186523\n",
            "epoch 0 batch 528 loss 1.6101802587509155\n",
            "epoch 0 batch 529 loss 0.9553403854370117\n",
            "epoch 0 batch 530 loss 0.5944744348526001\n",
            "epoch 0 batch 531 loss 0.5369401574134827\n",
            "epoch 0 batch 532 loss 0.8969894647598267\n",
            "epoch 0 batch 533 loss 0.45512211322784424\n",
            "epoch 0 batch 534 loss 2.5822668075561523\n",
            "epoch 0 batch 535 loss 3.4357759952545166\n",
            "epoch 0 batch 536 loss 2.4328391551971436\n",
            "epoch 0 batch 537 loss 0.8288481831550598\n",
            "epoch 0 batch 538 loss 2.6949284076690674\n",
            "epoch 0 batch 539 loss 0.559173047542572\n",
            "epoch 0 batch 540 loss 0.6553374528884888\n",
            "epoch 0 batch 541 loss 2.3039443492889404\n",
            "epoch 0 batch 542 loss 1.51990807056427\n",
            "epoch 0 batch 543 loss 1.4582772254943848\n",
            "epoch 0 batch 544 loss 1.745087742805481\n",
            "epoch 0 batch 545 loss 1.9433190822601318\n",
            "epoch 0 batch 546 loss 2.0267486572265625\n",
            "epoch 0 batch 547 loss 1.3749772310256958\n",
            "epoch 0 batch 548 loss 1.4624868631362915\n",
            "epoch 0 batch 549 loss 1.9515464305877686\n",
            "epoch 0 batch 550 loss 1.1037527322769165\n",
            "epoch 0 batch 551 loss 0.9101568460464478\n",
            "epoch 0 batch 552 loss 0.6219972968101501\n",
            "epoch 0 batch 553 loss 2.5527408123016357\n",
            "epoch 0 batch 554 loss 0.3040570616722107\n",
            "epoch 0 batch 555 loss 2.0426723957061768\n",
            "epoch 0 batch 556 loss 0.36032259464263916\n",
            "epoch 0 batch 557 loss 1.2340368032455444\n",
            "epoch 0 batch 558 loss 1.6176660060882568\n",
            "epoch 0 batch 559 loss 1.126486897468567\n",
            "epoch 0 batch 560 loss 1.9091259241104126\n",
            "epoch 0 batch 561 loss 0.6790426969528198\n",
            "epoch 0 batch 562 loss 0.8990840911865234\n",
            "epoch 0 batch 563 loss 1.2910739183425903\n",
            "epoch 0 batch 564 loss 2.2280805110931396\n",
            "Loading data...\n",
            "19\n",
            "Extracting features...\n",
            "torch.Size([26, 300, 37])\n",
            "Creating dataloader...\n",
            "Done data creation !\n",
            "-------------------start evaluating-------------------\n",
            "Accuracy:  0.3749728202819824\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# print(device)\n",
        "\n",
        "\n",
        "\n",
        "inverted_classes = {v: k for k, v in classes.items()}\n",
        "\n",
        "\n",
        "inp_vocab_size = 37\n",
        "hidden_dim = 128\n",
        "seq_len = 400\n",
        "num_classes = 16\n",
        "\n",
        "model = LSTM(inp_vocab_size, hidden_dim, seq_len, num_classes)\n",
        "# model.load_state_dict(torch.load(\"LSTM98.pth\", map_location=torch.device(\"cpu\")))\n",
        "# model.eval()\n",
        "\n",
        "Traindata = DataSet( \"Dataset/train.txt\", batch_size = 1 )\n",
        "Traindataloader = Traindata.getdata()\n",
        "\n",
        "model = LSTM(inp_vocab_size, hidden_dim, seq_len, num_classes)\n",
        "print(\"-------------------start training-------------------\")\n",
        "train(Traindataloader, model)\n",
        "\n",
        "\n",
        "validatindata=DataSet(\"Dataset/val.txt\",batch_size=1)\n",
        "validatindataloader=validatindata.getdata()\n",
        "print(\"-------------------start evaluating-------------------\")\n",
        "\n",
        "\n",
        "predictions = []\n",
        "actuals=[]\n",
        "def evaluate_model(test_dl, model):\n",
        "    predicted_classes = []\n",
        "    model.to(device)\n",
        "    for i, (inputs, targets) in enumerate(test_dl):\n",
        "        yhat = model(inputs.to(device))\n",
        "        yhat = yhat.detach().cpu().numpy()\n",
        "        # reshape the outputs to [batch_size * sequence_length, num_classes]\n",
        "        yhat = yhat.reshape(-1, yhat.shape[-1])\n",
        "        # get predicted classes\n",
        "        predicted_classes = np.argmax(yhat, axis=1)\n",
        "        # convert targets to numpy array and reshape\n",
        "        targets = targets.cpu().numpy().reshape(-1)\n",
        "        while len(targets)>0 and targets[-1]==15:\n",
        "            targets=targets[:-1]\n",
        "            predicted_classes=predicted_classes[:-1]\n",
        "        # store predictions and actuals\n",
        "        predictions.extend(predicted_classes)\n",
        "        actuals.extend(targets.tolist())\n",
        "\n",
        "data=validatindata.getx()\n",
        "\n",
        "\n",
        "evaluate_model(validatindataloader,model)\n",
        "result = \"\"\n",
        "for i in range(len(data)):\n",
        "    for j in range(len(data[i])):\n",
        "        result += data[i][j]\n",
        "        result += inverted_classes[predictions[i*10+j]]\n",
        "\n",
        "# print(\"Reversed string: \", result[::-1])\n",
        "\n",
        "# print(\"Predicted classes: \", predictions)\n",
        "actuals_res=[]\n",
        "predictions_res=[]\n",
        "for i in range(len(actuals)):\n",
        "  if actuals[i]!=15:\n",
        "    actuals_res.append(actuals[i])\n",
        "    predictions_res.append(predictions[i])\n",
        "predicted_classes_csv = [(i, label) for i, label in enumerate(predictions_res)]\n",
        "\n",
        "id_line_letter_df_after = pd.DataFrame(\n",
        "    {\n",
        "        \"id\": [info[0] for info in predicted_classes_csv],\n",
        "        \"label\": [info[1] for info in predicted_classes_csv],\n",
        "    }\n",
        ")\n",
        "# id_line_letter_df_after.to_csv(\"predicted_chars.csv\", index=False)\n",
        "\n",
        "\n",
        "\n",
        "def calculate_DER(actual_labels, predicted_labels):\n",
        "    # Convert lists to PyTorch tensors if they are not already\n",
        "    if not isinstance(actual_labels, torch.Tensor):\n",
        "        actual_labels = torch.tensor(actual_labels)\n",
        "    if not isinstance(predicted_labels, torch.Tensor):\n",
        "        predicted_labels = torch.tensor(predicted_labels)\n",
        "\n",
        "    # Check if the lengths of both label sequences match\n",
        "    if len(actual_labels) != len(predicted_labels):\n",
        "        raise ValueError(\"Lengths of actual and predicted labels should match.\")\n",
        "\n",
        "    total_errors = torch.sum(actual_labels != predicted_labels)\n",
        "    total_frames = len(actual_labels)\n",
        "\n",
        "    # DER calculation\n",
        "    DER = (1-(total_errors / total_frames)) * 100.0\n",
        "    return DER.item()  # Convert PyTorch scalar to Python float\n",
        "\n",
        "# predictions=[x for x in predictions if x!=15]\n",
        "# actuals=[x for x in actuals if x!=15]\n",
        "\n",
        "acc = calculate_DER(np.array(actuals_res), np.array(predictions_res))\n",
        "print(\"Accuracy: \", acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6svTOWYF255z",
        "outputId": "3487e575-cde0-4494-fd4c-4924d498bb24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ذ ه ب   ع ل ي   ا ل يَ َاَلَشَاَطَئَ\n"
          ]
        }
      ],
      "source": [
        "# test case\n",
        "data=\"ذهب علي الي الشاطئ\"\n",
        "enc = torch.empty(0, len(arabic_letters),dtype=torch.float32).to(device)\n",
        "for letter in data:\n",
        "    x = encoding(letter).unsqueeze(0).to(device)\n",
        "    enc = torch.cat((enc, x), 0)\n",
        "# print(encoded_data.shape)\n",
        "encoding_labels=torch.tensor([],dtype=torch.long).to(device)\n",
        "predictions=[]\n",
        "# for i in range(len(enc)):\n",
        "yhat = model(enc.to(device))\n",
        "yhat = yhat.detach().cpu().numpy()\n",
        "# reshape the outputs to [batch_size * sequence_length, num_classes]\n",
        "yhat = yhat.reshape(-1, yhat.shape[-1])\n",
        "# get predicted classes\n",
        "predicted_classes = np.argmax(yhat, axis=1)\n",
        "# store predictions and actuals\n",
        "predictions.extend(predicted_classes.tolist())\n",
        "res=\"\"\n",
        "inverted_classes = {v: k for k, v in classes.items()}\n",
        "\n",
        "for i in range(len(data)):\n",
        "  res+=data[i]\n",
        "  res+=inverted_classes[predictions[i]]\n",
        "print(res)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2_eYAAnCslV",
        "outputId": "7b1c9ac9-a96b-468c-a572-68c24115171e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "16\n",
            "Extracting features...\n",
            "torch.Size([28, 300, 37])\n",
            "Creating dataloader...\n",
            "Done data creation !\n",
            "-------------------start evaluating-------------------\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "inp_vocab_size = 37\n",
        "hidden_dim = 128\n",
        "seq_len = 400\n",
        "num_classes = 16\n",
        "\n",
        "# model = LSTM(inp_vocab_size, hidden_dim, seq_len, num_classes)\n",
        "# model.load_state_dict(torch.load(\"/content/model.pth\", map_location=torch.device(\"cpu\")))\n",
        "\n",
        "validatindata=DataSet(\"test_no_diacritics.txt\",batch_size=1)\n",
        "validatindataloader=validatindata.getdata()\n",
        "print(\"-------------------start evaluating-------------------\")\n",
        "\n",
        "\n",
        "predictions = []\n",
        "actuals=[]\n",
        "data=validatindata.getx()\n",
        "statments=[state for state in data]\n",
        "\n",
        "def evaluate_model(test_dl, model):\n",
        "    predicted_classes = []\n",
        "    model.to(device)\n",
        "    for i, (inputs, targets) in enumerate(test_dl):\n",
        "        yhat = model(inputs.to(device))\n",
        "        yhat = yhat.detach().cpu().numpy()\n",
        "        # reshape the outputs to [batch_size * sequence_length, num_classes]\n",
        "        yhat = yhat.reshape(-1, yhat.shape[-1])\n",
        "        # get predicted classes\n",
        "        predicted_classes = np.argmax(yhat, axis=1)\n",
        "        # convert targets to numpy array and reshape\n",
        "        targets = targets.cpu().numpy().reshape(-1)\n",
        "        while len(targets)>0 and targets[-1]==15:\n",
        "            targets=targets[:-1]\n",
        "            predicted_classes=predicted_classes[:-1]\n",
        "            statments[i]=statments[i][:-1]\n",
        "        # store predictions and actuals\n",
        "        predictions.extend(predicted_classes)\n",
        "        actuals.extend(targets.tolist())\n",
        "\n",
        "\n",
        "\n",
        "evaluate_model(validatindataloader,model)\n",
        "result = \"\"\n",
        "c=0\n",
        "for i in range(len(statments)):\n",
        "    for j in range(len(statments[i])):\n",
        "        result += statments[i][j]\n",
        "        result += inverted_classes[predictions[i*10+j]]\n",
        "        if (statments[i][j]==\" \"):\n",
        "          c+=1\n",
        "\n",
        "# print(\"Reversed string: \", result)\n",
        "\n",
        "# print(\"Predicted classes: \", predictions)\n",
        "actuals_res=[]\n",
        "predictions_res=[]\n",
        "# print (len(actuals))\n",
        "\n",
        "for i in range(len(actuals)):\n",
        "  if actuals[i]!=15:\n",
        "    actuals_res.append(actuals[i])\n",
        "    predictions_res.append(predictions[i])\n",
        "# print(len(actuals_res))\n",
        "predicted_classes_csv = [(i, label) for i, label in enumerate(predictions_res)]\n",
        "\n",
        "id_line_letter_df_after = pd.DataFrame(\n",
        "    {\n",
        "        \"ID\": [info[0] for info in predicted_classes_csv],\n",
        "        \"label\": [info[1] for info in predicted_classes_csv],\n",
        "    }\n",
        ")\n",
        "id_line_letter_df_after.to_csv(\"predicted_chars.csv\", index=False)\n",
        "\n",
        "\n",
        "\n",
        "# def calculate_DER(actual_labels, predicted_labels):\n",
        "#     # Convert lists to PyTorch tensors if they are not already\n",
        "#     if not isinstance(actual_labels, torch.Tensor):\n",
        "#         actual_labels = torch.tensor(actual_labels)\n",
        "#     if not isinstance(predicted_labels, torch.Tensor):\n",
        "#         predicted_labels = torch.tensor(predicted_labels)\n",
        "\n",
        "#     # Check if the lengths of both label sequences match\n",
        "#     if len(actual_labels) != len(predicted_labels):\n",
        "#         raise ValueError(\"Lengths of actual and predicted labels should match.\")\n",
        "\n",
        "#     total_errors = torch.sum(actual_labels != predicted_labels)\n",
        "#     total_frames = len(actual_labels)\n",
        "\n",
        "#     # DER calculation\n",
        "#     DER = (1-(total_errors / total_frames)) * 100.0\n",
        "#     return DER.item()  # Convert PyTorch scalar to Python float\n",
        "\n",
        "# predictions=[x for x in predictions if x!=15]\n",
        "# actuals=[x for x in actuals if x!=15]\n",
        "\n",
        "# acc = calculate_DER(np.array(actuals_res), np.array(predictions_res))\n",
        "# print(\"Accuracy: \", acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SA8vdWh96fUT",
        "outputId": "124404d7-d09f-4a27-aac4-3a0ef032c8a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "16\n",
            "Extracting features...\n",
            "torch.Size([28, 300, 37])\n",
            "Creating dataloader...\n",
            "Done data creation !\n",
            "-------------------start evaluating-------------------\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# inp_vocab_size = 37\n",
        "# hidden_dim = 128\n",
        "# seq_len = 400\n",
        "# num_classes = 16\n",
        "\n",
        "# model = LSTM(inp_vocab_size, hidden_dim, seq_len, num_classes)\n",
        "# model.load_state_dict(torch.load(\"/content/model.pth\", map_location=torch.device(\"cpu\")))\n",
        "\n",
        "validatindata=DataSet(\"test_no_diacritics.txt\",batch_size=1)\n",
        "validatindataloader=validatindata.getdata()\n",
        "print(\"-------------------start evaluating-------------------\")\n",
        "\n",
        "\n",
        "predictions = []\n",
        "actuals=[]\n",
        "data=validatindata.getx()\n",
        "statments=[state for state in data]\n",
        "\n",
        "def evaluate_model(test_dl, model):\n",
        "    predicted_classes = []\n",
        "    model.to(device)\n",
        "    for i, (inputs, targets) in enumerate(test_dl):\n",
        "        yhat = model(inputs.to(device))\n",
        "        yhat = yhat.detach().cpu().numpy()\n",
        "        # reshape the outputs to [batch_size * sequence_length, num_classes]\n",
        "        yhat = yhat.reshape(-1, yhat.shape[-1])\n",
        "        # get predicted classes\n",
        "        predicted_classes = np.argmax(yhat, axis=1)\n",
        "        # convert targets to numpy array and reshape\n",
        "        targets = targets.cpu().numpy().reshape(-1)\n",
        "        while len(targets)>0 and targets[-1]==15:\n",
        "            targets=targets[:-1]\n",
        "            predicted_classes=predicted_classes[:-1]\n",
        "            statments[i]=statments[i][:-1]\n",
        "        # store predictions and actuals\n",
        "        predictions.extend(predicted_classes)\n",
        "        actuals.extend(targets.tolist())\n",
        "\n",
        "\n",
        "\n",
        "evaluate_model(validatindataloader,model)\n",
        "result = \"\"\n",
        "c=0\n",
        "# for i in range(len(statments)):\n",
        "#     for j in range(len(statments[i])):\n",
        "#         result += statments[i][j]\n",
        "#         result += inverted_classes[predictions[i*10+j]]\n",
        "#         if (statments[i][j]==\" \"):\n",
        "#           c+=1\n",
        "\n",
        "# print(\"Reversed string: \", result)\n",
        "\n",
        "# print(\"Predicted classes: \", predictions)\n",
        "actuals_res=[]\n",
        "predictions_res=[]\n",
        "# print (len(actuals))\n",
        "\n",
        "for i in range(len(actuals)):\n",
        "  if actuals[i]!=15:\n",
        "    actuals_res.append(actuals[i])\n",
        "    predictions_res.append(predictions[i])\n",
        "# print(len(actuals_res))\n",
        "predicted_classes_csv = [(i, label) for i, label in enumerate(predictions_res)]\n",
        "\n",
        "id_line_letter_df_after = pd.DataFrame(\n",
        "    {\n",
        "        \"id\": [info[0] for info in predicted_classes_csv],\n",
        "        \"label\": [info[1] for info in predicted_classes_csv],\n",
        "    }\n",
        ")\n",
        "# id_line_letter_df_after.to_csv(\"predicted_chars.csv\", index=False)\n",
        "\n",
        "\n",
        "\n",
        "# def calculate_DER(actual_labels, predicted_labels):\n",
        "#     # Convert lists to PyTorch tensors if they are not already\n",
        "#     if not isinstance(actual_labels, torch.Tensor):\n",
        "#         actual_labels = torch.tensor(actual_labels)\n",
        "#     if not isinstance(predicted_labels, torch.Tensor):\n",
        "#         predicted_labels = torch.tensor(predicted_labels)\n",
        "\n",
        "#     # Check if the lengths of both label sequences match\n",
        "#     if len(actual_labels) != len(predicted_labels):\n",
        "#         raise ValueError(\"Lengths of actual and predicted labels should match.\")\n",
        "\n",
        "#     total_errors = torch.sum(actual_labels != predicted_labels)\n",
        "#     total_frames = len(actual_labels)\n",
        "\n",
        "#     # DER calculation\n",
        "#     DER = (1-(total_errors / total_frames)) * 100.0\n",
        "#     return DER.item()  # Convert PyTorch scalar to Python float\n",
        "\n",
        "# predictions=[x for x in predictions if x!=15]\n",
        "# actuals=[x for x in actuals if x!=15]\n",
        "\n",
        "# acc = calculate_DER(np.array(actuals_res), np.array(predictions_res))\n",
        "# print(\"Accuracy: \", acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "t4o1KQ6wZYmT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5276\n",
            "4239\n"
          ]
        }
      ],
      "source": [
        "actuals_res=[]\n",
        "print(len(actuals))\n",
        "for i in range(len(actuals)):\n",
        "  if actuals[i]!=15:\n",
        "    actuals_res.append(actuals[i])\n",
        "    predictions_res.append(predictions[i])\n",
        "print(len(actuals_res))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Up-QjXZdaiMK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/NLP project/test_no_diacritics.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m validatindata\u001b[38;5;241m=\u001b[39m\u001b[43mDataSet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/content/drive/MyDrive/NLP project/test_no_diacritics.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m validatindataloader\u001b[38;5;241m=\u001b[39mvalidatindata\u001b[38;5;241m.\u001b[39mgetdata()\n",
            "Cell \u001b[1;32mIn[30], line 443\u001b[0m, in \u001b[0;36mDataSet.__init__\u001b[1;34m(self, path, batch_size, test)\u001b[0m\n\u001b[0;32m    441\u001b[0m     data1,labels1\u001b[38;5;241m=\u001b[39mget_test(path)\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 443\u001b[0m     data1,labels1\u001b[38;5;241m=\u001b[39m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;66;03m# now labels is list of list [[1,2,3,4,5,15,15,0],[1,2,3,4,5,15,15,0]]\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# data is list of string ['احمد','محمد']\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting features...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[30], line 286\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_data\u001b[39m(path):\n\u001b[1;32m--> 286\u001b[0m     text\u001b[38;5;241m=\u001b[39m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m     text\u001b[38;5;241m=\u001b[39mpreprocess(text)\n\u001b[0;32m    288\u001b[0m     size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.01\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(text))\n",
            "Cell \u001b[1;32mIn[30], line 59\u001b[0m, in \u001b[0;36mread_text\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_text\u001b[39m(file_path):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mread()\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/NLP project/test_no_diacritics.txt'"
          ]
        }
      ],
      "source": [
        "validatindata=DataSet(\"/content/drive/MyDrive/NLP project/test_no_diacritics.txt\",batch_size=1)\n",
        "validatindataloader=validatindata.getdata()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
