{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DV7fmwYwVX_",
        "outputId": "c18d0e85-bb05-4b8e-a72f-3c80e108108e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "u2nZg8-K24xf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb631003-b167-4d33-b635-4ed1598df810"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import re\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import scipy.sparse\n",
        "# from gensim.models import KeyedVectors\n",
        "# from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import os\n",
        "\n",
        "# import stanfordnlp\n",
        "\n",
        "# Download the Arabic models for the neural pipeline\n",
        "# stanfordnlp.download('ar', force=True)\n",
        "# Build a neural pipeline using the Arabic models\n",
        "# nlp = stanfordnlp.Pipeline(lang='ar')\n",
        "\n",
        "# def split_arabic_sentences_with_stanfordnlp(corpus_text):\n",
        "#     # Process the text\n",
        "#     doc = nlp(corpus_text)\n",
        "\n",
        "#     # Extract sentences from the doc\n",
        "#     sentences = [sentence.text for sentence in doc.sentences]\n",
        "\n",
        "#     return sentences\n",
        "# file_path = '../SG_300_3_400/w2v_SG_300_3_400_10.model'\n",
        "# word_embed_model = Word2Vec.load(file_path)\n",
        "\n",
        "max_len=300\n",
        "\n",
        "with open('/content/drive/MyDrive/NLP project/arabic_letters.pickle', 'rb') as file:\n",
        "            ARABIC_LETTERS_LIST = pkl.load(file)\n",
        "with open('/content/drive/MyDrive/NLP project/diacritics.pickle', 'rb') as file:\n",
        "            DIACRITICS_LIST = pkl.load(file)\n",
        "with open('/content/drive/MyDrive/NLP project/diacritic2id.pickle', 'rb') as file:\n",
        "            DIACRITICS_LIST2ID = pkl.load(file)\n",
        "\n",
        "arabic_letters=[]\n",
        "for letter in ARABIC_LETTERS_LIST:\n",
        "    arabic_letters.append(letter[0])\n",
        "arabic_letters.append(\" \")\n",
        "\n",
        "\n",
        "dicritics=[]\n",
        "for letter in DIACRITICS_LIST:\n",
        "    dicritics.append(letter[0])\n",
        "classes={k:i for i,k in enumerate(DIACRITICS_LIST2ID)}\n",
        "classes[\" \"]=15\n",
        "\n",
        "\n",
        "\n",
        "inverted_classes = {v: k for k, v in classes.items()}\n",
        "\n",
        "##################### to delete #####################\n",
        "def read_text(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        return file.read()\n",
        "\n",
        "def write_to_file_second(dirctory,file_path, text):\n",
        "    if not os.path.exists(dirctory):\n",
        "        os.makedirs(dirctory)\n",
        "\n",
        "    with open(dirctory+\"/\"+file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        lines=text\n",
        "        for l in lines:\n",
        "            file.write(l)\n",
        "        file.write('\\n')\n",
        "def write_to_file_labels(dirctory,file_path, text):\n",
        "    if not os.path.exists(dirctory):\n",
        "        os.makedirs(dirctory)\n",
        "\n",
        "    with open(dirctory+\"/\"+file_path, \"a\", encoding=\"utf-8\") as file:\n",
        "        lines=text\n",
        "        for l in lines:\n",
        "            file.write(inverted_classes[l])\n",
        "        file.write('\\n')\n",
        "\n",
        "\n",
        "def write_to_file_string(dirctory,file_path, text):\n",
        "    if not os.path.exists(dirctory):\n",
        "        os.makedirs(dirctory)\n",
        "\n",
        "    with open(dirctory+\"/\"+file_path, \"a\", encoding=\"utf-8\") as file:\n",
        "        lines=text\n",
        "        file.write(lines)\n",
        "        file.write('\\n')\n",
        "        file.write('\\n')\n",
        "#####################################################\n",
        "\n",
        "def preprocess(text):\n",
        "        # Remove URLs\n",
        "        text = re.sub(r\"http[s|S]\\S+\", \"\", text,flags=re.MULTILINE)\n",
        "        text = re.sub(r\"www\\S+\", \"\", text,flags=re.MULTILINE)\n",
        "        # Remove English letters\n",
        "        text = re.sub(r\"[A-Za-z]+\", \"\", text,flags=re.MULTILINE)\n",
        "        # Remove Kashida Arabic character\n",
        "        text = re.sub(r\"\\u0640\", \"\", text,flags=re.MULTILINE)\n",
        "        # Add space before and after the numbers\n",
        "        text = re.sub(r\"(\\d+)\", r\" \\1 \", text,flags=re.MULTILINE)\n",
        "        # removes SHIFT+J Arabic character\n",
        "        text = re.sub(r\"\\u0691\", \"\", text,flags=re.MULTILINE)\n",
        "        # remove english numbers\n",
        "        text = re.sub(r\"[0-9]+\", \"\", text,flags=re.MULTILINE)\n",
        "        # remove arabic numbers\n",
        "        text = re.sub(r\"[٠-٩]+\", \"\", text,flags=re.MULTILINE)\n",
        "         # remove brackets\n",
        "        # text = re.sub(r\"\\[.*?\\]\", \"\", text)\n",
        "        # text = re.sub(r\"\\(.*?\\)\", \"\", text)\n",
        "        return text\n",
        "\n",
        "\n",
        "# text=read_text('Dataset/train.txt')\n",
        "# text=preprocess(text)\n",
        "# print(len(text))\n",
        "# # delete except arabic letters and dicritics and punctuation\n",
        "# # remove multiple spaces\n",
        "\n",
        "# write_to_file_string(\"test\",\"data.txt\",text)\n",
        "\n",
        "def split_text(text):\n",
        "    text=text.split('.')\n",
        "    # split text to sentences on all arabic sparatators\n",
        "\n",
        "    data=[]\n",
        "    for t in text:\n",
        "        if(len(t)==0): continue\n",
        "        if(len(t)<max_len):\n",
        "            while(len(t)<max_len):\n",
        "                 t+=\" \"\n",
        "            data.append(t)\n",
        "        if(len(t)>max_len):\n",
        "            data.append(t[:max_len])\n",
        "            supdata=t[max_len:]\n",
        "            while(len(supdata)>max_len):\n",
        "                data.append(supdata[:max_len])\n",
        "                supdata=supdata[max_len:]\n",
        "            if(len(supdata)<max_len):\n",
        "                while(len(supdata)<max_len):\n",
        "                    supdata+=\" \"\n",
        "                data.append(supdata)\n",
        "    return data\n",
        "\n",
        "\n",
        "# [\".\", \"،\", \":\", \"؛\", \"-\", \"؟\"]\n",
        "VALID_ARABIC = dicritics + arabic_letters + ['.']\n",
        "\n",
        "import re\n",
        "\n",
        "_whitespace_re = re.compile(r\"\\s+\")\n",
        "\n",
        "def remove_spaces(text):\n",
        "    text = re.sub(_whitespace_re, \" \", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def preprocessing(text):\n",
        "    text = filter(lambda char: char in VALID_ARABIC, text)\n",
        "    text = remove_spaces(''.join(list(text)))\n",
        "    text=text.strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "def get_data_labels(text):\n",
        "    data=\"\"\n",
        "    labels=[]\n",
        "    for i in range(len(text)):\n",
        "        if(text[i] in arabic_letters):\n",
        "            data+=text[i]\n",
        "            if(text[i]==\" \"):\n",
        "                labels.append(15)\n",
        "            elif(i+1<len(text) and text[i+1] in dicritics):\n",
        "                if(i+2<len(text) and text[i+2] in dicritics and (classes.get(text[i+1])==7 or classes.get(text[i+2])==7)):\n",
        "                    if(text[i+1]==7 ):\n",
        "                        labels.append(classes[text[i+2]+text[i+1]])\n",
        "                        i+=2\n",
        "                    else:\n",
        "                        labels.append(classes[text[i+1]+text[i+2]])\n",
        "                        i+=2\n",
        "                else:\n",
        "                    labels.append(classes[text[i+1]])\n",
        "                    i+=1\n",
        "            else:\n",
        "                labels.append(14)\n",
        "    return data,labels\n",
        "\n",
        "def encoding(text):\n",
        "    idx=arabic_letters.index(text)\n",
        "    encode=np.zeros(len(arabic_letters))\n",
        "    encode[idx]=1\n",
        "    return torch.tensor(encode,dtype=torch.float32).to(device)\n",
        "\n",
        "\n",
        "def get_dataloader(encoded_data, encoding_labels,batch_size=1):\n",
        "    # Create TensorDataset\n",
        "    dataset = TensorDataset(encoded_data.to(device), encoding_labels.to(device))\n",
        "    # Create DataLoader\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "def get_validation():\n",
        "    text=read_text('Dataset/val.txt')\n",
        "    text=preprocess(text)\n",
        "    # size=int(0.02*len(text))\n",
        "    # text=text[:size]\n",
        "    # text=split_text(text)\n",
        "    # write_to_file_second(\"test\",\"data.txt\",text)\n",
        "    # text=split_text(text)\n",
        "    data=[]\n",
        "    labels=[]\n",
        "    for i in range (len(text)):\n",
        "        if(text[i] in arabic_letters):\n",
        "                data+=(text[i])\n",
        "                if(i+1<len(text) and text[i+1] in dicritics):\n",
        "                    if(i+2<len(text) and classes[text[i+1]]==4 and text[i+2] in dicritics):\n",
        "                        labels.append(classes[text[i+1]+text[i+2]])\n",
        "                        i+=2\n",
        "                    else:\n",
        "                        labels.append(classes[text[i+1]])\n",
        "                        i+=1\n",
        "                else:\n",
        "                    labels.append(15)\n",
        "        # else:\n",
        "        #     data.append(text[i])\n",
        "        #     labels.append(15)\n",
        "    encoded_data = torch.empty(0, len(arabic_letters),dtype=torch.float32)\n",
        "\n",
        "    for letter in data:\n",
        "        if letter in arabic_letters:\n",
        "            x = encoding(letter).unsqueeze(0)\n",
        "        else:\n",
        "            x=np.zeros((1,len(arabic_letters)))\n",
        "            x=torch.tensor(x,dtype=torch.float32)\n",
        "        encoded_data = torch.cat((encoded_data, x), 0)\n",
        "    labels=torch.tensor(labels,dtype=torch.long)\n",
        "    # print(encoded_data.shape)\n",
        "    # print(labels.shape)\n",
        "    dataloader=get_dataloader(encoded_data,labels)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "def get_test(path):\n",
        "    text=read_text(path)\n",
        "    text=preprocess(text)\n",
        "    # size=int(0.01*len(text))\n",
        "    # text=text[:size]\n",
        "    # print(len(text))\n",
        "    text = preprocessing(text)\n",
        "    text=\"\".join(text)\n",
        "    text=text.split('.')\n",
        "    data=[]\n",
        "    labels=[]\n",
        "    for t in text:\n",
        "        t=t.strip()\n",
        "        d=\"\"\n",
        "        l=[]\n",
        "        if len(t)>max_len:\n",
        "            continue\n",
        "        else:\n",
        "            d,l=get_data_labels(t)\n",
        "            if(len(d)==0): continue\n",
        "            elif(len(d)<max_len):\n",
        "                data.append(d)\n",
        "                labels.append(l)\n",
        "                continue\n",
        "            elif(len(d)>max_len):\n",
        "                data.append(d[:max_len])\n",
        "                labels.append(l[:max_len])\n",
        "                supdata=d[max_len:]\n",
        "                suplabels=l[max_len:]\n",
        "                while(len(supdata)>max_len):\n",
        "                    data.append(supdata[:max_len])\n",
        "                    labels.append(suplabels[:max_len])\n",
        "                    supdata=supdata[max_len:]\n",
        "                    suplabels=suplabels[max_len:]\n",
        "                if(len(supdata)<max_len):\n",
        "                    data.append(supdata)\n",
        "                    labels.append(suplabels)\n",
        "    return data,labels\n",
        "\n",
        "def get_data(path):\n",
        "    text=read_text(path)\n",
        "    text=preprocess(text)\n",
        "    # size=int(len(text))\n",
        "    # text=text[:size]\n",
        "    text = preprocessing(text)\n",
        "    text=\"\".join(text)\n",
        "    # write_to_file_string(\"test\",\"data.txt\",text)\n",
        "    # text=split_text(text)\n",
        "    text=text.split('.')\n",
        "    print(len(text))\n",
        "    # print(text[:10])\n",
        "\n",
        "    # write_to_file_second(\"test\",\"data.txt\",text)\n",
        "    # # get max length of sentence in text\n",
        "    # maxdata=text.split('\\n')\n",
        "\n",
        "    # max_len=0\n",
        "    # for t in maxdata:\n",
        "    #     if(len(t)>max_len):\n",
        "    #         max_len=len(t)\n",
        "    # print(max_len)\n",
        "\n",
        "    # split text to sentences on all arabic sparatators\n",
        "    # text = split_arabic_sentences_with_stanfordnlp(text)\n",
        "\n",
        "    # Filter out empty strings or whitespace-only sentences\n",
        "    # text = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "    data=[]\n",
        "    labels=[]\n",
        "    for t in text:\n",
        "        d=\"\"\n",
        "        l=[]\n",
        "        d,l=get_data_labels(t)\n",
        "        if(len(d)==0): continue\n",
        "        if(len(d)<max_len):\n",
        "            while(len(d)<max_len):\n",
        "                 d+=\" \"\n",
        "                 l.append(15)\n",
        "            data.append(d)\n",
        "            labels.append(l)\n",
        "            continue\n",
        "        if(len(d)>max_len):\n",
        "            data.append(d[:max_len])\n",
        "            labels.append(l[:max_len])\n",
        "            supdata=d[max_len:]\n",
        "            suplabels=l[max_len:]\n",
        "            while(len(supdata)>max_len):\n",
        "                data.append(supdata[:max_len])\n",
        "                labels.append(suplabels[:max_len])\n",
        "                supdata=supdata[max_len:]\n",
        "                suplabels=suplabels[max_len:]\n",
        "            if(len(supdata)<max_len):\n",
        "                while(len(supdata)<max_len):\n",
        "                    supdata+=\" \"\n",
        "                    suplabels.append(15)\n",
        "                data.append(supdata)\n",
        "                labels.append(suplabels)\n",
        "        # data.append(d)\n",
        "        # labels.append(l)\n",
        "    print(\"first sententence :\" , data[0])\n",
        "    # for d in data:\n",
        "    #     write_to_file_string(\"test\",\"data.txt\",d)\n",
        "    return data,labels\n",
        "\n",
        "def get_features(data,labels):\n",
        "    encoded_data = torch.empty(0, max_len, len(arabic_letters),dtype=torch.float32).to(device)\n",
        "    for d in data:\n",
        "        enc = torch.empty(0, len(arabic_letters),dtype=torch.float32).to(device)\n",
        "        for letter in d:\n",
        "            x = encoding(letter).unsqueeze(0).to(device)\n",
        "            enc = torch.cat((enc, x), 0)\n",
        "        encoded_data = torch.cat((encoded_data, enc.unsqueeze(0)), 0)\n",
        "    # print(encoded_data.shape)\n",
        "    encoding_labels=torch.tensor(labels,dtype=torch.long).to(device)\n",
        "    # print(encoding_labels.shape)\n",
        "    return encoded_data,encoding_labels\n",
        "\n",
        "def get_word2vec_features(data, labels, model):\n",
        "    max_seq_length = 300\n",
        "    encoded_data = torch.empty(0, max_len, max_seq_length, dtype=torch.float32)\n",
        "\n",
        "    for sentence in data:\n",
        "        encoded_sentence = torch.empty(0, max_seq_length, dtype=torch.float32)\n",
        "        for letter in sentence:\n",
        "            if letter in model.wv:\n",
        "                # Convert NumPy array to PyTorch tensor\n",
        "                 embedding  = torch.tensor(model.wv[letter], dtype=torch.float32).unsqueeze(0)\n",
        "            else:\n",
        "                # If the word is not in the model's vocabulary, fill with zeros\n",
        "                embedding  = torch.zeros((1, max_seq_length), dtype=torch.float32)\n",
        "\n",
        "            encoded_sentence = torch.cat((encoded_sentence, embedding), 0)\n",
        "\n",
        "        encoded_data = torch.cat((encoded_data, encoded_sentence.unsqueeze(0)), 0)\n",
        "\n",
        "    encoding_labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return encoded_data, encoding_labels\n",
        "\n",
        "\n",
        "def get_tf_idf_features(data, labels):\n",
        "    # Create a one-hot encoding tensor with the same dimensions as Word2Vec\n",
        "    encoded_data = torch.empty(0, max_len, len(arabic_letters), dtype=torch.float32)\n",
        "\n",
        "    # Create a mapping of words to indices for one-hot encoding\n",
        "    word_to_idx = {word: idx for idx, word in enumerate(arabic_letters)}\n",
        "\n",
        "    for sentence in data:\n",
        "        encoded_sentence = torch.empty(0, len(arabic_letters), dtype=torch.float32)\n",
        "\n",
        "        for letter in sentence:\n",
        "            # Use one-hot encoding for each letter\n",
        "            if letter in word_to_idx:\n",
        "                idx = word_to_idx[letter]\n",
        "                one_hot = torch.zeros(len(arabic_letters), dtype=torch.float32)\n",
        "                one_hot[idx] = 1\n",
        "                one_hot = one_hot.unsqueeze(0)\n",
        "            else:\n",
        "                # If the word is not in the vocabulary, fill with zeros\n",
        "                one_hot = torch.zeros(len(arabic_letters), dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "            encoded_sentence = torch.cat((encoded_sentence, one_hot), 0)\n",
        "\n",
        "        encoded_data = torch.cat((encoded_data, encoded_sentence.unsqueeze(0)), 0)\n",
        "\n",
        "    encoding_labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return encoded_data, encoding_labels\n",
        "\n",
        "\n",
        "\n",
        "def get_bow_features(data, labels, max_seq_length=400):\n",
        "    # Create a CountVectorizer\n",
        "    vectorizer = CountVectorizer(analyzer='char', vocabulary=arabic_letters)\n",
        "\n",
        "    # Fit and transform the data\n",
        "    encoded_data = vectorizer.transform(data)\n",
        "\n",
        "    # Convert to a PyTorch sparse tensor with the specified max_seq_length\n",
        "    encoded_data = torch.sparse.FloatTensor(\n",
        "        torch.tensor(encoded_data.nonzero()).t(),\n",
        "        torch.ones(encoded_data.nnz),\n",
        "        (len(data), max_seq_length, len(arabic_letters))\n",
        "    )\n",
        "\n",
        "    encoding_labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return encoded_data, encoding_labels\n",
        "\n",
        "class DataSet():\n",
        "\n",
        "    def __init__(self,path,batch_size=1,test=False) :\n",
        "        print(\"Loading data...\")\n",
        "        if(test):\n",
        "            data1,labels1=get_test(path)\n",
        "        else:\n",
        "            data1,labels1=get_data(path)\n",
        "        # now labels is list of list [[1,2,3,4,5,15,15,0],[1,2,3,4,5,15,15,0]]\n",
        "        # data is list of string ['احمد','محمد']\n",
        "        print(\"Extracting features...\")\n",
        "        data,labels=get_features(data1,labels1)\n",
        "        # data,labels=get_word2vec_features(data1,labels1,word_embed_model)\n",
        "        print(data.shape)\n",
        "        # now the data and labels are tensor\n",
        "        # data is tensor of shape (number of sentences,max_len,37)\n",
        "        # labels is tensor of shape (number of sentences,max_len)\n",
        "        print(\"Creating dataloader...\")\n",
        "\n",
        "        dataloader=get_dataloader(data,labels,batch_size)\n",
        "        self.x=data1\n",
        "        self.y=labels1\n",
        "        self.dataloader=dataloader\n",
        "        print(\"Done data creation !\")\n",
        "\n",
        "    def __len__(self):\n",
        "         return len(self.y)\n",
        "    def item(self,idx):\n",
        "         return self.x[idx],self.y[idx]\n",
        "    def getdata(self):\n",
        "        return self.dataloader\n",
        "    def getx(self):\n",
        "        return self.x\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C5LvCIPR2eVx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch import optim\n",
        "from numpy import vstack\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "def write_to_file_string(dirctory,file_path, text):\n",
        "    if not os.path.exists(dirctory):\n",
        "        os.makedirs(dirctory)\n",
        "\n",
        "    with open(dirctory+\"/\"+file_path, \"a\", encoding=\"utf-8\") as file:\n",
        "        lines=text\n",
        "        file.write(lines)\n",
        "        file.write('\\n')\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, inp_vocab_size: int, hidden_dim: int = 256, seq_len: int = 600, num_classes: int = 16):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(inp_vocab_size, hidden_dim,num_layers=3, batch_first=True, bidirectional=True)\n",
        "        # self.lstm = nn.LSTM(inp_vocab_size, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # Output layer for 0 to 16 integers\n",
        "\n",
        "    def forward(self, input_sequence: torch.Tensor):\n",
        "        output, _ = self.lstm(input_sequence)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "class GRU(nn.Module):\n",
        "    def __init__(self, inp_vocab_size: int, hidden_dim: int = 256, seq_len: int = 600, num_classes: int = 16):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(inp_vocab_size, hidden_dim,num_layers=3, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # Output layer for 0 to 16 integers\n",
        "\n",
        "    def forward(self, input_sequence: torch.Tensor):\n",
        "        output, _ = self.gru(input_sequence)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "def train(train_dl, model):\n",
        "    # define the optimization\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "    model.to(device)\n",
        "    # enumerate epochs\n",
        "    for epoch in range(20):\n",
        "        for i, (inputs, targets) in enumerate(train_dl):\n",
        "            # convert the input and target to tensor\n",
        "            # clear the gradients\n",
        "            optimizer.zero_grad()\n",
        "            # compute the model output\n",
        "            yhat = model(inputs.to(device))\n",
        "            yhat = yhat.view(-1, yhat.size(2))  # Reshape model output to [batch_size * sequence_length, num_classes]\n",
        "            targets = targets.view(-1)  # Reshape targets to [batch_size * sequence_length]\n",
        "            # calculate loss\n",
        "            while len(targets)>0 and targets[-1]==15:\n",
        "              targets=targets[:-1]\n",
        "              yhat=yhat[:-1]\n",
        "            loss = criterion(yhat, targets.to(device))\n",
        "            loss.backward()\n",
        "            # update model weights\n",
        "            optimizer.step()\n",
        "            print(f'epoch {epoch} batch {i} loss {loss.item()}')\n",
        "            # break\n",
        "    # save model to file after training\n",
        "    torch.save(model.state_dict(), 'model.pth')\n",
        "\n",
        "\n",
        "def calculate_DER(actual_labels, predicted_labels):\n",
        "    # Convert lists to PyTorch tensors if they are not already\n",
        "    if not isinstance(actual_labels, torch.Tensor):\n",
        "        actual_labels = torch.tensor(actual_labels)\n",
        "    if not isinstance(predicted_labels, torch.Tensor):\n",
        "        predicted_labels = torch.tensor(predicted_labels)\n",
        "\n",
        "    # Check if the lengths of both label sequences match\n",
        "    if len(actual_labels) != len(predicted_labels):\n",
        "        raise ValueError(\"Lengths of actual and predicted labels should match.\")\n",
        "\n",
        "    total_errors = torch.sum(actual_labels != predicted_labels)\n",
        "    total_frames = len(actual_labels)\n",
        "\n",
        "    # DER calculation\n",
        "    DER = (1-(total_errors / total_frames)) * 100.0\n",
        "    return DER.item()  # Convert PyTorch scalar to Python float\n",
        "\n",
        "\n",
        "def evaluate_model(test_dl, model):\n",
        "    predictions, actuals = [], []\n",
        "    model.to(device)\n",
        "    for i, (inputs, targets) in enumerate(test_dl):\n",
        "        yhat = model(inputs.to(device))\n",
        "        yhat = yhat.detach().cpu().numpy()\n",
        "        # reshape the outputs to [batch_size * sequence_length, num_classes]\n",
        "        yhat = yhat.reshape(-1, yhat.shape[-1])\n",
        "        # get predicted classes\n",
        "        predicted_classes = np.argmax(yhat, axis=1)\n",
        "        # convert targets to numpy array and reshape\n",
        "        targets = targets.cpu().numpy().reshape(-1)\n",
        "        # store predictions and actuals\n",
        "        predictions.extend(predicted_classes.tolist())\n",
        "        actuals.extend(targets.tolist())\n",
        "        # break\n",
        "    # calculate accuracy\n",
        "    acc = calculate_DER(np.array(actuals), np.array(predictions))\n",
        "\n",
        "    return acc\n"
      ],
      "metadata": {
        "id": "qhy617nk2VPI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "print(device)\n",
        "\n",
        "\n",
        "\n",
        "inverted_classes = {v: k for k, v in classes.items()}\n",
        "\n",
        "\n",
        "inp_vocab_size = 37\n",
        "hidden_dim = 128\n",
        "seq_len = 400\n",
        "num_classes = 16\n",
        "\n",
        "model = LSTM(inp_vocab_size, hidden_dim, seq_len, num_classes)\n",
        "# model.load_state_dict(torch.load(\"LSTM98.pth\", map_location=torch.device(\"cpu\")))\n",
        "# model.eval()\n",
        "\n",
        "Traindata = DataSet( \"/content/drive/MyDrive/NLP project/Arabic-text-to-diacritics/Dataset/train.txt\", batch_size = 256 )\n",
        "Traindataloader = Traindata.getdata()\n",
        "\n",
        "model = LSTM(inp_vocab_size, hidden_dim, seq_len, num_classes)\n",
        "print(\"-------------------start training-------------------\")\n",
        "train(Traindataloader, model)\n",
        "\n",
        "\n",
        "validatindata=DataSet(\"/content/drive/MyDrive/NLP project/Arabic-text-to-diacritics/Dataset/val.txt\",batch_size=1)\n",
        "validatindataloader=validatindata.getdata()\n",
        "print(\"-------------------start evaluating-------------------\")\n",
        "\n",
        "\n",
        "predictions = []\n",
        "actuals=[]\n",
        "def evaluate_model(test_dl, model):\n",
        "    predicted_classes = []\n",
        "    model.to(device)\n",
        "    for i, (inputs, targets) in enumerate(test_dl):\n",
        "        yhat = model(inputs.to(device))\n",
        "        yhat = yhat.detach().cpu().numpy()\n",
        "        # reshape the outputs to [batch_size * sequence_length, num_classes]\n",
        "        yhat = yhat.reshape(-1, yhat.shape[-1])\n",
        "        # get predicted classes\n",
        "        predicted_classes = np.argmax(yhat, axis=1)\n",
        "        # convert targets to numpy array and reshape\n",
        "        targets = targets.cpu().numpy().reshape(-1)\n",
        "        while len(targets)>0 and targets[-1]==15:\n",
        "            targets=targets[:-1]\n",
        "            predicted_classes=predicted_classes[:-1]\n",
        "        # store predictions and actuals\n",
        "        predictions.extend(predicted_classes)\n",
        "        actuals.extend(targets.tolist())\n",
        "\n",
        "data=validatindata.getx()\n",
        "\n",
        "\n",
        "evaluate_model(validatindataloader,model)\n",
        "result = \"\"\n",
        "for i in range(len(data)):\n",
        "    for j in range(len(data[i])):\n",
        "        result += data[i][j]\n",
        "        result += inverted_classes[predictions[i*10+j]]\n",
        "\n",
        "# print(\"Reversed string: \", result[::-1])\n",
        "\n",
        "# print(\"Predicted classes: \", predictions)\n",
        "actuals_res=[]\n",
        "predictions_res=[]\n",
        "for i in range(len(actuals)):\n",
        "  if actuals[i]!=15:\n",
        "    actuals_res.append(actuals[i])\n",
        "    predictions_res.append(predictions[i])\n",
        "predicted_classes_csv = [(i, label) for i, label in enumerate(predictions_res)]\n",
        "\n",
        "id_line_letter_df_after = pd.DataFrame(\n",
        "    {\n",
        "        \"id\": [info[0] for info in predicted_classes_csv],\n",
        "        \"label\": [info[1] for info in predicted_classes_csv],\n",
        "    }\n",
        ")\n",
        "id_line_letter_df_after.to_csv(\"predicted_chars.csv\", index=False)\n",
        "\n",
        "\n",
        "\n",
        "def calculate_DER(actual_labels, predicted_labels):\n",
        "    # Convert lists to PyTorch tensors if they are not already\n",
        "    if not isinstance(actual_labels, torch.Tensor):\n",
        "        actual_labels = torch.tensor(actual_labels)\n",
        "    if not isinstance(predicted_labels, torch.Tensor):\n",
        "        predicted_labels = torch.tensor(predicted_labels)\n",
        "\n",
        "    # Check if the lengths of both label sequences match\n",
        "    if len(actual_labels) != len(predicted_labels):\n",
        "        raise ValueError(\"Lengths of actual and predicted labels should match.\")\n",
        "\n",
        "    total_errors = torch.sum(actual_labels != predicted_labels)\n",
        "    total_frames = len(actual_labels)\n",
        "\n",
        "    # DER calculation\n",
        "    DER = (1-(total_errors / total_frames)) * 100.0\n",
        "    return DER.item()  # Convert PyTorch scalar to Python float\n",
        "\n",
        "# predictions=[x for x in predictions if x!=15]\n",
        "# actuals=[x for x in actuals if x!=15]\n",
        "\n",
        "acc = calculate_DER(np.array(actuals_res), np.array(predictions_res))\n",
        "print(\"Accuracy: \", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqgXGly72RLu",
        "outputId": "f601eebd-b58e-463b-f6aa-f434b4aa4007"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Loading data...\n",
            "40836\n",
            "first sententence : قوله أو قطع الأول يده إلخ قال الزركشي ابن عرفة قوله بلفظ يقتضيه كإنكار غير حديث بالإسلام وجوب ما علم وجوبه من الدين ضرورة كإلقاء مصحف بقذر وشد زنار ابن عرفة قول ابن شاس أو بفعل يتضمنه هو كلبس الزنار وإلقاء المصحف في صريح النجاسة والسجود للصنم ونحو ذلك وسحر محمد قول مالك وأصحابه أن الساحر كافر بالله \n",
            "Extracting features...\n",
            "torch.Size([58948, 300, 37])\n",
            "Creating dataloader...\n",
            "Done data creation !\n",
            "-------------------start training-------------------\n",
            "epoch 0 batch 0 loss 2.751662492752075\n",
            "epoch 0 batch 1 loss 1.8202855587005615\n",
            "epoch 0 batch 2 loss 2.3587400913238525\n",
            "epoch 0 batch 3 loss 1.8088301420211792\n",
            "epoch 0 batch 4 loss 1.8949382305145264\n",
            "epoch 0 batch 5 loss 1.8655840158462524\n",
            "epoch 0 batch 6 loss 1.7213351726531982\n",
            "epoch 0 batch 7 loss 1.6065757274627686\n",
            "epoch 0 batch 8 loss 1.6278514862060547\n",
            "epoch 0 batch 9 loss 1.721007227897644\n",
            "epoch 0 batch 10 loss 1.7512080669403076\n",
            "epoch 0 batch 11 loss 1.5572909116744995\n",
            "epoch 0 batch 12 loss 1.508711576461792\n",
            "epoch 0 batch 13 loss 1.5656561851501465\n",
            "epoch 0 batch 14 loss 1.6301487684249878\n",
            "epoch 0 batch 15 loss 1.6221630573272705\n",
            "epoch 0 batch 16 loss 1.5872443914413452\n",
            "epoch 0 batch 17 loss 1.5243552923202515\n",
            "epoch 0 batch 18 loss 1.5686486959457397\n",
            "epoch 0 batch 19 loss 1.5696499347686768\n",
            "epoch 0 batch 20 loss 1.629144310951233\n",
            "epoch 0 batch 21 loss 1.4876335859298706\n",
            "epoch 0 batch 22 loss 1.4905080795288086\n",
            "epoch 0 batch 23 loss 1.4971611499786377\n",
            "epoch 0 batch 24 loss 1.4214308261871338\n",
            "epoch 0 batch 25 loss 1.3425617218017578\n",
            "epoch 0 batch 26 loss 1.2604262828826904\n",
            "epoch 0 batch 27 loss 1.2552690505981445\n",
            "epoch 0 batch 28 loss 1.233773112297058\n",
            "epoch 0 batch 29 loss 1.19898521900177\n",
            "epoch 0 batch 30 loss 1.1950243711471558\n",
            "epoch 0 batch 31 loss 1.202217936515808\n",
            "epoch 0 batch 32 loss 1.1475480794906616\n",
            "epoch 0 batch 33 loss 1.121527075767517\n",
            "epoch 0 batch 34 loss 1.0725420713424683\n",
            "epoch 0 batch 35 loss 1.0591063499450684\n",
            "epoch 0 batch 36 loss 1.235918402671814\n",
            "epoch 0 batch 37 loss 1.0628055334091187\n",
            "epoch 0 batch 38 loss 1.0478627681732178\n",
            "epoch 0 batch 39 loss 1.0763171911239624\n",
            "epoch 0 batch 40 loss 1.0393716096878052\n",
            "epoch 0 batch 41 loss 0.9506404995918274\n",
            "epoch 0 batch 42 loss 0.8369265794754028\n",
            "epoch 0 batch 43 loss 0.8617339134216309\n",
            "epoch 0 batch 44 loss 0.8538190126419067\n",
            "epoch 0 batch 45 loss 0.7796059846878052\n",
            "epoch 0 batch 46 loss 0.8114237189292908\n",
            "epoch 0 batch 47 loss 0.8104123473167419\n",
            "epoch 0 batch 48 loss 0.7765595316886902\n",
            "epoch 0 batch 49 loss 0.8115592002868652\n",
            "epoch 0 batch 50 loss 0.7916915416717529\n",
            "epoch 0 batch 51 loss 0.7356503009796143\n",
            "epoch 0 batch 52 loss 0.7250681519508362\n",
            "epoch 0 batch 53 loss 0.7575973868370056\n",
            "epoch 0 batch 54 loss 0.7429147958755493\n",
            "epoch 0 batch 55 loss 0.7543903589248657\n",
            "epoch 0 batch 56 loss 0.6478975415229797\n",
            "epoch 0 batch 57 loss 0.6756333708763123\n",
            "epoch 0 batch 58 loss 0.6946167945861816\n",
            "epoch 0 batch 59 loss 0.6262312531471252\n",
            "epoch 0 batch 60 loss 0.6402055621147156\n",
            "epoch 0 batch 61 loss 0.6276530623435974\n",
            "epoch 0 batch 62 loss 0.5847225189208984\n",
            "epoch 0 batch 63 loss 0.6013365387916565\n",
            "epoch 0 batch 64 loss 0.5692926049232483\n",
            "epoch 0 batch 65 loss 0.5616360306739807\n",
            "epoch 0 batch 66 loss 0.5856810212135315\n",
            "epoch 0 batch 67 loss 0.588121771812439\n",
            "epoch 0 batch 68 loss 0.558161735534668\n",
            "epoch 0 batch 69 loss 0.5458594560623169\n",
            "epoch 0 batch 70 loss 0.5514653325080872\n",
            "epoch 0 batch 71 loss 0.5006240606307983\n",
            "epoch 0 batch 72 loss 0.5316511392593384\n",
            "epoch 0 batch 73 loss 0.525047779083252\n",
            "epoch 0 batch 74 loss 0.5095944404602051\n",
            "epoch 0 batch 75 loss 0.5181640982627869\n",
            "epoch 0 batch 76 loss 0.46185794472694397\n",
            "epoch 0 batch 77 loss 0.5107048153877258\n",
            "epoch 0 batch 78 loss 0.5226615071296692\n",
            "epoch 0 batch 79 loss 0.4830985963344574\n",
            "epoch 0 batch 80 loss 0.48164889216423035\n",
            "epoch 0 batch 81 loss 0.48881539702415466\n",
            "epoch 0 batch 82 loss 0.5012557506561279\n",
            "epoch 0 batch 83 loss 0.44344955682754517\n",
            "epoch 0 batch 84 loss 0.4655216634273529\n",
            "epoch 0 batch 85 loss 0.46765491366386414\n",
            "epoch 0 batch 86 loss 0.43046480417251587\n",
            "epoch 0 batch 87 loss 0.4429589807987213\n",
            "epoch 0 batch 88 loss 0.42936739325523376\n",
            "epoch 0 batch 89 loss 0.44563010334968567\n",
            "epoch 0 batch 90 loss 0.4306565821170807\n",
            "epoch 0 batch 91 loss 0.44148972630500793\n",
            "epoch 0 batch 92 loss 0.4332142770290375\n",
            "epoch 0 batch 93 loss 0.43168917298316956\n",
            "epoch 0 batch 94 loss 0.424667626619339\n",
            "epoch 0 batch 95 loss 0.4382818639278412\n",
            "epoch 0 batch 96 loss 0.4196709990501404\n",
            "epoch 0 batch 97 loss 0.4183540940284729\n",
            "epoch 0 batch 98 loss 0.39015522599220276\n",
            "epoch 0 batch 99 loss 0.4119194746017456\n",
            "epoch 0 batch 100 loss 0.419977068901062\n",
            "epoch 0 batch 101 loss 0.3757021725177765\n",
            "epoch 0 batch 102 loss 0.3850593566894531\n",
            "epoch 0 batch 103 loss 0.41638314723968506\n",
            "epoch 0 batch 104 loss 0.38054031133651733\n",
            "epoch 0 batch 105 loss 0.35574907064437866\n",
            "epoch 0 batch 106 loss 0.3884919583797455\n",
            "epoch 0 batch 107 loss 0.3781295716762543\n",
            "epoch 0 batch 108 loss 0.36600568890571594\n",
            "epoch 0 batch 109 loss 0.36850470304489136\n",
            "epoch 0 batch 110 loss 0.336473673582077\n",
            "epoch 0 batch 111 loss 0.3369171917438507\n",
            "epoch 0 batch 112 loss 0.364176481962204\n",
            "epoch 0 batch 113 loss 0.36351361870765686\n",
            "epoch 0 batch 114 loss 0.3432759940624237\n",
            "epoch 0 batch 115 loss 0.3384275436401367\n",
            "epoch 0 batch 116 loss 0.3360825479030609\n",
            "epoch 0 batch 117 loss 0.3498985767364502\n",
            "epoch 0 batch 118 loss 0.3300468623638153\n",
            "epoch 0 batch 119 loss 0.34350425004959106\n",
            "epoch 0 batch 120 loss 0.32611867785453796\n",
            "epoch 0 batch 121 loss 0.32511472702026367\n",
            "epoch 0 batch 122 loss 0.33641842007637024\n",
            "epoch 0 batch 123 loss 0.32468751072883606\n",
            "epoch 0 batch 124 loss 0.32521453499794006\n",
            "epoch 0 batch 125 loss 0.3177631199359894\n",
            "epoch 0 batch 126 loss 0.3000306785106659\n",
            "epoch 0 batch 127 loss 0.3162664771080017\n",
            "epoch 0 batch 128 loss 0.29629063606262207\n",
            "epoch 0 batch 129 loss 0.3078683018684387\n",
            "epoch 0 batch 130 loss 0.31452181935310364\n",
            "epoch 0 batch 131 loss 0.3097684681415558\n",
            "epoch 0 batch 132 loss 0.30118075013160706\n",
            "epoch 0 batch 133 loss 0.28987589478492737\n",
            "epoch 0 batch 134 loss 0.2953282594680786\n",
            "epoch 0 batch 135 loss 0.28779590129852295\n",
            "epoch 0 batch 136 loss 0.2827145755290985\n",
            "epoch 0 batch 137 loss 0.28615376353263855\n",
            "epoch 0 batch 138 loss 0.28541040420532227\n",
            "epoch 0 batch 139 loss 0.2799789309501648\n",
            "epoch 0 batch 140 loss 0.2950843274593353\n",
            "epoch 0 batch 141 loss 0.25905951857566833\n",
            "epoch 0 batch 142 loss 0.25964194536209106\n",
            "epoch 0 batch 143 loss 0.25147566199302673\n",
            "epoch 0 batch 144 loss 0.27310794591903687\n",
            "epoch 0 batch 145 loss 0.262919545173645\n",
            "epoch 0 batch 146 loss 0.27266907691955566\n",
            "epoch 0 batch 147 loss 0.280123770236969\n",
            "epoch 0 batch 148 loss 0.2811118960380554\n",
            "epoch 0 batch 149 loss 0.24159613251686096\n",
            "epoch 0 batch 150 loss 0.2722696363925934\n",
            "epoch 0 batch 151 loss 0.26495885848999023\n",
            "epoch 0 batch 152 loss 0.2520442306995392\n",
            "epoch 0 batch 153 loss 0.2489853948354721\n",
            "epoch 0 batch 154 loss 0.2459365576505661\n",
            "epoch 0 batch 155 loss 0.2546222507953644\n",
            "epoch 0 batch 156 loss 0.2544656991958618\n",
            "epoch 0 batch 157 loss 0.24168290197849274\n",
            "epoch 0 batch 158 loss 0.24531428515911102\n",
            "epoch 0 batch 159 loss 0.22927945852279663\n",
            "epoch 0 batch 160 loss 0.2412097156047821\n",
            "epoch 0 batch 161 loss 0.24263693392276764\n",
            "epoch 0 batch 162 loss 0.25327324867248535\n",
            "epoch 0 batch 163 loss 0.2299615740776062\n",
            "epoch 0 batch 164 loss 0.2524450719356537\n",
            "epoch 0 batch 165 loss 0.24171064794063568\n",
            "epoch 0 batch 166 loss 0.25378793478012085\n",
            "epoch 0 batch 167 loss 0.23402759432792664\n",
            "epoch 0 batch 168 loss 0.26422327756881714\n",
            "epoch 0 batch 169 loss 0.2377946525812149\n",
            "epoch 0 batch 170 loss 0.22851210832595825\n",
            "epoch 0 batch 171 loss 0.23875178396701813\n",
            "epoch 0 batch 172 loss 0.23437729477882385\n",
            "epoch 0 batch 173 loss 0.23487307131290436\n",
            "epoch 0 batch 174 loss 0.22900418937206268\n",
            "epoch 0 batch 175 loss 0.22662201523780823\n",
            "epoch 0 batch 176 loss 0.22876383364200592\n",
            "epoch 0 batch 177 loss 0.2134423702955246\n",
            "epoch 0 batch 178 loss 0.21351151168346405\n",
            "epoch 0 batch 179 loss 0.23248669505119324\n",
            "epoch 0 batch 180 loss 0.22883331775665283\n",
            "epoch 0 batch 181 loss 0.2376524657011032\n",
            "epoch 0 batch 182 loss 0.24113838374614716\n",
            "epoch 0 batch 183 loss 0.23939664661884308\n",
            "epoch 0 batch 184 loss 0.24030998349189758\n",
            "epoch 0 batch 185 loss 0.22925592958927155\n",
            "epoch 0 batch 186 loss 0.22641710937023163\n",
            "epoch 0 batch 187 loss 0.22100123763084412\n",
            "epoch 0 batch 188 loss 0.20669403672218323\n",
            "epoch 0 batch 189 loss 0.215371772646904\n",
            "epoch 0 batch 190 loss 0.21131083369255066\n",
            "epoch 0 batch 191 loss 0.2209486961364746\n",
            "epoch 0 batch 192 loss 0.1878518909215927\n",
            "epoch 0 batch 193 loss 0.20537292957305908\n",
            "epoch 0 batch 194 loss 0.20480570197105408\n",
            "epoch 0 batch 195 loss 0.20419754087924957\n",
            "epoch 0 batch 196 loss 0.21662484109401703\n",
            "epoch 0 batch 197 loss 0.20362521708011627\n",
            "epoch 0 batch 198 loss 0.1951523721218109\n",
            "epoch 0 batch 199 loss 0.21605966985225677\n",
            "epoch 0 batch 200 loss 0.19650568068027496\n",
            "epoch 0 batch 201 loss 0.2054876685142517\n",
            "epoch 0 batch 202 loss 0.188020259141922\n",
            "epoch 0 batch 203 loss 0.1886335015296936\n",
            "epoch 0 batch 204 loss 0.18658718466758728\n",
            "epoch 0 batch 205 loss 0.18507350981235504\n",
            "epoch 0 batch 206 loss 0.16994886100292206\n",
            "epoch 0 batch 207 loss 0.19028086960315704\n",
            "epoch 0 batch 208 loss 0.20861458778381348\n",
            "epoch 0 batch 209 loss 0.18729820847511292\n",
            "epoch 0 batch 210 loss 0.2145158350467682\n",
            "epoch 0 batch 211 loss 0.18738804757595062\n",
            "epoch 0 batch 212 loss 0.18984565138816833\n",
            "epoch 0 batch 213 loss 0.17746679484844208\n",
            "epoch 0 batch 214 loss 0.1938508152961731\n",
            "epoch 0 batch 215 loss 0.17233450710773468\n",
            "epoch 0 batch 216 loss 0.17609021067619324\n",
            "epoch 0 batch 217 loss 0.15870364010334015\n",
            "epoch 0 batch 218 loss 0.1717388778924942\n",
            "epoch 0 batch 219 loss 0.17859581112861633\n",
            "epoch 0 batch 220 loss 0.18209539353847504\n",
            "epoch 0 batch 221 loss 0.17998754978179932\n",
            "epoch 0 batch 222 loss 0.18392768502235413\n",
            "epoch 0 batch 223 loss 0.18240046501159668\n",
            "epoch 0 batch 224 loss 0.18117593228816986\n",
            "epoch 0 batch 225 loss 0.17547962069511414\n",
            "epoch 0 batch 226 loss 0.17982928454875946\n",
            "epoch 0 batch 227 loss 0.17479494214057922\n",
            "epoch 0 batch 228 loss 0.16090214252471924\n",
            "epoch 0 batch 229 loss 0.17634844779968262\n",
            "epoch 0 batch 230 loss 0.1984243541955948\n",
            "epoch 1 batch 0 loss 0.178890660405159\n",
            "epoch 1 batch 1 loss 0.18063868582248688\n",
            "epoch 1 batch 2 loss 0.19290217757225037\n",
            "epoch 1 batch 3 loss 0.18880923092365265\n",
            "epoch 1 batch 4 loss 0.16858917474746704\n",
            "epoch 1 batch 5 loss 0.16393029689788818\n",
            "epoch 1 batch 6 loss 0.16923367977142334\n",
            "epoch 1 batch 7 loss 0.18291231989860535\n",
            "epoch 1 batch 8 loss 0.17232422530651093\n",
            "epoch 1 batch 9 loss 0.16827858984470367\n",
            "epoch 1 batch 10 loss 0.1940416693687439\n",
            "epoch 1 batch 11 loss 0.15442711114883423\n",
            "epoch 1 batch 12 loss 0.15399353206157684\n",
            "epoch 1 batch 13 loss 0.17563965916633606\n",
            "epoch 1 batch 14 loss 0.1625073403120041\n",
            "epoch 1 batch 15 loss 0.16304343938827515\n",
            "epoch 1 batch 16 loss 0.15859757363796234\n",
            "epoch 1 batch 17 loss 0.15860643982887268\n",
            "epoch 1 batch 18 loss 0.15890049934387207\n",
            "epoch 1 batch 19 loss 0.16601116955280304\n",
            "epoch 1 batch 20 loss 0.16928227245807648\n",
            "epoch 1 batch 21 loss 0.15965454280376434\n",
            "epoch 1 batch 22 loss 0.15047641098499298\n",
            "epoch 1 batch 23 loss 0.15742084383964539\n",
            "epoch 1 batch 24 loss 0.15848563611507416\n",
            "epoch 1 batch 25 loss 0.1644384115934372\n",
            "epoch 1 batch 26 loss 0.15286363661289215\n",
            "epoch 1 batch 27 loss 0.16319866478443146\n",
            "epoch 1 batch 28 loss 0.13746702671051025\n",
            "epoch 1 batch 29 loss 0.14907093346118927\n",
            "epoch 1 batch 30 loss 0.15345577895641327\n",
            "epoch 1 batch 31 loss 0.15333998203277588\n",
            "epoch 1 batch 32 loss 0.14758391678333282\n",
            "epoch 1 batch 33 loss 0.14804638922214508\n",
            "epoch 1 batch 34 loss 0.14071881771087646\n",
            "epoch 1 batch 35 loss 0.14027823507785797\n",
            "epoch 1 batch 36 loss 0.1659458577632904\n",
            "epoch 1 batch 37 loss 0.15443435311317444\n",
            "epoch 1 batch 38 loss 0.14852754771709442\n",
            "epoch 1 batch 39 loss 0.14798897504806519\n",
            "epoch 1 batch 40 loss 0.14950186014175415\n",
            "epoch 1 batch 41 loss 0.1453416496515274\n",
            "epoch 1 batch 42 loss 0.13799557089805603\n",
            "epoch 1 batch 43 loss 0.1497671902179718\n",
            "epoch 1 batch 44 loss 0.15398801863193512\n",
            "epoch 1 batch 45 loss 0.1415107101202011\n",
            "epoch 1 batch 46 loss 0.14575277268886566\n",
            "epoch 1 batch 47 loss 0.15363137423992157\n",
            "epoch 1 batch 48 loss 0.14262761175632477\n",
            "epoch 1 batch 49 loss 0.14610125124454498\n",
            "epoch 1 batch 50 loss 0.1445772349834442\n",
            "epoch 1 batch 51 loss 0.14761067926883698\n",
            "epoch 1 batch 52 loss 0.1412661075592041\n",
            "epoch 1 batch 53 loss 0.14079692959785461\n",
            "epoch 1 batch 54 loss 0.14575041830539703\n",
            "epoch 1 batch 55 loss 0.14379197359085083\n",
            "epoch 1 batch 56 loss 0.12728384137153625\n",
            "epoch 1 batch 57 loss 0.14453519880771637\n",
            "epoch 1 batch 58 loss 0.15062540769577026\n",
            "epoch 1 batch 59 loss 0.13406458497047424\n",
            "epoch 1 batch 60 loss 0.13868534564971924\n",
            "epoch 1 batch 61 loss 0.1415000557899475\n",
            "epoch 1 batch 62 loss 0.13191579282283783\n",
            "epoch 1 batch 63 loss 0.1398823857307434\n",
            "epoch 1 batch 64 loss 0.14690344035625458\n",
            "epoch 1 batch 65 loss 0.13274067640304565\n",
            "epoch 1 batch 66 loss 0.14248532056808472\n",
            "epoch 1 batch 67 loss 0.15116415917873383\n",
            "epoch 1 batch 68 loss 0.1426473706960678\n",
            "epoch 1 batch 69 loss 0.13922980427742004\n",
            "epoch 1 batch 70 loss 0.14414763450622559\n",
            "epoch 1 batch 71 loss 0.12387781590223312\n",
            "epoch 1 batch 72 loss 0.13842715322971344\n",
            "epoch 1 batch 73 loss 0.1414075344800949\n",
            "epoch 1 batch 74 loss 0.13771116733551025\n",
            "epoch 1 batch 75 loss 0.13696813583374023\n",
            "epoch 1 batch 76 loss 0.11704932153224945\n",
            "epoch 1 batch 77 loss 0.137979656457901\n",
            "epoch 1 batch 78 loss 0.13835790753364563\n",
            "epoch 1 batch 79 loss 0.13009235262870789\n",
            "epoch 1 batch 80 loss 0.1351117044687271\n",
            "epoch 1 batch 81 loss 0.1349588930606842\n",
            "epoch 1 batch 82 loss 0.14351136982440948\n",
            "epoch 1 batch 83 loss 0.12279078364372253\n",
            "epoch 1 batch 84 loss 0.12663492560386658\n",
            "epoch 1 batch 85 loss 0.1328582763671875\n",
            "epoch 1 batch 86 loss 0.11837198585271835\n",
            "epoch 1 batch 87 loss 0.1264304220676422\n",
            "epoch 1 batch 88 loss 0.12181192636489868\n",
            "epoch 1 batch 89 loss 0.13639701902866364\n",
            "epoch 1 batch 90 loss 0.13048575818538666\n",
            "epoch 1 batch 91 loss 0.13177454471588135\n",
            "epoch 1 batch 92 loss 0.12856344878673553\n",
            "epoch 1 batch 93 loss 0.13092559576034546\n",
            "epoch 1 batch 94 loss 0.12980186939239502\n",
            "epoch 1 batch 95 loss 0.13844014704227448\n",
            "epoch 1 batch 96 loss 0.12824198603630066\n",
            "epoch 1 batch 97 loss 0.1267414689064026\n",
            "epoch 1 batch 98 loss 0.11852438002824783\n",
            "epoch 1 batch 99 loss 0.1352955549955368\n",
            "epoch 1 batch 100 loss 0.13698647916316986\n",
            "epoch 1 batch 101 loss 0.1189718022942543\n",
            "epoch 1 batch 102 loss 0.12303666770458221\n",
            "epoch 1 batch 103 loss 0.13509993255138397\n",
            "epoch 1 batch 104 loss 0.1250797063112259\n",
            "epoch 1 batch 105 loss 0.11694697290658951\n",
            "epoch 1 batch 106 loss 0.12241052836179733\n",
            "epoch 1 batch 107 loss 0.1243145763874054\n",
            "epoch 1 batch 108 loss 0.1255737841129303\n",
            "epoch 1 batch 109 loss 0.1231767013669014\n",
            "epoch 1 batch 110 loss 0.1164700984954834\n",
            "epoch 1 batch 111 loss 0.11727170646190643\n",
            "epoch 1 batch 112 loss 0.130838543176651\n",
            "epoch 1 batch 113 loss 0.12986235320568085\n",
            "epoch 1 batch 114 loss 0.11760924756526947\n",
            "epoch 1 batch 115 loss 0.1292877048254013\n",
            "epoch 1 batch 116 loss 0.12255682796239853\n",
            "epoch 1 batch 117 loss 0.12909062206745148\n",
            "epoch 1 batch 118 loss 0.12146750092506409\n",
            "epoch 1 batch 119 loss 0.12302013486623764\n",
            "epoch 1 batch 120 loss 0.11515685170888901\n",
            "epoch 1 batch 121 loss 0.12276158481836319\n",
            "epoch 1 batch 122 loss 0.12931139767169952\n",
            "epoch 1 batch 123 loss 0.11788450926542282\n",
            "epoch 1 batch 124 loss 0.12395793199539185\n",
            "epoch 1 batch 125 loss 0.11831244826316833\n",
            "epoch 1 batch 126 loss 0.11185691505670547\n",
            "epoch 1 batch 127 loss 0.12412074953317642\n",
            "epoch 1 batch 128 loss 0.11328830569982529\n",
            "epoch 1 batch 129 loss 0.11989450454711914\n",
            "epoch 1 batch 130 loss 0.13402460515499115\n",
            "epoch 1 batch 131 loss 0.12339536845684052\n",
            "epoch 1 batch 132 loss 0.12307358533143997\n",
            "epoch 1 batch 133 loss 0.11379630863666534\n",
            "epoch 1 batch 134 loss 0.11820726841688156\n",
            "epoch 1 batch 135 loss 0.1214890331029892\n",
            "epoch 1 batch 136 loss 0.11370305716991425\n",
            "epoch 1 batch 137 loss 0.11454377323389053\n",
            "epoch 1 batch 138 loss 0.11769381165504456\n",
            "epoch 1 batch 139 loss 0.11913067102432251\n",
            "epoch 1 batch 140 loss 0.1231052428483963\n",
            "epoch 1 batch 141 loss 0.10621162503957748\n",
            "epoch 1 batch 142 loss 0.10518178343772888\n",
            "epoch 1 batch 143 loss 0.0988481268286705\n",
            "epoch 1 batch 144 loss 0.11144258081912994\n",
            "epoch 1 batch 145 loss 0.11286620795726776\n",
            "epoch 1 batch 146 loss 0.11873334646224976\n",
            "epoch 1 batch 147 loss 0.11703168600797653\n",
            "epoch 1 batch 148 loss 0.11460349708795547\n",
            "epoch 1 batch 149 loss 0.10453031957149506\n",
            "epoch 1 batch 150 loss 0.11465280503034592\n",
            "epoch 1 batch 151 loss 0.11137925833463669\n",
            "epoch 1 batch 152 loss 0.10391250997781754\n",
            "epoch 1 batch 153 loss 0.10734160244464874\n",
            "epoch 1 batch 154 loss 0.11234863847494125\n",
            "epoch 1 batch 155 loss 0.11383238434791565\n",
            "epoch 1 batch 156 loss 0.11314870417118073\n",
            "epoch 1 batch 157 loss 0.1050841212272644\n",
            "epoch 1 batch 158 loss 0.10693277418613434\n",
            "epoch 1 batch 159 loss 0.09749745577573776\n",
            "epoch 1 batch 160 loss 0.10966932773590088\n",
            "epoch 1 batch 161 loss 0.10626375675201416\n",
            "epoch 1 batch 162 loss 0.11664848029613495\n",
            "epoch 1 batch 163 loss 0.10871922969818115\n",
            "epoch 1 batch 164 loss 0.11136338859796524\n",
            "epoch 1 batch 165 loss 0.10881628096103668\n",
            "epoch 1 batch 166 loss 0.12024901062250137\n",
            "epoch 1 batch 167 loss 0.10890530049800873\n",
            "epoch 1 batch 168 loss 0.12185858935117722\n",
            "epoch 1 batch 169 loss 0.11388196796178818\n",
            "epoch 1 batch 170 loss 0.1057189404964447\n",
            "epoch 1 batch 171 loss 0.11493658274412155\n",
            "epoch 1 batch 172 loss 0.11126663535833359\n",
            "epoch 1 batch 173 loss 0.1041501834988594\n",
            "epoch 1 batch 174 loss 0.1083235964179039\n",
            "epoch 1 batch 175 loss 0.1098649874329567\n",
            "epoch 1 batch 176 loss 0.1088704764842987\n",
            "epoch 1 batch 177 loss 0.10027118027210236\n",
            "epoch 1 batch 178 loss 0.10022135078907013\n",
            "epoch 1 batch 179 loss 0.10479024797677994\n",
            "epoch 1 batch 180 loss 0.1078929752111435\n",
            "epoch 1 batch 181 loss 0.10749383270740509\n",
            "epoch 1 batch 182 loss 0.10727927833795547\n",
            "epoch 1 batch 183 loss 0.11473873257637024\n",
            "epoch 1 batch 184 loss 0.11807268112897873\n",
            "epoch 1 batch 185 loss 0.11098121851682663\n",
            "epoch 1 batch 186 loss 0.11753029376268387\n",
            "epoch 1 batch 187 loss 0.10276497155427933\n",
            "epoch 1 batch 188 loss 0.09755104035139084\n",
            "epoch 1 batch 189 loss 0.11056143790483475\n",
            "epoch 1 batch 190 loss 0.10452276468276978\n",
            "epoch 1 batch 191 loss 0.1151861771941185\n",
            "epoch 1 batch 192 loss 0.0928560346364975\n",
            "epoch 1 batch 193 loss 0.10306145250797272\n",
            "epoch 1 batch 194 loss 0.10298413038253784\n",
            "epoch 1 batch 195 loss 0.10304348170757294\n",
            "epoch 1 batch 196 loss 0.11123555898666382\n",
            "epoch 1 batch 197 loss 0.10060414671897888\n",
            "epoch 1 batch 198 loss 0.09906947612762451\n",
            "epoch 1 batch 199 loss 0.11187594383955002\n",
            "epoch 1 batch 200 loss 0.10240687429904938\n",
            "epoch 1 batch 201 loss 0.11043042689561844\n",
            "epoch 1 batch 202 loss 0.09765520691871643\n",
            "epoch 1 batch 203 loss 0.09751887619495392\n",
            "epoch 1 batch 204 loss 0.09317336976528168\n",
            "epoch 1 batch 205 loss 0.10015417635440826\n",
            "epoch 1 batch 206 loss 0.08598779886960983\n",
            "epoch 1 batch 207 loss 0.09724000096321106\n",
            "epoch 1 batch 208 loss 0.11204883456230164\n",
            "epoch 1 batch 209 loss 0.09842605888843536\n",
            "epoch 1 batch 210 loss 0.11298421770334244\n",
            "epoch 1 batch 211 loss 0.09893675893545151\n",
            "epoch 1 batch 212 loss 0.09427987784147263\n",
            "epoch 1 batch 213 loss 0.09516493231058121\n",
            "epoch 1 batch 214 loss 0.10192667692899704\n",
            "epoch 1 batch 215 loss 0.09070620685815811\n",
            "epoch 1 batch 216 loss 0.09731364995241165\n",
            "epoch 1 batch 217 loss 0.08219113200902939\n",
            "epoch 1 batch 218 loss 0.09305345267057419\n",
            "epoch 1 batch 219 loss 0.09913350641727448\n",
            "epoch 1 batch 220 loss 0.09843495488166809\n",
            "epoch 1 batch 221 loss 0.09903448820114136\n",
            "epoch 1 batch 222 loss 0.10303451865911484\n",
            "epoch 1 batch 223 loss 0.10113852471113205\n",
            "epoch 1 batch 224 loss 0.10034941881895065\n",
            "epoch 1 batch 225 loss 0.09994615614414215\n",
            "epoch 1 batch 226 loss 0.09912209212779999\n",
            "epoch 1 batch 227 loss 0.09617328643798828\n",
            "epoch 1 batch 228 loss 0.09237876534461975\n",
            "epoch 1 batch 229 loss 0.09860485047101974\n",
            "epoch 1 batch 230 loss 0.10471341013908386\n",
            "epoch 2 batch 0 loss 0.10269909352064133\n",
            "epoch 2 batch 1 loss 0.09948544949293137\n",
            "epoch 2 batch 2 loss 0.10914459824562073\n",
            "epoch 2 batch 3 loss 0.11043325811624527\n",
            "epoch 2 batch 4 loss 0.09767849743366241\n",
            "epoch 2 batch 5 loss 0.09569593518972397\n",
            "epoch 2 batch 6 loss 0.09989598393440247\n",
            "epoch 2 batch 7 loss 0.10704786330461502\n",
            "epoch 2 batch 8 loss 0.10103802382946014\n",
            "epoch 2 batch 9 loss 0.09777996689081192\n",
            "epoch 2 batch 10 loss 0.11524602025747299\n",
            "epoch 2 batch 11 loss 0.0902089774608612\n",
            "epoch 2 batch 12 loss 0.08896023035049438\n",
            "epoch 2 batch 13 loss 0.10708242654800415\n",
            "epoch 2 batch 14 loss 0.09493619203567505\n",
            "epoch 2 batch 15 loss 0.09782630205154419\n",
            "epoch 2 batch 16 loss 0.09605541825294495\n",
            "epoch 2 batch 17 loss 0.09381944686174393\n",
            "epoch 2 batch 18 loss 0.09342043101787567\n",
            "epoch 2 batch 19 loss 0.09886830300092697\n",
            "epoch 2 batch 20 loss 0.09796455502510071\n",
            "epoch 2 batch 21 loss 0.09744065999984741\n",
            "epoch 2 batch 22 loss 0.08685854077339172\n",
            "epoch 2 batch 23 loss 0.09774084389209747\n",
            "epoch 2 batch 24 loss 0.09346293658018112\n",
            "epoch 2 batch 25 loss 0.10539087653160095\n",
            "epoch 2 batch 26 loss 0.0928570032119751\n",
            "epoch 2 batch 27 loss 0.10201610624790192\n",
            "epoch 2 batch 28 loss 0.08428223431110382\n",
            "epoch 2 batch 29 loss 0.09110486507415771\n",
            "epoch 2 batch 30 loss 0.0958072766661644\n",
            "epoch 2 batch 31 loss 0.08894690871238708\n",
            "epoch 2 batch 32 loss 0.08742952346801758\n",
            "epoch 2 batch 33 loss 0.09132026880979538\n",
            "epoch 2 batch 34 loss 0.08405104279518127\n",
            "epoch 2 batch 35 loss 0.0870557427406311\n",
            "epoch 2 batch 36 loss 0.10220153629779816\n",
            "epoch 2 batch 37 loss 0.09828633815050125\n",
            "epoch 2 batch 38 loss 0.08984733372926712\n",
            "epoch 2 batch 39 loss 0.090908944606781\n",
            "epoch 2 batch 40 loss 0.09288402646780014\n",
            "epoch 2 batch 41 loss 0.08997175842523575\n",
            "epoch 2 batch 42 loss 0.0877736508846283\n",
            "epoch 2 batch 43 loss 0.09356030076742172\n",
            "epoch 2 batch 44 loss 0.09951326996088028\n",
            "epoch 2 batch 45 loss 0.09100461006164551\n",
            "epoch 2 batch 46 loss 0.08859964460134506\n",
            "epoch 2 batch 47 loss 0.09789147228002548\n",
            "epoch 2 batch 48 loss 0.08614794909954071\n",
            "epoch 2 batch 49 loss 0.08721163123846054\n",
            "epoch 2 batch 50 loss 0.09286664426326752\n",
            "epoch 2 batch 51 loss 0.09500861167907715\n",
            "epoch 2 batch 52 loss 0.09140092134475708\n",
            "epoch 2 batch 53 loss 0.08632677048444748\n",
            "epoch 2 batch 54 loss 0.09141270816326141\n",
            "epoch 2 batch 55 loss 0.09178843349218369\n",
            "epoch 2 batch 56 loss 0.08248807489871979\n",
            "epoch 2 batch 57 loss 0.09591398388147354\n",
            "epoch 2 batch 58 loss 0.0958169549703598\n",
            "epoch 2 batch 59 loss 0.08773969113826752\n",
            "epoch 2 batch 60 loss 0.08893979340791702\n",
            "epoch 2 batch 61 loss 0.09291431307792664\n",
            "epoch 2 batch 62 loss 0.08227445185184479\n",
            "epoch 2 batch 63 loss 0.08898556977510452\n",
            "epoch 2 batch 64 loss 0.0955345556139946\n",
            "epoch 2 batch 65 loss 0.08508025109767914\n",
            "epoch 2 batch 66 loss 0.09276945143938065\n",
            "epoch 2 batch 67 loss 0.09890378266572952\n",
            "epoch 2 batch 68 loss 0.09103658050298691\n",
            "epoch 2 batch 69 loss 0.08994489163160324\n",
            "epoch 2 batch 70 loss 0.09361924231052399\n",
            "epoch 2 batch 71 loss 0.08118672668933868\n",
            "epoch 2 batch 72 loss 0.08915314823389053\n",
            "epoch 2 batch 73 loss 0.09503906220197678\n",
            "epoch 2 batch 74 loss 0.08802133798599243\n",
            "epoch 2 batch 75 loss 0.09189490973949432\n",
            "epoch 2 batch 76 loss 0.07617487013339996\n",
            "epoch 2 batch 77 loss 0.09146706759929657\n",
            "epoch 2 batch 78 loss 0.09000067412853241\n",
            "epoch 2 batch 79 loss 0.08556024730205536\n",
            "epoch 2 batch 80 loss 0.08995112776756287\n",
            "epoch 2 batch 81 loss 0.09048712253570557\n",
            "epoch 2 batch 82 loss 0.09539539366960526\n",
            "epoch 2 batch 83 loss 0.08181139081716537\n",
            "epoch 2 batch 84 loss 0.08197328448295593\n",
            "epoch 2 batch 85 loss 0.08718160539865494\n",
            "epoch 2 batch 86 loss 0.07896195352077484\n",
            "epoch 2 batch 87 loss 0.08555728942155838\n",
            "epoch 2 batch 88 loss 0.08420490473508835\n",
            "epoch 2 batch 89 loss 0.09160119295120239\n",
            "epoch 2 batch 90 loss 0.08834459632635117\n",
            "epoch 2 batch 91 loss 0.09161163121461868\n",
            "epoch 2 batch 92 loss 0.08820993453264236\n",
            "epoch 2 batch 93 loss 0.09271577000617981\n",
            "epoch 2 batch 94 loss 0.08731400966644287\n",
            "epoch 2 batch 95 loss 0.09250781685113907\n",
            "epoch 2 batch 96 loss 0.08630792051553726\n",
            "epoch 2 batch 97 loss 0.08516676723957062\n",
            "epoch 2 batch 98 loss 0.08157072216272354\n",
            "epoch 2 batch 99 loss 0.09287454932928085\n",
            "epoch 2 batch 100 loss 0.09609749913215637\n",
            "epoch 2 batch 101 loss 0.077797070145607\n",
            "epoch 2 batch 102 loss 0.0826980397105217\n",
            "epoch 2 batch 103 loss 0.09506193548440933\n",
            "epoch 2 batch 104 loss 0.08492450416088104\n",
            "epoch 2 batch 105 loss 0.07716318964958191\n",
            "epoch 2 batch 106 loss 0.08363810926675797\n",
            "epoch 2 batch 107 loss 0.08130647987127304\n",
            "epoch 2 batch 108 loss 0.08823361992835999\n",
            "epoch 2 batch 109 loss 0.08682636171579361\n",
            "epoch 2 batch 110 loss 0.07931777089834213\n",
            "epoch 2 batch 111 loss 0.0833965390920639\n",
            "epoch 2 batch 112 loss 0.0924757644534111\n",
            "epoch 2 batch 113 loss 0.08574722707271576\n",
            "epoch 2 batch 114 loss 0.08161397278308868\n",
            "epoch 2 batch 115 loss 0.087575763463974\n",
            "epoch 2 batch 116 loss 0.08644668757915497\n",
            "epoch 2 batch 117 loss 0.08948775380849838\n",
            "epoch 2 batch 118 loss 0.08335132151842117\n",
            "epoch 2 batch 119 loss 0.08654958754777908\n",
            "epoch 2 batch 120 loss 0.08039776235818863\n",
            "epoch 2 batch 121 loss 0.0849340632557869\n",
            "epoch 2 batch 122 loss 0.08904113620519638\n",
            "epoch 2 batch 123 loss 0.08253587782382965\n",
            "epoch 2 batch 124 loss 0.08615189790725708\n",
            "epoch 2 batch 125 loss 0.08100605010986328\n",
            "epoch 2 batch 126 loss 0.07622983306646347\n",
            "epoch 2 batch 127 loss 0.08754931390285492\n",
            "epoch 2 batch 128 loss 0.07800651341676712\n",
            "epoch 2 batch 129 loss 0.0819765031337738\n",
            "epoch 2 batch 130 loss 0.0992283970117569\n",
            "epoch 2 batch 131 loss 0.08742696046829224\n",
            "epoch 2 batch 132 loss 0.08890250325202942\n",
            "epoch 2 batch 133 loss 0.0788334310054779\n",
            "epoch 2 batch 134 loss 0.08623427152633667\n",
            "epoch 2 batch 135 loss 0.08787404000759125\n",
            "epoch 2 batch 136 loss 0.08044164627790451\n",
            "epoch 2 batch 137 loss 0.0814935564994812\n",
            "epoch 2 batch 138 loss 0.08575675636529922\n",
            "epoch 2 batch 139 loss 0.08456270396709442\n",
            "epoch 2 batch 140 loss 0.08907503634691238\n",
            "epoch 2 batch 141 loss 0.07804469764232635\n",
            "epoch 2 batch 142 loss 0.07708740234375\n",
            "epoch 2 batch 143 loss 0.07089304178953171\n",
            "epoch 2 batch 144 loss 0.07929949462413788\n",
            "epoch 2 batch 145 loss 0.08310268819332123\n",
            "epoch 2 batch 146 loss 0.08816652745008469\n",
            "epoch 2 batch 147 loss 0.0847373828291893\n",
            "epoch 2 batch 148 loss 0.08275645971298218\n",
            "epoch 2 batch 149 loss 0.07808542251586914\n",
            "epoch 2 batch 150 loss 0.08233574777841568\n",
            "epoch 2 batch 151 loss 0.07951470464468002\n",
            "epoch 2 batch 152 loss 0.07400697469711304\n",
            "epoch 2 batch 153 loss 0.07773624360561371\n",
            "epoch 2 batch 154 loss 0.08469211310148239\n",
            "epoch 2 batch 155 loss 0.08322946727275848\n",
            "epoch 2 batch 156 loss 0.08441968262195587\n",
            "epoch 2 batch 157 loss 0.07873131334781647\n",
            "epoch 2 batch 158 loss 0.07846017181873322\n",
            "epoch 2 batch 159 loss 0.06866888701915741\n",
            "epoch 2 batch 160 loss 0.08255919814109802\n",
            "epoch 2 batch 161 loss 0.0785239189863205\n",
            "epoch 2 batch 162 loss 0.08571583777666092\n",
            "epoch 2 batch 163 loss 0.08103172481060028\n",
            "epoch 2 batch 164 loss 0.0827588140964508\n",
            "epoch 2 batch 165 loss 0.07960789650678635\n",
            "epoch 2 batch 166 loss 0.09061306715011597\n",
            "epoch 2 batch 167 loss 0.0805521160364151\n",
            "epoch 2 batch 168 loss 0.09290765225887299\n",
            "epoch 2 batch 169 loss 0.0838390588760376\n",
            "epoch 2 batch 170 loss 0.0767134577035904\n",
            "epoch 2 batch 171 loss 0.08519959449768066\n",
            "epoch 2 batch 172 loss 0.08342476189136505\n",
            "epoch 2 batch 173 loss 0.0771317258477211\n",
            "epoch 2 batch 174 loss 0.08088108897209167\n",
            "epoch 2 batch 175 loss 0.0817108005285263\n",
            "epoch 2 batch 176 loss 0.08097914606332779\n",
            "epoch 2 batch 177 loss 0.07458902895450592\n",
            "epoch 2 batch 178 loss 0.07666831463575363\n",
            "epoch 2 batch 179 loss 0.07662888616323471\n",
            "epoch 2 batch 180 loss 0.08020962029695511\n",
            "epoch 2 batch 181 loss 0.08106797933578491\n",
            "epoch 2 batch 182 loss 0.08192466199398041\n",
            "epoch 2 batch 183 loss 0.08452533930540085\n",
            "epoch 2 batch 184 loss 0.09161002188920975\n",
            "epoch 2 batch 185 loss 0.08176389336585999\n",
            "epoch 2 batch 186 loss 0.09196102619171143\n",
            "epoch 2 batch 187 loss 0.07418910413980484\n",
            "epoch 2 batch 188 loss 0.07414352148771286\n",
            "epoch 2 batch 189 loss 0.08575521409511566\n",
            "epoch 2 batch 190 loss 0.07686268538236618\n",
            "epoch 2 batch 191 loss 0.08334343880414963\n",
            "epoch 2 batch 192 loss 0.06971680372953415\n",
            "epoch 2 batch 193 loss 0.07536246627569199\n",
            "epoch 2 batch 194 loss 0.07544729113578796\n",
            "epoch 2 batch 195 loss 0.07631469517946243\n",
            "epoch 2 batch 196 loss 0.08175969868898392\n",
            "epoch 2 batch 197 loss 0.07674971222877502\n",
            "epoch 2 batch 198 loss 0.07390354573726654\n",
            "epoch 2 batch 199 loss 0.08602669835090637\n",
            "epoch 2 batch 200 loss 0.08075598627328873\n",
            "epoch 2 batch 201 loss 0.0833696573972702\n",
            "epoch 2 batch 202 loss 0.07495338469743729\n",
            "epoch 2 batch 203 loss 0.07300051301717758\n",
            "epoch 2 batch 204 loss 0.06898529082536697\n",
            "epoch 2 batch 205 loss 0.07845750451087952\n",
            "epoch 2 batch 206 loss 0.06461163610219955\n",
            "epoch 2 batch 207 loss 0.07444925606250763\n",
            "epoch 2 batch 208 loss 0.08643838763237\n",
            "epoch 2 batch 209 loss 0.0765228271484375\n",
            "epoch 2 batch 210 loss 0.08499973267316818\n",
            "epoch 2 batch 211 loss 0.07595779746770859\n",
            "epoch 2 batch 212 loss 0.07342617213726044\n",
            "epoch 2 batch 213 loss 0.07275104522705078\n",
            "epoch 2 batch 214 loss 0.07890375703573227\n",
            "epoch 2 batch 215 loss 0.06911905109882355\n",
            "epoch 2 batch 216 loss 0.07531630992889404\n",
            "epoch 2 batch 217 loss 0.062294963747262955\n",
            "epoch 2 batch 218 loss 0.07233840972185135\n",
            "epoch 2 batch 219 loss 0.07537539303302765\n",
            "epoch 2 batch 220 loss 0.07719765603542328\n",
            "epoch 2 batch 221 loss 0.0750812292098999\n",
            "epoch 2 batch 222 loss 0.0815628170967102\n",
            "epoch 2 batch 223 loss 0.07645402103662491\n",
            "epoch 2 batch 224 loss 0.07659756392240524\n",
            "epoch 2 batch 225 loss 0.07810130715370178\n",
            "epoch 2 batch 226 loss 0.07661446928977966\n",
            "epoch 2 batch 227 loss 0.0729794055223465\n",
            "epoch 2 batch 228 loss 0.07143988460302353\n",
            "epoch 2 batch 229 loss 0.07455996423959732\n",
            "epoch 2 batch 230 loss 0.07616482675075531\n",
            "epoch 3 batch 0 loss 0.07781689614057541\n",
            "epoch 3 batch 1 loss 0.07468821108341217\n",
            "epoch 3 batch 2 loss 0.08531786501407623\n",
            "epoch 3 batch 3 loss 0.08138030022382736\n",
            "epoch 3 batch 4 loss 0.07629072666168213\n",
            "epoch 3 batch 5 loss 0.07360358536243439\n",
            "epoch 3 batch 6 loss 0.07758454233407974\n",
            "epoch 3 batch 7 loss 0.08455679565668106\n",
            "epoch 3 batch 8 loss 0.07682914286851883\n",
            "epoch 3 batch 9 loss 0.07618406414985657\n",
            "epoch 3 batch 10 loss 0.08911871165037155\n",
            "epoch 3 batch 11 loss 0.06896649301052094\n",
            "epoch 3 batch 12 loss 0.0693163126707077\n",
            "epoch 3 batch 13 loss 0.08371054381132126\n",
            "epoch 3 batch 14 loss 0.07129152119159698\n",
            "epoch 3 batch 15 loss 0.07552485913038254\n",
            "epoch 3 batch 16 loss 0.07367617636919022\n",
            "epoch 3 batch 17 loss 0.07239071279764175\n",
            "epoch 3 batch 18 loss 0.07317125797271729\n",
            "epoch 3 batch 19 loss 0.07549028843641281\n",
            "epoch 3 batch 20 loss 0.07555276155471802\n",
            "epoch 3 batch 21 loss 0.07614420354366302\n",
            "epoch 3 batch 22 loss 0.06629661470651627\n",
            "epoch 3 batch 23 loss 0.07727932929992676\n",
            "epoch 3 batch 24 loss 0.07268312573432922\n",
            "epoch 3 batch 25 loss 0.0860258936882019\n",
            "epoch 3 batch 26 loss 0.07390216737985611\n",
            "epoch 3 batch 27 loss 0.08046282827854156\n",
            "epoch 3 batch 28 loss 0.06530986726284027\n",
            "epoch 3 batch 29 loss 0.07222278416156769\n",
            "epoch 3 batch 30 loss 0.07534527033567429\n",
            "epoch 3 batch 31 loss 0.0706428587436676\n",
            "epoch 3 batch 32 loss 0.06783509999513626\n",
            "epoch 3 batch 33 loss 0.07058417052030563\n",
            "epoch 3 batch 34 loss 0.06668571382761002\n",
            "epoch 3 batch 35 loss 0.0701669231057167\n",
            "epoch 3 batch 36 loss 0.082840196788311\n",
            "epoch 3 batch 37 loss 0.07648468017578125\n",
            "epoch 3 batch 38 loss 0.07215458154678345\n",
            "epoch 3 batch 39 loss 0.07543226331472397\n",
            "epoch 3 batch 40 loss 0.07492438703775406\n",
            "epoch 3 batch 41 loss 0.07036733627319336\n",
            "epoch 3 batch 42 loss 0.07103107869625092\n",
            "epoch 3 batch 43 loss 0.07711667567491531\n",
            "epoch 3 batch 44 loss 0.07985060662031174\n",
            "epoch 3 batch 45 loss 0.07255569100379944\n",
            "epoch 3 batch 46 loss 0.07308536767959595\n",
            "epoch 3 batch 47 loss 0.07705950736999512\n",
            "epoch 3 batch 48 loss 0.0687887966632843\n",
            "epoch 3 batch 49 loss 0.0694044679403305\n",
            "epoch 3 batch 50 loss 0.07366520911455154\n",
            "epoch 3 batch 51 loss 0.07719291746616364\n",
            "epoch 3 batch 52 loss 0.07442867010831833\n",
            "epoch 3 batch 53 loss 0.06808090955018997\n",
            "epoch 3 batch 54 loss 0.07132769376039505\n",
            "epoch 3 batch 55 loss 0.07304839789867401\n",
            "epoch 3 batch 56 loss 0.06584947556257248\n",
            "epoch 3 batch 57 loss 0.07719600945711136\n",
            "epoch 3 batch 58 loss 0.07726631313562393\n",
            "epoch 3 batch 59 loss 0.07100262492895126\n",
            "epoch 3 batch 60 loss 0.07089387625455856\n",
            "epoch 3 batch 61 loss 0.07590293139219284\n",
            "epoch 3 batch 62 loss 0.06709922850131989\n",
            "epoch 3 batch 63 loss 0.07148991525173187\n",
            "epoch 3 batch 64 loss 0.07728948444128036\n",
            "epoch 3 batch 65 loss 0.06784157454967499\n",
            "epoch 3 batch 66 loss 0.07565139979124069\n",
            "epoch 3 batch 67 loss 0.0798608809709549\n",
            "epoch 3 batch 68 loss 0.07155492156744003\n",
            "epoch 3 batch 69 loss 0.07302335649728775\n",
            "epoch 3 batch 70 loss 0.07524197548627853\n",
            "epoch 3 batch 71 loss 0.06600441783666611\n",
            "epoch 3 batch 72 loss 0.0719367042183876\n",
            "epoch 3 batch 73 loss 0.07732214778661728\n",
            "epoch 3 batch 74 loss 0.07176120579242706\n",
            "epoch 3 batch 75 loss 0.07600060850381851\n",
            "epoch 3 batch 76 loss 0.06206563115119934\n",
            "epoch 3 batch 77 loss 0.07378028333187103\n",
            "epoch 3 batch 78 loss 0.07304268330335617\n",
            "epoch 3 batch 79 loss 0.06869009137153625\n",
            "epoch 3 batch 80 loss 0.07314843684434891\n",
            "epoch 3 batch 81 loss 0.0745537281036377\n",
            "epoch 3 batch 82 loss 0.07739821076393127\n",
            "epoch 3 batch 83 loss 0.066134512424469\n",
            "epoch 3 batch 84 loss 0.06729833781719208\n",
            "epoch 3 batch 85 loss 0.07265883684158325\n",
            "epoch 3 batch 86 loss 0.06269900500774384\n",
            "epoch 3 batch 87 loss 0.06982114166021347\n",
            "epoch 3 batch 88 loss 0.06968020647764206\n",
            "epoch 3 batch 89 loss 0.07585459202528\n",
            "epoch 3 batch 90 loss 0.0724528580904007\n",
            "epoch 3 batch 91 loss 0.07448834180831909\n",
            "epoch 3 batch 92 loss 0.07184905558824539\n",
            "epoch 3 batch 93 loss 0.07558490335941315\n",
            "epoch 3 batch 94 loss 0.06975702196359634\n",
            "epoch 3 batch 95 loss 0.0752696618437767\n",
            "epoch 3 batch 96 loss 0.07011445611715317\n",
            "epoch 3 batch 97 loss 0.07056131958961487\n",
            "epoch 3 batch 98 loss 0.06701027601957321\n",
            "epoch 3 batch 99 loss 0.07591886073350906\n",
            "epoch 3 batch 100 loss 0.0782620757818222\n",
            "epoch 3 batch 101 loss 0.06433200091123581\n",
            "epoch 3 batch 102 loss 0.06744919717311859\n",
            "epoch 3 batch 103 loss 0.07720736414194107\n",
            "epoch 3 batch 104 loss 0.07063206285238266\n",
            "epoch 3 batch 105 loss 0.062285568565130234\n",
            "epoch 3 batch 106 loss 0.06670723110437393\n",
            "epoch 3 batch 107 loss 0.06699051707983017\n",
            "epoch 3 batch 108 loss 0.07171954959630966\n",
            "epoch 3 batch 109 loss 0.07087966799736023\n",
            "epoch 3 batch 110 loss 0.06493004411458969\n",
            "epoch 3 batch 111 loss 0.06802494823932648\n",
            "epoch 3 batch 112 loss 0.07690971344709396\n",
            "epoch 3 batch 113 loss 0.07067342102527618\n",
            "epoch 3 batch 114 loss 0.06689143180847168\n",
            "epoch 3 batch 115 loss 0.07292871177196503\n",
            "epoch 3 batch 116 loss 0.07049703598022461\n",
            "epoch 3 batch 117 loss 0.07472618669271469\n",
            "epoch 3 batch 118 loss 0.06840871274471283\n",
            "epoch 3 batch 119 loss 0.0730089321732521\n",
            "epoch 3 batch 120 loss 0.06521683931350708\n",
            "epoch 3 batch 121 loss 0.07047931104898453\n",
            "epoch 3 batch 122 loss 0.07265561074018478\n",
            "epoch 3 batch 123 loss 0.06856744736433029\n",
            "epoch 3 batch 124 loss 0.07298732548952103\n",
            "epoch 3 batch 125 loss 0.06846822053194046\n",
            "epoch 3 batch 126 loss 0.061764687299728394\n",
            "epoch 3 batch 127 loss 0.07443082332611084\n",
            "epoch 3 batch 128 loss 0.06529556959867477\n",
            "epoch 3 batch 129 loss 0.06777556240558624\n",
            "epoch 3 batch 130 loss 0.08344139158725739\n",
            "epoch 3 batch 131 loss 0.0725451409816742\n",
            "epoch 3 batch 132 loss 0.0749201625585556\n",
            "epoch 3 batch 133 loss 0.0666569322347641\n",
            "epoch 3 batch 134 loss 0.07256105542182922\n",
            "epoch 3 batch 135 loss 0.07457911968231201\n",
            "epoch 3 batch 136 loss 0.06721541285514832\n",
            "epoch 3 batch 137 loss 0.06820950657129288\n",
            "epoch 3 batch 138 loss 0.07049421221017838\n",
            "epoch 3 batch 139 loss 0.06976065039634705\n",
            "epoch 3 batch 140 loss 0.07411612570285797\n",
            "epoch 3 batch 141 loss 0.0656917616724968\n",
            "epoch 3 batch 142 loss 0.06258261203765869\n",
            "epoch 3 batch 143 loss 0.05717014521360397\n",
            "epoch 3 batch 144 loss 0.0667402371764183\n",
            "epoch 3 batch 145 loss 0.068860724568367\n",
            "epoch 3 batch 146 loss 0.07411123812198639\n",
            "epoch 3 batch 147 loss 0.07092416286468506\n",
            "epoch 3 batch 148 loss 0.06820490956306458\n",
            "epoch 3 batch 149 loss 0.06773971021175385\n",
            "epoch 3 batch 150 loss 0.06864022463560104\n",
            "epoch 3 batch 151 loss 0.06567332148551941\n",
            "epoch 3 batch 152 loss 0.061628635972738266\n",
            "epoch 3 batch 153 loss 0.0668306052684784\n",
            "epoch 3 batch 154 loss 0.07020564377307892\n",
            "epoch 3 batch 155 loss 0.06923466920852661\n",
            "epoch 3 batch 156 loss 0.07024090737104416\n",
            "epoch 3 batch 157 loss 0.06553705781698227\n",
            "epoch 3 batch 158 loss 0.06352677941322327\n",
            "epoch 3 batch 159 loss 0.05805085599422455\n",
            "epoch 3 batch 160 loss 0.0698496550321579\n",
            "epoch 3 batch 161 loss 0.06360312551259995\n",
            "epoch 3 batch 162 loss 0.07266398519277573\n",
            "epoch 3 batch 163 loss 0.06790325790643692\n",
            "epoch 3 batch 164 loss 0.06777843832969666\n",
            "epoch 3 batch 165 loss 0.06489875167608261\n",
            "epoch 3 batch 166 loss 0.07456125319004059\n",
            "epoch 3 batch 167 loss 0.06721463799476624\n",
            "epoch 3 batch 168 loss 0.07768604904413223\n",
            "epoch 3 batch 169 loss 0.06782786548137665\n",
            "epoch 3 batch 170 loss 0.06412937492132187\n",
            "epoch 3 batch 171 loss 0.06993306428194046\n",
            "epoch 3 batch 172 loss 0.0699518546462059\n",
            "epoch 3 batch 173 loss 0.06368237733840942\n",
            "epoch 3 batch 174 loss 0.06785620003938675\n",
            "epoch 3 batch 175 loss 0.06885865330696106\n",
            "epoch 3 batch 176 loss 0.06802578270435333\n",
            "epoch 3 batch 177 loss 0.060406722128391266\n",
            "epoch 3 batch 178 loss 0.06457602232694626\n",
            "epoch 3 batch 179 loss 0.06316814571619034\n",
            "epoch 3 batch 180 loss 0.06808778643608093\n",
            "epoch 3 batch 181 loss 0.07043016701936722\n",
            "epoch 3 batch 182 loss 0.0684642568230629\n",
            "epoch 3 batch 183 loss 0.07195557653903961\n",
            "epoch 3 batch 184 loss 0.07861366868019104\n",
            "epoch 3 batch 185 loss 0.06918562948703766\n",
            "epoch 3 batch 186 loss 0.07911653816699982\n",
            "epoch 3 batch 187 loss 0.06323554366827011\n",
            "epoch 3 batch 188 loss 0.06135221943259239\n",
            "epoch 3 batch 189 loss 0.07510761171579361\n",
            "epoch 3 batch 190 loss 0.06393308192491531\n",
            "epoch 3 batch 191 loss 0.07088349759578705\n",
            "epoch 3 batch 192 loss 0.0591428279876709\n",
            "epoch 3 batch 193 loss 0.06444372981786728\n",
            "epoch 3 batch 194 loss 0.06455197185277939\n",
            "epoch 3 batch 195 loss 0.06622716784477234\n",
            "epoch 3 batch 196 loss 0.07056984305381775\n",
            "epoch 3 batch 197 loss 0.06526555120944977\n",
            "epoch 3 batch 198 loss 0.06322190165519714\n",
            "epoch 3 batch 199 loss 0.07408709079027176\n",
            "epoch 3 batch 200 loss 0.0706019178032875\n",
            "epoch 3 batch 201 loss 0.0705115869641304\n",
            "epoch 3 batch 202 loss 0.06541911512613297\n",
            "epoch 3 batch 203 loss 0.06327033042907715\n",
            "epoch 3 batch 204 loss 0.05677947402000427\n",
            "epoch 3 batch 205 loss 0.069964699447155\n",
            "epoch 3 batch 206 loss 0.054299816489219666\n",
            "epoch 3 batch 207 loss 0.0636349767446518\n",
            "epoch 3 batch 208 loss 0.07356096059083939\n",
            "epoch 3 batch 209 loss 0.06514757871627808\n",
            "epoch 3 batch 210 loss 0.07283494621515274\n",
            "epoch 3 batch 211 loss 0.0657137781381607\n",
            "epoch 3 batch 212 loss 0.06244608387351036\n",
            "epoch 3 batch 213 loss 0.062216948717832565\n",
            "epoch 3 batch 214 loss 0.06759587675333023\n",
            "epoch 3 batch 215 loss 0.058346349745988846\n",
            "epoch 3 batch 216 loss 0.06366369128227234\n",
            "epoch 3 batch 217 loss 0.05410606786608696\n",
            "epoch 3 batch 218 loss 0.06209876388311386\n",
            "epoch 3 batch 219 loss 0.0658828541636467\n",
            "epoch 3 batch 220 loss 0.06547566503286362\n",
            "epoch 3 batch 221 loss 0.06385847181081772\n",
            "epoch 3 batch 222 loss 0.0690826028585434\n",
            "epoch 3 batch 223 loss 0.06533266603946686\n",
            "epoch 3 batch 224 loss 0.06511728465557098\n",
            "epoch 3 batch 225 loss 0.06640614569187164\n",
            "epoch 3 batch 226 loss 0.06474045664072037\n",
            "epoch 3 batch 227 loss 0.06244245171546936\n",
            "epoch 3 batch 228 loss 0.0629059374332428\n",
            "epoch 3 batch 229 loss 0.0634913295507431\n",
            "epoch 3 batch 230 loss 0.06361420452594757\n",
            "epoch 4 batch 0 loss 0.06831274926662445\n",
            "epoch 4 batch 1 loss 0.06434236466884613\n",
            "epoch 4 batch 2 loss 0.07386971265077591\n",
            "epoch 4 batch 3 loss 0.06948094815015793\n",
            "epoch 4 batch 4 loss 0.06520628929138184\n",
            "epoch 4 batch 5 loss 0.0648721233010292\n",
            "epoch 4 batch 6 loss 0.06661947071552277\n",
            "epoch 4 batch 7 loss 0.07434149086475372\n",
            "epoch 4 batch 8 loss 0.06587884575128555\n",
            "epoch 4 batch 9 loss 0.0653180256485939\n",
            "epoch 4 batch 10 loss 0.07709191739559174\n",
            "epoch 4 batch 11 loss 0.05977039784193039\n",
            "epoch 4 batch 12 loss 0.060747869312763214\n",
            "epoch 4 batch 13 loss 0.07215289771556854\n",
            "epoch 4 batch 14 loss 0.06155567988753319\n",
            "epoch 4 batch 15 loss 0.06423129141330719\n",
            "epoch 4 batch 16 loss 0.06265990436077118\n",
            "epoch 4 batch 17 loss 0.06222778558731079\n",
            "epoch 4 batch 18 loss 0.06312324106693268\n",
            "epoch 4 batch 19 loss 0.06488242745399475\n",
            "epoch 4 batch 20 loss 0.06534059345722198\n",
            "epoch 4 batch 21 loss 0.06573718786239624\n",
            "epoch 4 batch 22 loss 0.05741686001420021\n",
            "epoch 4 batch 23 loss 0.06614522635936737\n",
            "epoch 4 batch 24 loss 0.06359001249074936\n",
            "epoch 4 batch 25 loss 0.07513423264026642\n",
            "epoch 4 batch 26 loss 0.06485624611377716\n",
            "epoch 4 batch 27 loss 0.07104292511940002\n",
            "epoch 4 batch 28 loss 0.05727794021368027\n",
            "epoch 4 batch 29 loss 0.06210041791200638\n",
            "epoch 4 batch 30 loss 0.06546995788812637\n",
            "epoch 4 batch 31 loss 0.06162189319729805\n",
            "epoch 4 batch 32 loss 0.05723128467798233\n",
            "epoch 4 batch 33 loss 0.06107150763273239\n",
            "epoch 4 batch 34 loss 0.05771149322390556\n",
            "epoch 4 batch 35 loss 0.06148124113678932\n",
            "epoch 4 batch 36 loss 0.07330019026994705\n",
            "epoch 4 batch 37 loss 0.06651222705841064\n",
            "epoch 4 batch 38 loss 0.062323637306690216\n",
            "epoch 4 batch 39 loss 0.06516993790864944\n",
            "epoch 4 batch 40 loss 0.06446962058544159\n",
            "epoch 4 batch 41 loss 0.06215513125061989\n",
            "epoch 4 batch 42 loss 0.061391718685626984\n",
            "epoch 4 batch 43 loss 0.0677528977394104\n",
            "epoch 4 batch 44 loss 0.07048121839761734\n",
            "epoch 4 batch 45 loss 0.06244918704032898\n",
            "epoch 4 batch 46 loss 0.06320768594741821\n",
            "epoch 4 batch 47 loss 0.0685734823346138\n",
            "epoch 4 batch 48 loss 0.05865894630551338\n",
            "epoch 4 batch 49 loss 0.06230379641056061\n",
            "epoch 4 batch 50 loss 0.0648573562502861\n",
            "epoch 4 batch 51 loss 0.06686399132013321\n",
            "epoch 4 batch 52 loss 0.06601731479167938\n",
            "epoch 4 batch 53 loss 0.058716852217912674\n",
            "epoch 4 batch 54 loss 0.0651124119758606\n",
            "epoch 4 batch 55 loss 0.06232040002942085\n",
            "epoch 4 batch 56 loss 0.059657689183950424\n",
            "epoch 4 batch 57 loss 0.06861609220504761\n",
            "epoch 4 batch 58 loss 0.06741762906312943\n",
            "epoch 4 batch 59 loss 0.06254694610834122\n",
            "epoch 4 batch 60 loss 0.06241113692522049\n",
            "epoch 4 batch 61 loss 0.06378035992383957\n",
            "epoch 4 batch 62 loss 0.05861946567893028\n",
            "epoch 4 batch 63 loss 0.0618562251329422\n",
            "epoch 4 batch 64 loss 0.06622307002544403\n",
            "epoch 4 batch 65 loss 0.059644076973199844\n",
            "epoch 4 batch 66 loss 0.06449779868125916\n",
            "epoch 4 batch 67 loss 0.06902304291725159\n",
            "epoch 4 batch 68 loss 0.06297304481267929\n",
            "epoch 4 batch 69 loss 0.06338264048099518\n",
            "epoch 4 batch 70 loss 0.06488770991563797\n",
            "epoch 4 batch 71 loss 0.057810794562101364\n",
            "epoch 4 batch 72 loss 0.0625605434179306\n",
            "epoch 4 batch 73 loss 0.06627984344959259\n",
            "epoch 4 batch 74 loss 0.06269777566194534\n",
            "epoch 4 batch 75 loss 0.06658946722745895\n",
            "epoch 4 batch 76 loss 0.0535871647298336\n",
            "epoch 4 batch 77 loss 0.06359829753637314\n",
            "epoch 4 batch 78 loss 0.06249554082751274\n",
            "epoch 4 batch 79 loss 0.05925559997558594\n",
            "epoch 4 batch 80 loss 0.06440900266170502\n",
            "epoch 4 batch 81 loss 0.06263946741819382\n",
            "epoch 4 batch 82 loss 0.06821496039628983\n",
            "epoch 4 batch 83 loss 0.05668409541249275\n",
            "epoch 4 batch 84 loss 0.057358164340257645\n",
            "epoch 4 batch 85 loss 0.0635736882686615\n",
            "epoch 4 batch 86 loss 0.054617270827293396\n",
            "epoch 4 batch 87 loss 0.06174561008810997\n",
            "epoch 4 batch 88 loss 0.05916617810726166\n",
            "epoch 4 batch 89 loss 0.06777700781822205\n",
            "epoch 4 batch 90 loss 0.06314582377672195\n",
            "epoch 4 batch 91 loss 0.06696806848049164\n",
            "epoch 4 batch 92 loss 0.06267093867063522\n",
            "epoch 4 batch 93 loss 0.06686275452375412\n",
            "epoch 4 batch 94 loss 0.06143922731280327\n",
            "epoch 4 batch 95 loss 0.06560732424259186\n",
            "epoch 4 batch 96 loss 0.061600301414728165\n",
            "epoch 4 batch 97 loss 0.06188651919364929\n",
            "epoch 4 batch 98 loss 0.05980055406689644\n",
            "epoch 4 batch 99 loss 0.0665220096707344\n",
            "epoch 4 batch 100 loss 0.06932562589645386\n",
            "epoch 4 batch 101 loss 0.05538671463727951\n",
            "epoch 4 batch 102 loss 0.05926258862018585\n",
            "epoch 4 batch 103 loss 0.06627339124679565\n",
            "epoch 4 batch 104 loss 0.06065160036087036\n",
            "epoch 4 batch 105 loss 0.05379858613014221\n",
            "epoch 4 batch 106 loss 0.057088036090135574\n",
            "epoch 4 batch 107 loss 0.05918430909514427\n",
            "epoch 4 batch 108 loss 0.060433510690927505\n",
            "epoch 4 batch 109 loss 0.06204497814178467\n",
            "epoch 4 batch 110 loss 0.056548528373241425\n",
            "epoch 4 batch 111 loss 0.06102554500102997\n",
            "epoch 4 batch 112 loss 0.06861385703086853\n",
            "epoch 4 batch 113 loss 0.06185786798596382\n",
            "epoch 4 batch 114 loss 0.06106117367744446\n",
            "epoch 4 batch 115 loss 0.06392752379179001\n",
            "epoch 4 batch 116 loss 0.06383828073740005\n",
            "epoch 4 batch 117 loss 0.06446907669305801\n",
            "epoch 4 batch 118 loss 0.06330578774213791\n",
            "epoch 4 batch 119 loss 0.06296646595001221\n",
            "epoch 4 batch 120 loss 0.0592888668179512\n",
            "epoch 4 batch 121 loss 0.0611320361495018\n",
            "epoch 4 batch 122 loss 0.06357305496931076\n",
            "epoch 4 batch 123 loss 0.059965308755636215\n",
            "epoch 4 batch 124 loss 0.06542721390724182\n",
            "epoch 4 batch 125 loss 0.059876326471567154\n",
            "epoch 4 batch 126 loss 0.05426156520843506\n",
            "epoch 4 batch 127 loss 0.06661524623632431\n",
            "epoch 4 batch 128 loss 0.058748651295900345\n",
            "epoch 4 batch 129 loss 0.0583711601793766\n",
            "epoch 4 batch 130 loss 0.07358039170503616\n",
            "epoch 4 batch 131 loss 0.0637948215007782\n",
            "epoch 4 batch 132 loss 0.06524002552032471\n",
            "epoch 4 batch 133 loss 0.057643961161375046\n",
            "epoch 4 batch 134 loss 0.06332632899284363\n",
            "epoch 4 batch 135 loss 0.06669200211763382\n",
            "epoch 4 batch 136 loss 0.05966649204492569\n",
            "epoch 4 batch 137 loss 0.06046263128519058\n",
            "epoch 4 batch 138 loss 0.061264343559741974\n",
            "epoch 4 batch 139 loss 0.060655489563941956\n",
            "epoch 4 batch 140 loss 0.06466945260763168\n",
            "epoch 4 batch 141 loss 0.057078491896390915\n",
            "epoch 4 batch 142 loss 0.05483314394950867\n",
            "epoch 4 batch 143 loss 0.04960670322179794\n",
            "epoch 4 batch 144 loss 0.05816943570971489\n",
            "epoch 4 batch 145 loss 0.06120773404836655\n",
            "epoch 4 batch 146 loss 0.06649754196405411\n",
            "epoch 4 batch 147 loss 0.0638003721833229\n",
            "epoch 4 batch 148 loss 0.05808226764202118\n",
            "epoch 4 batch 149 loss 0.06003901734948158\n",
            "epoch 4 batch 150 loss 0.061922598630189896\n",
            "epoch 4 batch 151 loss 0.05700342729687691\n",
            "epoch 4 batch 152 loss 0.05416356772184372\n",
            "epoch 4 batch 153 loss 0.05863821133971214\n",
            "epoch 4 batch 154 loss 0.06212017685174942\n",
            "epoch 4 batch 155 loss 0.0600786916911602\n",
            "epoch 4 batch 156 loss 0.06172430142760277\n",
            "epoch 4 batch 157 loss 0.05837626755237579\n",
            "epoch 4 batch 158 loss 0.05643497779965401\n",
            "epoch 4 batch 159 loss 0.0509091392159462\n",
            "epoch 4 batch 160 loss 0.06103057414293289\n",
            "epoch 4 batch 161 loss 0.05636577308177948\n",
            "epoch 4 batch 162 loss 0.06369327753782272\n",
            "epoch 4 batch 163 loss 0.06090288236737251\n",
            "epoch 4 batch 164 loss 0.05994158610701561\n",
            "epoch 4 batch 165 loss 0.05768926814198494\n",
            "epoch 4 batch 166 loss 0.06572288274765015\n",
            "epoch 4 batch 167 loss 0.06004955619573593\n",
            "epoch 4 batch 168 loss 0.06788986921310425\n",
            "epoch 4 batch 169 loss 0.06010211259126663\n",
            "epoch 4 batch 170 loss 0.05656495690345764\n",
            "epoch 4 batch 171 loss 0.06177758052945137\n",
            "epoch 4 batch 172 loss 0.06249896436929703\n",
            "epoch 4 batch 173 loss 0.056436363607645035\n",
            "epoch 4 batch 174 loss 0.06044365093111992\n",
            "epoch 4 batch 175 loss 0.06282354146242142\n",
            "epoch 4 batch 176 loss 0.06154238060116768\n",
            "epoch 4 batch 177 loss 0.05457799509167671\n",
            "epoch 4 batch 178 loss 0.0575319342315197\n",
            "epoch 4 batch 179 loss 0.05528190732002258\n",
            "epoch 4 batch 180 loss 0.059745799750089645\n",
            "epoch 4 batch 181 loss 0.06263142079114914\n",
            "epoch 4 batch 182 loss 0.06138765439391136\n",
            "epoch 4 batch 183 loss 0.06426283717155457\n",
            "epoch 4 batch 184 loss 0.07123377919197083\n",
            "epoch 4 batch 185 loss 0.06235472857952118\n",
            "epoch 4 batch 186 loss 0.07147454470396042\n",
            "epoch 4 batch 187 loss 0.055283356457948685\n",
            "epoch 4 batch 188 loss 0.054801635444164276\n",
            "epoch 4 batch 189 loss 0.06810101866722107\n",
            "epoch 4 batch 190 loss 0.05695681646466255\n",
            "epoch 4 batch 191 loss 0.06470906734466553\n",
            "epoch 4 batch 192 loss 0.05298929288983345\n",
            "epoch 4 batch 193 loss 0.05696232616901398\n",
            "epoch 4 batch 194 loss 0.058008670806884766\n",
            "epoch 4 batch 195 loss 0.05878309905529022\n",
            "epoch 4 batch 196 loss 0.06257428973913193\n",
            "epoch 4 batch 197 loss 0.05686255171895027\n",
            "epoch 4 batch 198 loss 0.0551808699965477\n",
            "epoch 4 batch 199 loss 0.06737382709980011\n",
            "epoch 4 batch 200 loss 0.06246984004974365\n",
            "epoch 4 batch 201 loss 0.06342843919992447\n",
            "epoch 4 batch 202 loss 0.057390905916690826\n",
            "epoch 4 batch 203 loss 0.05703677982091904\n",
            "epoch 4 batch 204 loss 0.050452351570129395\n",
            "epoch 4 batch 205 loss 0.06125052273273468\n",
            "epoch 4 batch 206 loss 0.04864194244146347\n",
            "epoch 4 batch 207 loss 0.056879978626966476\n",
            "epoch 4 batch 208 loss 0.06548868119716644\n",
            "epoch 4 batch 209 loss 0.05822846665978432\n",
            "epoch 4 batch 210 loss 0.06562988460063934\n",
            "epoch 4 batch 211 loss 0.059801362454891205\n",
            "epoch 4 batch 212 loss 0.05703587457537651\n",
            "epoch 4 batch 213 loss 0.056238804012537\n",
            "epoch 4 batch 214 loss 0.058690644800662994\n",
            "epoch 4 batch 215 loss 0.05345101282000542\n",
            "epoch 4 batch 216 loss 0.05589925870299339\n",
            "epoch 4 batch 217 loss 0.048358894884586334\n",
            "epoch 4 batch 218 loss 0.05663197860121727\n",
            "epoch 4 batch 219 loss 0.059726789593696594\n",
            "epoch 4 batch 220 loss 0.05824006721377373\n",
            "epoch 4 batch 221 loss 0.057814374566078186\n",
            "epoch 4 batch 222 loss 0.06109533831477165\n",
            "epoch 4 batch 223 loss 0.05903899669647217\n",
            "epoch 4 batch 224 loss 0.05825315788388252\n",
            "epoch 4 batch 225 loss 0.05872858315706253\n",
            "epoch 4 batch 226 loss 0.056904666125774384\n",
            "epoch 4 batch 227 loss 0.05702158808708191\n",
            "epoch 4 batch 228 loss 0.05704132094979286\n",
            "epoch 4 batch 229 loss 0.05714929476380348\n",
            "epoch 4 batch 230 loss 0.053986113518476486\n",
            "epoch 5 batch 0 loss 0.060795463621616364\n",
            "epoch 5 batch 1 loss 0.05871296674013138\n",
            "epoch 5 batch 2 loss 0.06676946580410004\n",
            "epoch 5 batch 3 loss 0.06331966817378998\n",
            "epoch 5 batch 4 loss 0.05840754881501198\n",
            "epoch 5 batch 5 loss 0.05898148939013481\n",
            "epoch 5 batch 6 loss 0.05979445204138756\n",
            "epoch 5 batch 7 loss 0.06721938401460648\n",
            "epoch 5 batch 8 loss 0.05905153974890709\n",
            "epoch 5 batch 9 loss 0.05932776629924774\n",
            "epoch 5 batch 10 loss 0.06947911530733109\n",
            "epoch 5 batch 11 loss 0.054744161665439606\n",
            "epoch 5 batch 12 loss 0.05486283078789711\n",
            "epoch 5 batch 13 loss 0.06471403688192368\n",
            "epoch 5 batch 14 loss 0.05604340508580208\n",
            "epoch 5 batch 15 loss 0.05935971438884735\n",
            "epoch 5 batch 16 loss 0.0566335991024971\n",
            "epoch 5 batch 17 loss 0.05710287764668465\n",
            "epoch 5 batch 18 loss 0.057425789535045624\n",
            "epoch 5 batch 19 loss 0.05886317789554596\n",
            "epoch 5 batch 20 loss 0.058937545865774155\n",
            "epoch 5 batch 21 loss 0.059632398188114166\n",
            "epoch 5 batch 22 loss 0.05207156389951706\n",
            "epoch 5 batch 23 loss 0.06051010265946388\n",
            "epoch 5 batch 24 loss 0.05731050670146942\n",
            "epoch 5 batch 25 loss 0.06857709586620331\n",
            "epoch 5 batch 26 loss 0.05838579684495926\n",
            "epoch 5 batch 27 loss 0.06586357206106186\n",
            "epoch 5 batch 28 loss 0.05194714292883873\n",
            "epoch 5 batch 29 loss 0.057323530316352844\n",
            "epoch 5 batch 30 loss 0.0588764026761055\n",
            "epoch 5 batch 31 loss 0.05715832859277725\n",
            "epoch 5 batch 32 loss 0.053314320743083954\n",
            "epoch 5 batch 33 loss 0.05568457767367363\n",
            "epoch 5 batch 34 loss 0.05328521877527237\n",
            "epoch 5 batch 35 loss 0.05559004843235016\n",
            "epoch 5 batch 36 loss 0.06934709846973419\n",
            "epoch 5 batch 37 loss 0.059535954147577286\n",
            "epoch 5 batch 38 loss 0.05819056183099747\n",
            "epoch 5 batch 39 loss 0.05800987035036087\n",
            "epoch 5 batch 40 loss 0.05905400589108467\n",
            "epoch 5 batch 41 loss 0.05616238713264465\n",
            "epoch 5 batch 42 loss 0.05757207050919533\n",
            "epoch 5 batch 43 loss 0.06160741671919823\n",
            "epoch 5 batch 44 loss 0.06360698491334915\n",
            "epoch 5 batch 45 loss 0.057298868894577026\n",
            "epoch 5 batch 46 loss 0.05567735806107521\n",
            "epoch 5 batch 47 loss 0.06368283927440643\n",
            "epoch 5 batch 48 loss 0.05316277593374252\n",
            "epoch 5 batch 49 loss 0.05515424162149429\n",
            "epoch 5 batch 50 loss 0.05865681916475296\n",
            "epoch 5 batch 51 loss 0.060818929225206375\n",
            "epoch 5 batch 52 loss 0.057181671261787415\n",
            "epoch 5 batch 53 loss 0.05255534127354622\n",
            "epoch 5 batch 54 loss 0.05773194134235382\n",
            "epoch 5 batch 55 loss 0.056987714022397995\n",
            "epoch 5 batch 56 loss 0.05313408747315407\n",
            "epoch 5 batch 57 loss 0.061848703771829605\n",
            "epoch 5 batch 58 loss 0.060653556138277054\n",
            "epoch 5 batch 59 loss 0.05608753114938736\n",
            "epoch 5 batch 60 loss 0.05731958895921707\n",
            "epoch 5 batch 61 loss 0.057186685502529144\n",
            "epoch 5 batch 62 loss 0.052341148257255554\n",
            "epoch 5 batch 63 loss 0.0569564625620842\n",
            "epoch 5 batch 64 loss 0.06159048154950142\n",
            "epoch 5 batch 65 loss 0.05238325893878937\n",
            "epoch 5 batch 66 loss 0.05826081708073616\n",
            "epoch 5 batch 67 loss 0.06224147230386734\n",
            "epoch 5 batch 68 loss 0.05653222277760506\n",
            "epoch 5 batch 69 loss 0.05676422268152237\n",
            "epoch 5 batch 70 loss 0.058799806982278824\n",
            "epoch 5 batch 71 loss 0.05142240226268768\n",
            "epoch 5 batch 72 loss 0.05513344705104828\n",
            "epoch 5 batch 73 loss 0.05958400294184685\n",
            "epoch 5 batch 74 loss 0.05642103776335716\n",
            "epoch 5 batch 75 loss 0.060202956199645996\n",
            "epoch 5 batch 76 loss 0.048157453536987305\n",
            "epoch 5 batch 77 loss 0.05740444362163544\n",
            "epoch 5 batch 78 loss 0.056371018290519714\n",
            "epoch 5 batch 79 loss 0.053633157163858414\n",
            "epoch 5 batch 80 loss 0.05739232897758484\n",
            "epoch 5 batch 81 loss 0.05682205408811569\n",
            "epoch 5 batch 82 loss 0.06114256754517555\n",
            "epoch 5 batch 83 loss 0.05195778235793114\n",
            "epoch 5 batch 84 loss 0.052328649908304214\n",
            "epoch 5 batch 85 loss 0.05715607851743698\n",
            "epoch 5 batch 86 loss 0.048842571675777435\n",
            "epoch 5 batch 87 loss 0.05548352003097534\n",
            "epoch 5 batch 88 loss 0.05406422168016434\n",
            "epoch 5 batch 89 loss 0.061747025698423386\n",
            "epoch 5 batch 90 loss 0.05769873037934303\n",
            "epoch 5 batch 91 loss 0.05920061096549034\n",
            "epoch 5 batch 92 loss 0.057750288397073746\n",
            "epoch 5 batch 93 loss 0.05963588505983353\n",
            "epoch 5 batch 94 loss 0.05483444035053253\n",
            "epoch 5 batch 95 loss 0.06053372099995613\n",
            "epoch 5 batch 96 loss 0.05495277792215347\n",
            "epoch 5 batch 97 loss 0.05578066036105156\n",
            "epoch 5 batch 98 loss 0.05434228107333183\n",
            "epoch 5 batch 99 loss 0.060064032673835754\n",
            "epoch 5 batch 100 loss 0.06288959085941315\n",
            "epoch 5 batch 101 loss 0.05120332911610603\n",
            "epoch 5 batch 102 loss 0.05330634117126465\n",
            "epoch 5 batch 103 loss 0.05958395451307297\n",
            "epoch 5 batch 104 loss 0.05513310059905052\n",
            "epoch 5 batch 105 loss 0.04906833916902542\n",
            "epoch 5 batch 106 loss 0.05124545469880104\n",
            "epoch 5 batch 107 loss 0.05327128246426582\n",
            "epoch 5 batch 108 loss 0.05517364665865898\n",
            "epoch 5 batch 109 loss 0.057018935680389404\n",
            "epoch 5 batch 110 loss 0.05066869035363197\n",
            "epoch 5 batch 111 loss 0.055943526327610016\n",
            "epoch 5 batch 112 loss 0.06339351832866669\n",
            "epoch 5 batch 113 loss 0.056214217096567154\n",
            "epoch 5 batch 114 loss 0.056463561952114105\n",
            "epoch 5 batch 115 loss 0.0593445710837841\n",
            "epoch 5 batch 116 loss 0.059261348098516464\n",
            "epoch 5 batch 117 loss 0.05923004075884819\n",
            "epoch 5 batch 118 loss 0.05719767138361931\n",
            "epoch 5 batch 119 loss 0.05928187817335129\n",
            "epoch 5 batch 120 loss 0.054854344576597214\n",
            "epoch 5 batch 121 loss 0.05696839094161987\n",
            "epoch 5 batch 122 loss 0.05776163563132286\n",
            "epoch 5 batch 123 loss 0.05498102307319641\n",
            "epoch 5 batch 124 loss 0.059036608785390854\n",
            "epoch 5 batch 125 loss 0.0560593344271183\n",
            "epoch 5 batch 126 loss 0.04985225573182106\n",
            "epoch 5 batch 127 loss 0.060839444398880005\n",
            "epoch 5 batch 128 loss 0.053826287388801575\n",
            "epoch 5 batch 129 loss 0.053133103996515274\n",
            "epoch 5 batch 130 loss 0.06564272940158844\n",
            "epoch 5 batch 131 loss 0.05761897563934326\n",
            "epoch 5 batch 132 loss 0.05943181738257408\n",
            "epoch 5 batch 133 loss 0.05269017070531845\n",
            "epoch 5 batch 134 loss 0.059057578444480896\n",
            "epoch 5 batch 135 loss 0.06086396053433418\n",
            "epoch 5 batch 136 loss 0.053588028997182846\n",
            "epoch 5 batch 137 loss 0.05463257431983948\n",
            "epoch 5 batch 138 loss 0.057180922478437424\n",
            "epoch 5 batch 139 loss 0.055617399513721466\n",
            "epoch 5 batch 140 loss 0.058357544243335724\n",
            "epoch 5 batch 141 loss 0.05237172916531563\n",
            "epoch 5 batch 142 loss 0.05070989206433296\n",
            "epoch 5 batch 143 loss 0.04491589963436127\n",
            "epoch 5 batch 144 loss 0.05254306271672249\n",
            "epoch 5 batch 145 loss 0.05602728947997093\n",
            "epoch 5 batch 146 loss 0.061226800084114075\n",
            "epoch 5 batch 147 loss 0.058334849774837494\n",
            "epoch 5 batch 148 loss 0.053225137293338776\n",
            "epoch 5 batch 149 loss 0.05586427450180054\n",
            "epoch 5 batch 150 loss 0.05613867565989494\n",
            "epoch 5 batch 151 loss 0.05286996439099312\n",
            "epoch 5 batch 152 loss 0.049725841730833054\n",
            "epoch 5 batch 153 loss 0.05450978875160217\n",
            "epoch 5 batch 154 loss 0.05574733763933182\n",
            "epoch 5 batch 155 loss 0.05459529161453247\n",
            "epoch 5 batch 156 loss 0.05783027037978172\n",
            "epoch 5 batch 157 loss 0.053376924246549606\n",
            "epoch 5 batch 158 loss 0.05220731720328331\n",
            "epoch 5 batch 159 loss 0.04611268267035484\n",
            "epoch 5 batch 160 loss 0.055552706122398376\n",
            "epoch 5 batch 161 loss 0.05205823481082916\n",
            "epoch 5 batch 162 loss 0.05880672484636307\n",
            "epoch 5 batch 163 loss 0.05592143535614014\n",
            "epoch 5 batch 164 loss 0.05530999228358269\n",
            "epoch 5 batch 165 loss 0.05280747637152672\n",
            "epoch 5 batch 166 loss 0.05984693765640259\n",
            "epoch 5 batch 167 loss 0.054988227784633636\n",
            "epoch 5 batch 168 loss 0.06246485188603401\n",
            "epoch 5 batch 169 loss 0.055030521005392075\n",
            "epoch 5 batch 170 loss 0.05139918997883797\n",
            "epoch 5 batch 171 loss 0.056341130286455154\n",
            "epoch 5 batch 172 loss 0.05763141065835953\n",
            "epoch 5 batch 173 loss 0.052640970796346664\n",
            "epoch 5 batch 174 loss 0.055766716599464417\n",
            "epoch 5 batch 175 loss 0.05857156589627266\n",
            "epoch 5 batch 176 loss 0.056653961539268494\n",
            "epoch 5 batch 177 loss 0.051112040877342224\n",
            "epoch 5 batch 178 loss 0.05293644592165947\n",
            "epoch 5 batch 179 loss 0.04998822882771492\n",
            "epoch 5 batch 180 loss 0.05420931428670883\n",
            "epoch 5 batch 181 loss 0.058137938380241394\n",
            "epoch 5 batch 182 loss 0.055876366794109344\n",
            "epoch 5 batch 183 loss 0.05795809626579285\n",
            "epoch 5 batch 184 loss 0.06523608416318893\n",
            "epoch 5 batch 185 loss 0.0565013624727726\n",
            "epoch 5 batch 186 loss 0.06587784737348557\n",
            "epoch 5 batch 187 loss 0.049744077026844025\n",
            "epoch 5 batch 188 loss 0.05065451189875603\n",
            "epoch 5 batch 189 loss 0.06219703331589699\n",
            "epoch 5 batch 190 loss 0.05176859721541405\n",
            "epoch 5 batch 191 loss 0.05870407074689865\n",
            "epoch 5 batch 192 loss 0.04853138327598572\n",
            "epoch 5 batch 193 loss 0.05155623331665993\n",
            "epoch 5 batch 194 loss 0.05180564150214195\n",
            "epoch 5 batch 195 loss 0.054275598376989365\n",
            "epoch 5 batch 196 loss 0.05662615969777107\n",
            "epoch 5 batch 197 loss 0.05300888046622276\n",
            "epoch 5 batch 198 loss 0.049288060516119\n",
            "epoch 5 batch 199 loss 0.06209747865796089\n",
            "epoch 5 batch 200 loss 0.057227764278650284\n",
            "epoch 5 batch 201 loss 0.057399630546569824\n",
            "epoch 5 batch 202 loss 0.05313694477081299\n",
            "epoch 5 batch 203 loss 0.05113344267010689\n",
            "epoch 5 batch 204 loss 0.045548319816589355\n",
            "epoch 5 batch 205 loss 0.05587470158934593\n",
            "epoch 5 batch 206 loss 0.044514551758766174\n",
            "epoch 5 batch 207 loss 0.05087098851799965\n",
            "epoch 5 batch 208 loss 0.061221346259117126\n",
            "epoch 5 batch 209 loss 0.053890883922576904\n",
            "epoch 5 batch 210 loss 0.060468606650829315\n",
            "epoch 5 batch 211 loss 0.055742572993040085\n",
            "epoch 5 batch 212 loss 0.051585111767053604\n",
            "epoch 5 batch 213 loss 0.051614921540021896\n",
            "epoch 5 batch 214 loss 0.053906623274087906\n",
            "epoch 5 batch 215 loss 0.049873992800712585\n",
            "epoch 5 batch 216 loss 0.05162714421749115\n",
            "epoch 5 batch 217 loss 0.044322047382593155\n",
            "epoch 5 batch 218 loss 0.0517200268805027\n",
            "epoch 5 batch 219 loss 0.055304646492004395\n",
            "epoch 5 batch 220 loss 0.05469082295894623\n",
            "epoch 5 batch 221 loss 0.052804622799158096\n",
            "epoch 5 batch 222 loss 0.056337788701057434\n",
            "epoch 5 batch 223 loss 0.054615043103694916\n",
            "epoch 5 batch 224 loss 0.05396245792508125\n",
            "epoch 5 batch 225 loss 0.054131049662828445\n",
            "epoch 5 batch 226 loss 0.053146395832300186\n",
            "epoch 5 batch 227 loss 0.05267123132944107\n",
            "epoch 5 batch 228 loss 0.05263906344771385\n",
            "epoch 5 batch 229 loss 0.05302921310067177\n",
            "epoch 5 batch 230 loss 0.047352977097034454\n",
            "epoch 6 batch 0 loss 0.05634710565209389\n",
            "epoch 6 batch 1 loss 0.05466075241565704\n",
            "epoch 6 batch 2 loss 0.0622669942677021\n",
            "epoch 6 batch 3 loss 0.05940625071525574\n",
            "epoch 6 batch 4 loss 0.05388250574469566\n",
            "epoch 6 batch 5 loss 0.05500415712594986\n",
            "epoch 6 batch 6 loss 0.05439530313014984\n",
            "epoch 6 batch 7 loss 0.06242949888110161\n",
            "epoch 6 batch 8 loss 0.05460013821721077\n",
            "epoch 6 batch 9 loss 0.05519027262926102\n",
            "epoch 6 batch 10 loss 0.06394854187965393\n",
            "epoch 6 batch 11 loss 0.05056314915418625\n",
            "epoch 6 batch 12 loss 0.05102572217583656\n",
            "epoch 6 batch 13 loss 0.0599210187792778\n",
            "epoch 6 batch 14 loss 0.05176885426044464\n",
            "epoch 6 batch 15 loss 0.055013783276081085\n",
            "epoch 6 batch 16 loss 0.05377360060811043\n",
            "epoch 6 batch 17 loss 0.05184415355324745\n",
            "epoch 6 batch 18 loss 0.052266526967287064\n",
            "epoch 6 batch 19 loss 0.05418478697538376\n",
            "epoch 6 batch 20 loss 0.05560560151934624\n",
            "epoch 6 batch 21 loss 0.055477552115917206\n",
            "epoch 6 batch 22 loss 0.047282345592975616\n",
            "epoch 6 batch 23 loss 0.055850800126791\n",
            "epoch 6 batch 24 loss 0.054094377905130386\n",
            "epoch 6 batch 25 loss 0.06379858404397964\n",
            "epoch 6 batch 26 loss 0.05295928195118904\n",
            "epoch 6 batch 27 loss 0.06120235472917557\n",
            "epoch 6 batch 28 loss 0.04752444103360176\n",
            "epoch 6 batch 29 loss 0.05393355339765549\n",
            "epoch 6 batch 30 loss 0.055301979184150696\n",
            "epoch 6 batch 31 loss 0.052462007850408554\n",
            "epoch 6 batch 32 loss 0.05155956745147705\n",
            "epoch 6 batch 33 loss 0.052391648292541504\n",
            "epoch 6 batch 34 loss 0.04957535117864609\n",
            "epoch 6 batch 35 loss 0.05039362981915474\n",
            "epoch 6 batch 36 loss 0.06608637422323227\n",
            "epoch 6 batch 37 loss 0.05621352791786194\n",
            "epoch 6 batch 38 loss 0.053076453506946564\n",
            "epoch 6 batch 39 loss 0.05571096017956734\n",
            "epoch 6 batch 40 loss 0.055244334042072296\n",
            "epoch 6 batch 41 loss 0.05200861021876335\n",
            "epoch 6 batch 42 loss 0.05383347347378731\n",
            "epoch 6 batch 43 loss 0.05843834578990936\n",
            "epoch 6 batch 44 loss 0.05742960423231125\n",
            "epoch 6 batch 45 loss 0.05298282206058502\n",
            "epoch 6 batch 46 loss 0.05505631864070892\n",
            "epoch 6 batch 47 loss 0.05713494494557381\n",
            "epoch 6 batch 48 loss 0.04884323105216026\n",
            "epoch 6 batch 49 loss 0.05407997593283653\n",
            "epoch 6 batch 50 loss 0.05625258758664131\n",
            "epoch 6 batch 51 loss 0.05487043038010597\n",
            "epoch 6 batch 52 loss 0.053720325231552124\n",
            "epoch 6 batch 53 loss 0.048710886389017105\n",
            "epoch 6 batch 54 loss 0.05370516702532768\n",
            "epoch 6 batch 55 loss 0.052754998207092285\n",
            "epoch 6 batch 56 loss 0.05046312138438225\n",
            "epoch 6 batch 57 loss 0.05827973037958145\n",
            "epoch 6 batch 58 loss 0.055504586547613144\n",
            "epoch 6 batch 59 loss 0.052379902452230453\n",
            "epoch 6 batch 60 loss 0.0542452372610569\n",
            "epoch 6 batch 61 loss 0.052901580929756165\n",
            "epoch 6 batch 62 loss 0.0494312085211277\n",
            "epoch 6 batch 63 loss 0.054603274911642075\n",
            "epoch 6 batch 64 loss 0.058161914348602295\n",
            "epoch 6 batch 65 loss 0.04883737862110138\n",
            "epoch 6 batch 66 loss 0.05604901537299156\n",
            "epoch 6 batch 67 loss 0.05868835374712944\n",
            "epoch 6 batch 68 loss 0.054048676043748856\n",
            "epoch 6 batch 69 loss 0.0542805977165699\n",
            "epoch 6 batch 70 loss 0.05505495145916939\n",
            "epoch 6 batch 71 loss 0.04893646016716957\n",
            "epoch 6 batch 72 loss 0.05156499892473221\n",
            "epoch 6 batch 73 loss 0.05636056512594223\n",
            "epoch 6 batch 74 loss 0.05214236304163933\n",
            "epoch 6 batch 75 loss 0.056008078157901764\n",
            "epoch 6 batch 76 loss 0.04460033029317856\n",
            "epoch 6 batch 77 loss 0.05358405038714409\n",
            "epoch 6 batch 78 loss 0.052431464195251465\n",
            "epoch 6 batch 79 loss 0.050865329802036285\n",
            "epoch 6 batch 80 loss 0.05431212857365608\n",
            "epoch 6 batch 81 loss 0.053030047565698624\n",
            "epoch 6 batch 82 loss 0.0562046617269516\n",
            "epoch 6 batch 83 loss 0.04783037677407265\n",
            "epoch 6 batch 84 loss 0.04999220743775368\n",
            "epoch 6 batch 85 loss 0.05293523892760277\n",
            "epoch 6 batch 86 loss 0.044849324971437454\n",
            "epoch 6 batch 87 loss 0.05136583000421524\n",
            "epoch 6 batch 88 loss 0.050302330404520035\n",
            "epoch 6 batch 89 loss 0.0558626689016819\n",
            "epoch 6 batch 90 loss 0.053026698529720306\n",
            "epoch 6 batch 91 loss 0.05448571965098381\n",
            "epoch 6 batch 92 loss 0.053067781031131744\n",
            "epoch 6 batch 93 loss 0.054240528494119644\n",
            "epoch 6 batch 94 loss 0.05025482550263405\n",
            "epoch 6 batch 95 loss 0.05633390322327614\n",
            "epoch 6 batch 96 loss 0.05120394378900528\n",
            "epoch 6 batch 97 loss 0.05230512470006943\n",
            "epoch 6 batch 98 loss 0.04959217831492424\n",
            "epoch 6 batch 99 loss 0.05559394508600235\n",
            "epoch 6 batch 100 loss 0.059505265206098557\n",
            "epoch 6 batch 101 loss 0.04728591442108154\n",
            "epoch 6 batch 102 loss 0.05011891573667526\n",
            "epoch 6 batch 103 loss 0.05585656687617302\n",
            "epoch 6 batch 104 loss 0.05105665698647499\n",
            "epoch 6 batch 105 loss 0.04680537059903145\n",
            "epoch 6 batch 106 loss 0.047812577337026596\n",
            "epoch 6 batch 107 loss 0.0514787882566452\n",
            "epoch 6 batch 108 loss 0.05185435339808464\n",
            "epoch 6 batch 109 loss 0.05294209346175194\n",
            "epoch 6 batch 110 loss 0.04736790433526039\n",
            "epoch 6 batch 111 loss 0.05175132676959038\n",
            "epoch 6 batch 112 loss 0.06039508804678917\n",
            "epoch 6 batch 113 loss 0.052036531269550323\n",
            "epoch 6 batch 114 loss 0.05108215659856796\n",
            "epoch 6 batch 115 loss 0.05555588752031326\n",
            "epoch 6 batch 116 loss 0.05391482636332512\n",
            "epoch 6 batch 117 loss 0.05641625449061394\n",
            "epoch 6 batch 118 loss 0.053117480129003525\n",
            "epoch 6 batch 119 loss 0.055678825825452805\n",
            "epoch 6 batch 120 loss 0.05177619308233261\n",
            "epoch 6 batch 121 loss 0.05263474956154823\n",
            "epoch 6 batch 122 loss 0.05433759465813637\n",
            "epoch 6 batch 123 loss 0.0526672899723053\n",
            "epoch 6 batch 124 loss 0.05549446865916252\n",
            "epoch 6 batch 125 loss 0.05009999871253967\n",
            "epoch 6 batch 126 loss 0.04663221165537834\n",
            "epoch 6 batch 127 loss 0.057937126606702805\n",
            "epoch 6 batch 128 loss 0.05047942325472832\n",
            "epoch 6 batch 129 loss 0.049228180199861526\n",
            "epoch 6 batch 130 loss 0.0616079606115818\n",
            "epoch 6 batch 131 loss 0.056839339435100555\n",
            "epoch 6 batch 132 loss 0.05593481659889221\n",
            "epoch 6 batch 133 loss 0.049552515149116516\n",
            "epoch 6 batch 134 loss 0.057893380522727966\n",
            "epoch 6 batch 135 loss 0.05786791443824768\n",
            "epoch 6 batch 136 loss 0.050488702952861786\n",
            "epoch 6 batch 137 loss 0.05373833701014519\n",
            "epoch 6 batch 138 loss 0.052090808749198914\n",
            "epoch 6 batch 139 loss 0.05281311646103859\n",
            "epoch 6 batch 140 loss 0.055118873715400696\n",
            "epoch 6 batch 141 loss 0.04972293600440025\n",
            "epoch 6 batch 142 loss 0.048581771552562714\n",
            "epoch 6 batch 143 loss 0.042195916175842285\n",
            "epoch 6 batch 144 loss 0.04981367290019989\n",
            "epoch 6 batch 145 loss 0.05403756722807884\n",
            "epoch 6 batch 146 loss 0.05794179067015648\n",
            "epoch 6 batch 147 loss 0.055059392005205154\n",
            "epoch 6 batch 148 loss 0.05009070783853531\n",
            "epoch 6 batch 149 loss 0.05262846127152443\n",
            "epoch 6 batch 150 loss 0.052990663796663284\n",
            "epoch 6 batch 151 loss 0.04993303865194321\n",
            "epoch 6 batch 152 loss 0.047388650476932526\n",
            "epoch 6 batch 153 loss 0.0509917214512825\n",
            "epoch 6 batch 154 loss 0.05288344994187355\n",
            "epoch 6 batch 155 loss 0.051313236355781555\n",
            "epoch 6 batch 156 loss 0.05456212908029556\n",
            "epoch 6 batch 157 loss 0.049962110817432404\n",
            "epoch 6 batch 158 loss 0.049042489379644394\n",
            "epoch 6 batch 159 loss 0.04345686733722687\n",
            "epoch 6 batch 160 loss 0.052745040506124496\n",
            "epoch 6 batch 161 loss 0.04845985397696495\n",
            "epoch 6 batch 162 loss 0.05453117936849594\n",
            "epoch 6 batch 163 loss 0.052410565316677094\n",
            "epoch 6 batch 164 loss 0.051287941634655\n",
            "epoch 6 batch 165 loss 0.04988058656454086\n",
            "epoch 6 batch 166 loss 0.05668218061327934\n",
            "epoch 6 batch 167 loss 0.051376085728406906\n",
            "epoch 6 batch 168 loss 0.05748889967799187\n",
            "epoch 6 batch 169 loss 0.05198002979159355\n",
            "epoch 6 batch 170 loss 0.04960789903998375\n",
            "epoch 6 batch 171 loss 0.051758065819740295\n",
            "epoch 6 batch 172 loss 0.053971149027347565\n",
            "epoch 6 batch 173 loss 0.04935982823371887\n",
            "epoch 6 batch 174 loss 0.0513094924390316\n",
            "epoch 6 batch 175 loss 0.054964061826467514\n",
            "epoch 6 batch 176 loss 0.05210518464446068\n",
            "epoch 6 batch 177 loss 0.04736708104610443\n",
            "epoch 6 batch 178 loss 0.04916169121861458\n",
            "epoch 6 batch 179 loss 0.047449517995119095\n",
            "epoch 6 batch 180 loss 0.05199001729488373\n",
            "epoch 6 batch 181 loss 0.054297126829624176\n",
            "epoch 6 batch 182 loss 0.05202597379684448\n",
            "epoch 6 batch 183 loss 0.05374467372894287\n",
            "epoch 6 batch 184 loss 0.06215757876634598\n",
            "epoch 6 batch 185 loss 0.053696561604738235\n",
            "epoch 6 batch 186 loss 0.061327412724494934\n",
            "epoch 6 batch 187 loss 0.046297792345285416\n",
            "epoch 6 batch 188 loss 0.04799995571374893\n",
            "epoch 6 batch 189 loss 0.058963578194379807\n",
            "epoch 6 batch 190 loss 0.04798707365989685\n",
            "epoch 6 batch 191 loss 0.05512365326285362\n",
            "epoch 6 batch 192 loss 0.04521263390779495\n",
            "epoch 6 batch 193 loss 0.047314222902059555\n",
            "epoch 6 batch 194 loss 0.04986390843987465\n",
            "epoch 6 batch 195 loss 0.050498686730861664\n",
            "epoch 6 batch 196 loss 0.05315157771110535\n",
            "epoch 6 batch 197 loss 0.048271726816892624\n",
            "epoch 6 batch 198 loss 0.046112656593322754\n",
            "epoch 6 batch 199 loss 0.05812728404998779\n",
            "epoch 6 batch 200 loss 0.05352884158492088\n",
            "epoch 6 batch 201 loss 0.05260121449828148\n",
            "epoch 6 batch 202 loss 0.048804860562086105\n",
            "epoch 6 batch 203 loss 0.04832767695188522\n",
            "epoch 6 batch 204 loss 0.04322496056556702\n",
            "epoch 6 batch 205 loss 0.053117536008358\n",
            "epoch 6 batch 206 loss 0.04066407307982445\n",
            "epoch 6 batch 207 loss 0.04783976823091507\n",
            "epoch 6 batch 208 loss 0.057627931237220764\n",
            "epoch 6 batch 209 loss 0.05065389722585678\n",
            "epoch 6 batch 210 loss 0.05727682262659073\n",
            "epoch 6 batch 211 loss 0.05255025252699852\n",
            "epoch 6 batch 212 loss 0.04888365417718887\n",
            "epoch 6 batch 213 loss 0.04901886358857155\n",
            "epoch 6 batch 214 loss 0.05152487754821777\n",
            "epoch 6 batch 215 loss 0.04579557105898857\n",
            "epoch 6 batch 216 loss 0.04790402948856354\n",
            "epoch 6 batch 217 loss 0.042548611760139465\n",
            "epoch 6 batch 218 loss 0.04899440333247185\n",
            "epoch 6 batch 219 loss 0.053209878504276276\n",
            "epoch 6 batch 220 loss 0.05095228552818298\n",
            "epoch 6 batch 221 loss 0.04998619481921196\n",
            "epoch 6 batch 222 loss 0.05304170399904251\n",
            "epoch 6 batch 223 loss 0.051880016922950745\n",
            "epoch 6 batch 224 loss 0.05172166973352432\n",
            "epoch 6 batch 225 loss 0.05061032623052597\n",
            "epoch 6 batch 226 loss 0.05064365640282631\n",
            "epoch 6 batch 227 loss 0.04902917519211769\n",
            "epoch 6 batch 228 loss 0.05022072419524193\n",
            "epoch 6 batch 229 loss 0.050331778824329376\n",
            "epoch 6 batch 230 loss 0.04179592430591583\n",
            "epoch 7 batch 0 loss 0.05243876576423645\n",
            "epoch 7 batch 1 loss 0.05254041776061058\n",
            "epoch 7 batch 2 loss 0.058204807341098785\n",
            "epoch 7 batch 3 loss 0.054374512284994125\n",
            "epoch 7 batch 4 loss 0.05122675001621246\n",
            "epoch 7 batch 5 loss 0.051070332527160645\n",
            "epoch 7 batch 6 loss 0.051788393408060074\n",
            "epoch 7 batch 7 loss 0.05856034532189369\n",
            "epoch 7 batch 8 loss 0.050842031836509705\n",
            "epoch 7 batch 9 loss 0.05254568159580231\n",
            "epoch 7 batch 10 loss 0.06045583263039589\n",
            "epoch 7 batch 11 loss 0.0464821383357048\n",
            "epoch 7 batch 12 loss 0.048037681728601456\n",
            "epoch 7 batch 13 loss 0.05667184293270111\n",
            "epoch 7 batch 14 loss 0.048303667455911636\n",
            "epoch 7 batch 15 loss 0.051154084503650665\n",
            "epoch 7 batch 16 loss 0.049162358045578\n",
            "epoch 7 batch 17 loss 0.048759885132312775\n",
            "epoch 7 batch 18 loss 0.05030938237905502\n",
            "epoch 7 batch 19 loss 0.05104199796915054\n",
            "epoch 7 batch 20 loss 0.051869384944438934\n",
            "epoch 7 batch 21 loss 0.05070421099662781\n",
            "epoch 7 batch 22 loss 0.04458446055650711\n",
            "epoch 7 batch 23 loss 0.05196080729365349\n",
            "epoch 7 batch 24 loss 0.05048711597919464\n",
            "epoch 7 batch 25 loss 0.06004835665225983\n",
            "epoch 7 batch 26 loss 0.05041712895035744\n",
            "epoch 7 batch 27 loss 0.05795770511031151\n",
            "epoch 7 batch 28 loss 0.044480059295892715\n",
            "epoch 7 batch 29 loss 0.05097874626517296\n",
            "epoch 7 batch 30 loss 0.05148689076304436\n",
            "epoch 7 batch 31 loss 0.050092604011297226\n",
            "epoch 7 batch 32 loss 0.04788541421294212\n",
            "epoch 7 batch 33 loss 0.048559486865997314\n",
            "epoch 7 batch 34 loss 0.048429884016513824\n",
            "epoch 7 batch 35 loss 0.048182982951402664\n",
            "epoch 7 batch 36 loss 0.060946740210056305\n",
            "epoch 7 batch 37 loss 0.053465597331523895\n",
            "epoch 7 batch 38 loss 0.04979968070983887\n",
            "epoch 7 batch 39 loss 0.05136007443070412\n",
            "epoch 7 batch 40 loss 0.05153311416506767\n",
            "epoch 7 batch 41 loss 0.05013636499643326\n",
            "epoch 7 batch 42 loss 0.049955014139413834\n",
            "epoch 7 batch 43 loss 0.05332816764712334\n",
            "epoch 7 batch 44 loss 0.054754436016082764\n",
            "epoch 7 batch 45 loss 0.04933376610279083\n",
            "epoch 7 batch 46 loss 0.04995689168572426\n",
            "epoch 7 batch 47 loss 0.05428752303123474\n",
            "epoch 7 batch 48 loss 0.04480075463652611\n",
            "epoch 7 batch 49 loss 0.0493156798183918\n",
            "epoch 7 batch 50 loss 0.05352308973670006\n",
            "epoch 7 batch 51 loss 0.05114477127790451\n",
            "epoch 7 batch 52 loss 0.049996908754110336\n",
            "epoch 7 batch 53 loss 0.045798029750585556\n",
            "epoch 7 batch 54 loss 0.051720257848501205\n",
            "epoch 7 batch 55 loss 0.04980406537652016\n",
            "epoch 7 batch 56 loss 0.046887755393981934\n",
            "epoch 7 batch 57 loss 0.056932009756565094\n",
            "epoch 7 batch 58 loss 0.053120218217372894\n",
            "epoch 7 batch 59 loss 0.04876668006181717\n",
            "epoch 7 batch 60 loss 0.051432106643915176\n",
            "epoch 7 batch 61 loss 0.05252443253993988\n",
            "epoch 7 batch 62 loss 0.04675677418708801\n",
            "epoch 7 batch 63 loss 0.05380092188715935\n",
            "epoch 7 batch 64 loss 0.05550937354564667\n",
            "epoch 7 batch 65 loss 0.047542765736579895\n",
            "epoch 7 batch 66 loss 0.05454282835125923\n",
            "epoch 7 batch 67 loss 0.05560023710131645\n",
            "epoch 7 batch 68 loss 0.052161723375320435\n",
            "epoch 7 batch 69 loss 0.05324987322092056\n",
            "epoch 7 batch 70 loss 0.05242721363902092\n",
            "epoch 7 batch 71 loss 0.0467788390815258\n",
            "epoch 7 batch 72 loss 0.04930322244763374\n",
            "epoch 7 batch 73 loss 0.052803680300712585\n",
            "epoch 7 batch 74 loss 0.04856861010193825\n",
            "epoch 7 batch 75 loss 0.05399937182664871\n",
            "epoch 7 batch 76 loss 0.04411991313099861\n",
            "epoch 7 batch 77 loss 0.05078510940074921\n",
            "epoch 7 batch 78 loss 0.05019524320960045\n",
            "epoch 7 batch 79 loss 0.04731407016515732\n",
            "epoch 7 batch 80 loss 0.05070989951491356\n",
            "epoch 7 batch 81 loss 0.04985884204506874\n",
            "epoch 7 batch 82 loss 0.053795527666807175\n",
            "epoch 7 batch 83 loss 0.04416791722178459\n",
            "epoch 7 batch 84 loss 0.048175130039453506\n",
            "epoch 7 batch 85 loss 0.050610654056072235\n",
            "epoch 7 batch 86 loss 0.04274284467101097\n",
            "epoch 7 batch 87 loss 0.049445997923612595\n",
            "epoch 7 batch 88 loss 0.04693446680903435\n",
            "epoch 7 batch 89 loss 0.05279594287276268\n",
            "epoch 7 batch 90 loss 0.048890840262174606\n",
            "epoch 7 batch 91 loss 0.05046762153506279\n",
            "epoch 7 batch 92 loss 0.04885236173868179\n",
            "epoch 7 batch 93 loss 0.0512232780456543\n",
            "epoch 7 batch 94 loss 0.0478573776781559\n",
            "epoch 7 batch 95 loss 0.05132520943880081\n",
            "epoch 7 batch 96 loss 0.048916369676589966\n",
            "epoch 7 batch 97 loss 0.049495354294776917\n",
            "epoch 7 batch 98 loss 0.046400438994169235\n",
            "epoch 7 batch 99 loss 0.052122898399829865\n",
            "epoch 7 batch 100 loss 0.05698142573237419\n",
            "epoch 7 batch 101 loss 0.044576387852430344\n",
            "epoch 7 batch 102 loss 0.04686714708805084\n",
            "epoch 7 batch 103 loss 0.05244947224855423\n",
            "epoch 7 batch 104 loss 0.04743482917547226\n",
            "epoch 7 batch 105 loss 0.04280345141887665\n",
            "epoch 7 batch 106 loss 0.045657213777303696\n",
            "epoch 7 batch 107 loss 0.04847487807273865\n",
            "epoch 7 batch 108 loss 0.05005252733826637\n",
            "epoch 7 batch 109 loss 0.05044806748628616\n",
            "epoch 7 batch 110 loss 0.04385564848780632\n",
            "epoch 7 batch 111 loss 0.04962010681629181\n",
            "epoch 7 batch 112 loss 0.056033018976449966\n",
            "epoch 7 batch 113 loss 0.04850248247385025\n",
            "epoch 7 batch 114 loss 0.047945670783519745\n",
            "epoch 7 batch 115 loss 0.05055028945207596\n",
            "epoch 7 batch 116 loss 0.04998818039894104\n",
            "epoch 7 batch 117 loss 0.05250011384487152\n",
            "epoch 7 batch 118 loss 0.050103336572647095\n",
            "epoch 7 batch 119 loss 0.050724953413009644\n",
            "epoch 7 batch 120 loss 0.04825751855969429\n",
            "epoch 7 batch 121 loss 0.05046873167157173\n",
            "epoch 7 batch 122 loss 0.04973267763853073\n",
            "epoch 7 batch 123 loss 0.04786575213074684\n",
            "epoch 7 batch 124 loss 0.052743587642908096\n",
            "epoch 7 batch 125 loss 0.045939911156892776\n",
            "epoch 7 batch 126 loss 0.042858414351940155\n",
            "epoch 7 batch 127 loss 0.055582039058208466\n",
            "epoch 7 batch 128 loss 0.04757436364889145\n",
            "epoch 7 batch 129 loss 0.046094272285699844\n",
            "epoch 7 batch 130 loss 0.059545114636421204\n",
            "epoch 7 batch 131 loss 0.05096308887004852\n",
            "epoch 7 batch 132 loss 0.05227307975292206\n",
            "epoch 7 batch 133 loss 0.04692454636096954\n",
            "epoch 7 batch 134 loss 0.0515073761343956\n",
            "epoch 7 batch 135 loss 0.05494140088558197\n",
            "epoch 7 batch 136 loss 0.04870656505227089\n",
            "epoch 7 batch 137 loss 0.04811924695968628\n",
            "epoch 7 batch 138 loss 0.04883440211415291\n",
            "epoch 7 batch 139 loss 0.0487503781914711\n",
            "epoch 7 batch 140 loss 0.05144715681672096\n",
            "epoch 7 batch 141 loss 0.04709136113524437\n",
            "epoch 7 batch 142 loss 0.045001085847616196\n",
            "epoch 7 batch 143 loss 0.03965086489915848\n",
            "epoch 7 batch 144 loss 0.04716971889138222\n",
            "epoch 7 batch 145 loss 0.049844082444906235\n",
            "epoch 7 batch 146 loss 0.0539785772562027\n",
            "epoch 7 batch 147 loss 0.053463228046894073\n",
            "epoch 7 batch 148 loss 0.04775180667638779\n",
            "epoch 7 batch 149 loss 0.04862242192029953\n",
            "epoch 7 batch 150 loss 0.049958836287260056\n",
            "epoch 7 batch 151 loss 0.04839296266436577\n",
            "epoch 7 batch 152 loss 0.04318845272064209\n",
            "epoch 7 batch 153 loss 0.04841691255569458\n",
            "epoch 7 batch 154 loss 0.05037347227334976\n",
            "epoch 7 batch 155 loss 0.04793935269117355\n",
            "epoch 7 batch 156 loss 0.05091479793190956\n",
            "epoch 7 batch 157 loss 0.047993920743465424\n",
            "epoch 7 batch 158 loss 0.04669862240552902\n",
            "epoch 7 batch 159 loss 0.0409700870513916\n",
            "epoch 7 batch 160 loss 0.04903823137283325\n",
            "epoch 7 batch 161 loss 0.04652797803282738\n",
            "epoch 7 batch 162 loss 0.051546141505241394\n",
            "epoch 7 batch 163 loss 0.04939795285463333\n",
            "epoch 7 batch 164 loss 0.048290055245161057\n",
            "epoch 7 batch 165 loss 0.047123853117227554\n",
            "epoch 7 batch 166 loss 0.05339011549949646\n",
            "epoch 7 batch 167 loss 0.04877405986189842\n",
            "epoch 7 batch 168 loss 0.05490801855921745\n",
            "epoch 7 batch 169 loss 0.048569321632385254\n",
            "epoch 7 batch 170 loss 0.04688740149140358\n",
            "epoch 7 batch 171 loss 0.048904359340667725\n",
            "epoch 7 batch 172 loss 0.05075991898775101\n",
            "epoch 7 batch 173 loss 0.0461382232606411\n",
            "epoch 7 batch 174 loss 0.04910212382674217\n",
            "epoch 7 batch 175 loss 0.05147561430931091\n",
            "epoch 7 batch 176 loss 0.04971544072031975\n",
            "epoch 7 batch 177 loss 0.04473385587334633\n",
            "epoch 7 batch 178 loss 0.04643084108829498\n",
            "epoch 7 batch 179 loss 0.04459013789892197\n",
            "epoch 7 batch 180 loss 0.04976975917816162\n",
            "epoch 7 batch 181 loss 0.05121178179979324\n",
            "epoch 7 batch 182 loss 0.04924933984875679\n",
            "epoch 7 batch 183 loss 0.050337519496679306\n",
            "epoch 7 batch 184 loss 0.05860593169927597\n",
            "epoch 7 batch 185 loss 0.050323981791734695\n",
            "epoch 7 batch 186 loss 0.05693110451102257\n",
            "epoch 7 batch 187 loss 0.043396808207035065\n",
            "epoch 7 batch 188 loss 0.04540816321969032\n",
            "epoch 7 batch 189 loss 0.05550650879740715\n",
            "epoch 7 batch 190 loss 0.04591796547174454\n",
            "epoch 7 batch 191 loss 0.05229082703590393\n",
            "epoch 7 batch 192 loss 0.04329488426446915\n",
            "epoch 7 batch 193 loss 0.04437274858355522\n",
            "epoch 7 batch 194 loss 0.04662952572107315\n",
            "epoch 7 batch 195 loss 0.04869462549686432\n",
            "epoch 7 batch 196 loss 0.0491759218275547\n",
            "epoch 7 batch 197 loss 0.04583902284502983\n",
            "epoch 7 batch 198 loss 0.04404095932841301\n",
            "epoch 7 batch 199 loss 0.05582806095480919\n",
            "epoch 7 batch 200 loss 0.05109991878271103\n",
            "epoch 7 batch 201 loss 0.05006444826722145\n",
            "epoch 7 batch 202 loss 0.04585932567715645\n",
            "epoch 7 batch 203 loss 0.0458308681845665\n",
            "epoch 7 batch 204 loss 0.04068564251065254\n",
            "epoch 7 batch 205 loss 0.05096738412976265\n",
            "epoch 7 batch 206 loss 0.03902982175350189\n",
            "epoch 7 batch 207 loss 0.044466160237789154\n",
            "epoch 7 batch 208 loss 0.05456874892115593\n",
            "epoch 7 batch 209 loss 0.04880623146891594\n",
            "epoch 7 batch 210 loss 0.056089285761117935\n",
            "epoch 7 batch 211 loss 0.0500376895070076\n",
            "epoch 7 batch 212 loss 0.046396654099226\n",
            "epoch 7 batch 213 loss 0.04667336493730545\n",
            "epoch 7 batch 214 loss 0.048329368233680725\n",
            "epoch 7 batch 215 loss 0.04392850026488304\n",
            "epoch 7 batch 216 loss 0.045619864016771317\n",
            "epoch 7 batch 217 loss 0.04019641503691673\n",
            "epoch 7 batch 218 loss 0.046423088759183884\n",
            "epoch 7 batch 219 loss 0.049578312784433365\n",
            "epoch 7 batch 220 loss 0.04795778542757034\n",
            "epoch 7 batch 221 loss 0.04742875322699547\n",
            "epoch 7 batch 222 loss 0.05010092258453369\n",
            "epoch 7 batch 223 loss 0.04896971583366394\n",
            "epoch 7 batch 224 loss 0.0501641184091568\n",
            "epoch 7 batch 225 loss 0.048361409455537796\n",
            "epoch 7 batch 226 loss 0.047435350716114044\n",
            "epoch 7 batch 227 loss 0.04669006168842316\n",
            "epoch 7 batch 228 loss 0.045372482389211655\n",
            "epoch 7 batch 229 loss 0.04696318879723549\n",
            "epoch 7 batch 230 loss 0.03684065490961075\n",
            "epoch 8 batch 0 loss 0.04918001592159271\n",
            "epoch 8 batch 1 loss 0.0498916357755661\n",
            "epoch 8 batch 2 loss 0.05467852205038071\n",
            "epoch 8 batch 3 loss 0.05070355534553528\n",
            "epoch 8 batch 4 loss 0.04840679466724396\n",
            "epoch 8 batch 5 loss 0.04874366149306297\n",
            "epoch 8 batch 6 loss 0.048290301114320755\n",
            "epoch 8 batch 7 loss 0.05693585425615311\n",
            "epoch 8 batch 8 loss 0.04859029874205589\n",
            "epoch 8 batch 9 loss 0.04838498681783676\n",
            "epoch 8 batch 10 loss 0.05763427913188934\n",
            "epoch 8 batch 11 loss 0.045021649450063705\n",
            "epoch 8 batch 12 loss 0.045118752866983414\n",
            "epoch 8 batch 13 loss 0.05380009487271309\n",
            "epoch 8 batch 14 loss 0.045183975249528885\n",
            "epoch 8 batch 15 loss 0.04866898059844971\n",
            "epoch 8 batch 16 loss 0.04632805287837982\n",
            "epoch 8 batch 17 loss 0.046820878982543945\n",
            "epoch 8 batch 18 loss 0.04637884721159935\n",
            "epoch 8 batch 19 loss 0.04817688837647438\n",
            "epoch 8 batch 20 loss 0.050424497574567795\n",
            "epoch 8 batch 21 loss 0.047860000282526016\n",
            "epoch 8 batch 22 loss 0.04319215938448906\n",
            "epoch 8 batch 23 loss 0.04991091042757034\n",
            "epoch 8 batch 24 loss 0.04700053110718727\n",
            "epoch 8 batch 25 loss 0.05697217211127281\n",
            "epoch 8 batch 26 loss 0.04810865595936775\n",
            "epoch 8 batch 27 loss 0.05514882504940033\n",
            "epoch 8 batch 28 loss 0.04187937453389168\n",
            "epoch 8 batch 29 loss 0.049616873264312744\n",
            "epoch 8 batch 30 loss 0.04949710890650749\n",
            "epoch 8 batch 31 loss 0.046576593071222305\n",
            "epoch 8 batch 32 loss 0.04562734439969063\n",
            "epoch 8 batch 33 loss 0.046580128371715546\n",
            "epoch 8 batch 34 loss 0.044537462294101715\n",
            "epoch 8 batch 35 loss 0.046633243560791016\n",
            "epoch 8 batch 36 loss 0.05625604838132858\n",
            "epoch 8 batch 37 loss 0.05044884979724884\n",
            "epoch 8 batch 38 loss 0.04879732429981232\n",
            "epoch 8 batch 39 loss 0.04525795206427574\n",
            "epoch 8 batch 40 loss 0.04846800118684769\n",
            "epoch 8 batch 41 loss 0.04799957200884819\n",
            "epoch 8 batch 42 loss 0.04626840725541115\n",
            "epoch 8 batch 43 loss 0.05002318695187569\n",
            "epoch 8 batch 44 loss 0.05295747518539429\n",
            "epoch 8 batch 45 loss 0.045083336532115936\n",
            "epoch 8 batch 46 loss 0.046683624386787415\n",
            "epoch 8 batch 47 loss 0.05154681205749512\n",
            "epoch 8 batch 48 loss 0.04261172190308571\n",
            "epoch 8 batch 49 loss 0.04621083661913872\n",
            "epoch 8 batch 50 loss 0.04998868331313133\n",
            "epoch 8 batch 51 loss 0.04876083508133888\n",
            "epoch 8 batch 52 loss 0.047108471393585205\n",
            "epoch 8 batch 53 loss 0.04227183386683464\n",
            "epoch 8 batch 54 loss 0.04816557466983795\n",
            "epoch 8 batch 55 loss 0.04619712755084038\n",
            "epoch 8 batch 56 loss 0.04440492391586304\n",
            "epoch 8 batch 57 loss 0.05225319787859917\n",
            "epoch 8 batch 58 loss 0.04947856813669205\n",
            "epoch 8 batch 59 loss 0.046167366206645966\n",
            "epoch 8 batch 60 loss 0.04652953892946243\n",
            "epoch 8 batch 61 loss 0.04827956110239029\n",
            "epoch 8 batch 62 loss 0.04450508579611778\n",
            "epoch 8 batch 63 loss 0.04773685336112976\n",
            "epoch 8 batch 64 loss 0.05172392725944519\n",
            "epoch 8 batch 65 loss 0.04482337832450867\n",
            "epoch 8 batch 66 loss 0.05046539381146431\n",
            "epoch 8 batch 67 loss 0.05321148782968521\n",
            "epoch 8 batch 68 loss 0.047733429819345474\n",
            "epoch 8 batch 69 loss 0.04885435104370117\n",
            "epoch 8 batch 70 loss 0.04905388504266739\n",
            "epoch 8 batch 71 loss 0.04396562650799751\n",
            "epoch 8 batch 72 loss 0.04498250037431717\n",
            "epoch 8 batch 73 loss 0.050046950578689575\n",
            "epoch 8 batch 74 loss 0.04583943262696266\n",
            "epoch 8 batch 75 loss 0.05078132823109627\n",
            "epoch 8 batch 76 loss 0.04048281908035278\n",
            "epoch 8 batch 77 loss 0.04919219762086868\n",
            "epoch 8 batch 78 loss 0.04833497479557991\n",
            "epoch 8 batch 79 loss 0.04408007115125656\n",
            "epoch 8 batch 80 loss 0.049088384956121445\n",
            "epoch 8 batch 81 loss 0.04653962701559067\n",
            "epoch 8 batch 82 loss 0.05013478174805641\n",
            "epoch 8 batch 83 loss 0.04321902245283127\n",
            "epoch 8 batch 84 loss 0.044944409281015396\n",
            "epoch 8 batch 85 loss 0.047436218708753586\n",
            "epoch 8 batch 86 loss 0.04077724739909172\n",
            "epoch 8 batch 87 loss 0.04577526077628136\n",
            "epoch 8 batch 88 loss 0.04382675513625145\n",
            "epoch 8 batch 89 loss 0.05078211426734924\n",
            "epoch 8 batch 90 loss 0.04632509872317314\n",
            "epoch 8 batch 91 loss 0.048128221184015274\n",
            "epoch 8 batch 92 loss 0.04759600758552551\n",
            "epoch 8 batch 93 loss 0.04819924756884575\n",
            "epoch 8 batch 94 loss 0.04538988694548607\n",
            "epoch 8 batch 95 loss 0.04876270145177841\n",
            "epoch 8 batch 96 loss 0.045997124165296555\n",
            "epoch 8 batch 97 loss 0.04671470820903778\n",
            "epoch 8 batch 98 loss 0.044086311012506485\n",
            "epoch 8 batch 99 loss 0.04889281094074249\n",
            "epoch 8 batch 100 loss 0.05391313508152962\n",
            "epoch 8 batch 101 loss 0.04307309910655022\n",
            "epoch 8 batch 102 loss 0.044673796743154526\n",
            "epoch 8 batch 103 loss 0.04834466427564621\n",
            "epoch 8 batch 104 loss 0.044751666486263275\n",
            "epoch 8 batch 105 loss 0.041268471628427505\n",
            "epoch 8 batch 106 loss 0.04197883978486061\n",
            "epoch 8 batch 107 loss 0.044768158346414566\n",
            "epoch 8 batch 108 loss 0.046433981508016586\n",
            "epoch 8 batch 109 loss 0.047173626720905304\n",
            "epoch 8 batch 110 loss 0.04152851551771164\n",
            "epoch 8 batch 111 loss 0.04608069360256195\n",
            "epoch 8 batch 112 loss 0.05190702900290489\n",
            "epoch 8 batch 113 loss 0.04560232535004616\n",
            "epoch 8 batch 114 loss 0.04432954266667366\n",
            "epoch 8 batch 115 loss 0.047380346804857254\n",
            "epoch 8 batch 116 loss 0.0467945821583271\n",
            "epoch 8 batch 117 loss 0.04863136261701584\n",
            "epoch 8 batch 118 loss 0.046129222959280014\n",
            "epoch 8 batch 119 loss 0.04856741055846214\n",
            "epoch 8 batch 120 loss 0.044858451932668686\n",
            "epoch 8 batch 121 loss 0.04671023413538933\n",
            "epoch 8 batch 122 loss 0.04674084112048149\n",
            "epoch 8 batch 123 loss 0.044865742325782776\n",
            "epoch 8 batch 124 loss 0.04929538071155548\n",
            "epoch 8 batch 125 loss 0.044020455330610275\n",
            "epoch 8 batch 126 loss 0.040044818073511124\n",
            "epoch 8 batch 127 loss 0.05260162428021431\n",
            "epoch 8 batch 128 loss 0.04649822786450386\n",
            "epoch 8 batch 129 loss 0.04240318387746811\n",
            "epoch 8 batch 130 loss 0.05548848956823349\n",
            "epoch 8 batch 131 loss 0.04887395352125168\n",
            "epoch 8 batch 132 loss 0.04794696718454361\n",
            "epoch 8 batch 133 loss 0.04434335231781006\n",
            "epoch 8 batch 134 loss 0.050697725266218185\n",
            "epoch 8 batch 135 loss 0.05167112499475479\n",
            "epoch 8 batch 136 loss 0.045340847223997116\n",
            "epoch 8 batch 137 loss 0.04554017633199692\n",
            "epoch 8 batch 138 loss 0.0458872988820076\n",
            "epoch 8 batch 139 loss 0.045645564794540405\n",
            "epoch 8 batch 140 loss 0.04888099059462547\n",
            "epoch 8 batch 141 loss 0.04368989169597626\n",
            "epoch 8 batch 142 loss 0.040882062166929245\n",
            "epoch 8 batch 143 loss 0.037030283361673355\n",
            "epoch 8 batch 144 loss 0.04358021914958954\n",
            "epoch 8 batch 145 loss 0.047245439141988754\n",
            "epoch 8 batch 146 loss 0.05261187255382538\n",
            "epoch 8 batch 147 loss 0.04895244538784027\n",
            "epoch 8 batch 148 loss 0.045429665595293045\n",
            "epoch 8 batch 149 loss 0.0467398576438427\n",
            "epoch 8 batch 150 loss 0.047842755913734436\n",
            "epoch 8 batch 151 loss 0.04467810317873955\n",
            "epoch 8 batch 152 loss 0.04170197993516922\n",
            "epoch 8 batch 153 loss 0.04637669399380684\n",
            "epoch 8 batch 154 loss 0.047189608216285706\n",
            "epoch 8 batch 155 loss 0.046045735478401184\n",
            "epoch 8 batch 156 loss 0.04967007040977478\n",
            "epoch 8 batch 157 loss 0.04500441625714302\n",
            "epoch 8 batch 158 loss 0.04393865540623665\n",
            "epoch 8 batch 159 loss 0.03953683003783226\n",
            "epoch 8 batch 160 loss 0.047520965337753296\n",
            "epoch 8 batch 161 loss 0.04488297551870346\n",
            "epoch 8 batch 162 loss 0.05074779689311981\n",
            "epoch 8 batch 163 loss 0.04822744056582451\n",
            "epoch 8 batch 164 loss 0.04565931484103203\n",
            "epoch 8 batch 165 loss 0.044484369456768036\n",
            "epoch 8 batch 166 loss 0.05072852224111557\n",
            "epoch 8 batch 167 loss 0.04444466903805733\n",
            "epoch 8 batch 168 loss 0.05197979882359505\n",
            "epoch 8 batch 169 loss 0.04617932438850403\n",
            "epoch 8 batch 170 loss 0.04412420094013214\n",
            "epoch 8 batch 171 loss 0.04671550169587135\n",
            "epoch 8 batch 172 loss 0.04818518087267876\n",
            "epoch 8 batch 173 loss 0.04325658455491066\n",
            "epoch 8 batch 174 loss 0.04626823961734772\n",
            "epoch 8 batch 175 loss 0.04916324093937874\n",
            "epoch 8 batch 176 loss 0.047211404889822006\n",
            "epoch 8 batch 177 loss 0.04345066845417023\n",
            "epoch 8 batch 178 loss 0.04363735765218735\n",
            "epoch 8 batch 179 loss 0.04223514720797539\n",
            "epoch 8 batch 180 loss 0.046189866960048676\n",
            "epoch 8 batch 181 loss 0.04821399226784706\n",
            "epoch 8 batch 182 loss 0.04632583260536194\n",
            "epoch 8 batch 183 loss 0.047177016735076904\n",
            "epoch 8 batch 184 loss 0.055479105561971664\n",
            "epoch 8 batch 185 loss 0.04748240113258362\n",
            "epoch 8 batch 186 loss 0.05405985191464424\n",
            "epoch 8 batch 187 loss 0.04107231646776199\n",
            "epoch 8 batch 188 loss 0.04206777364015579\n",
            "epoch 8 batch 189 loss 0.05253613740205765\n",
            "epoch 8 batch 190 loss 0.04226059466600418\n",
            "epoch 8 batch 191 loss 0.04884585365653038\n",
            "epoch 8 batch 192 loss 0.0407768189907074\n",
            "epoch 8 batch 193 loss 0.042811162769794464\n",
            "epoch 8 batch 194 loss 0.043894212692976\n",
            "epoch 8 batch 195 loss 0.04521773010492325\n",
            "epoch 8 batch 196 loss 0.04659406840801239\n",
            "epoch 8 batch 197 loss 0.04345916956663132\n",
            "epoch 8 batch 198 loss 0.04223520681262016\n",
            "epoch 8 batch 199 loss 0.052735596895217896\n",
            "epoch 8 batch 200 loss 0.04861736297607422\n",
            "epoch 8 batch 201 loss 0.04786091670393944\n",
            "epoch 8 batch 202 loss 0.04394228756427765\n",
            "epoch 8 batch 203 loss 0.042898498475551605\n",
            "epoch 8 batch 204 loss 0.03881761059165001\n",
            "epoch 8 batch 205 loss 0.0494389683008194\n",
            "epoch 8 batch 206 loss 0.03742239996790886\n",
            "epoch 8 batch 207 loss 0.0427863672375679\n",
            "epoch 8 batch 208 loss 0.05223313719034195\n",
            "epoch 8 batch 209 loss 0.045735664665699005\n",
            "epoch 8 batch 210 loss 0.05272567644715309\n",
            "epoch 8 batch 211 loss 0.04770864546298981\n",
            "epoch 8 batch 212 loss 0.04447251185774803\n",
            "epoch 8 batch 213 loss 0.04390574246644974\n",
            "epoch 8 batch 214 loss 0.045721013098955154\n",
            "epoch 8 batch 215 loss 0.04232865944504738\n",
            "epoch 8 batch 216 loss 0.04268383979797363\n",
            "epoch 8 batch 217 loss 0.038054365664720535\n",
            "epoch 8 batch 218 loss 0.04450172185897827\n",
            "epoch 8 batch 219 loss 0.046345993876457214\n",
            "epoch 8 batch 220 loss 0.04509798064827919\n",
            "epoch 8 batch 221 loss 0.044090259820222855\n",
            "epoch 8 batch 222 loss 0.04730859398841858\n",
            "epoch 8 batch 223 loss 0.04666917771100998\n",
            "epoch 8 batch 224 loss 0.04796313866972923\n",
            "epoch 8 batch 225 loss 0.04573049023747444\n",
            "epoch 8 batch 226 loss 0.044849324971437454\n",
            "epoch 8 batch 227 loss 0.04470779746770859\n",
            "epoch 8 batch 228 loss 0.04355305805802345\n",
            "epoch 8 batch 229 loss 0.0439738966524601\n",
            "epoch 8 batch 230 loss 0.032422520220279694\n",
            "epoch 9 batch 0 loss 0.04768557846546173\n",
            "epoch 9 batch 1 loss 0.04563869163393974\n",
            "epoch 9 batch 2 loss 0.052215490490198135\n",
            "epoch 9 batch 3 loss 0.048932623118162155\n",
            "epoch 9 batch 4 loss 0.046133968979120255\n",
            "epoch 9 batch 5 loss 0.046161964535713196\n",
            "epoch 9 batch 6 loss 0.045663028955459595\n",
            "epoch 9 batch 7 loss 0.05448700860142708\n",
            "epoch 9 batch 8 loss 0.0466720312833786\n",
            "epoch 9 batch 9 loss 0.04521021246910095\n",
            "epoch 9 batch 10 loss 0.053630948066711426\n",
            "epoch 9 batch 11 loss 0.042600348591804504\n",
            "epoch 9 batch 12 loss 0.0433514341711998\n",
            "epoch 9 batch 13 loss 0.05109855905175209\n",
            "epoch 9 batch 14 loss 0.04159654676914215\n",
            "epoch 9 batch 15 loss 0.04535200074315071\n",
            "epoch 9 batch 16 loss 0.04278653487563133\n",
            "epoch 9 batch 17 loss 0.04498468339443207\n",
            "epoch 9 batch 18 loss 0.04521995782852173\n",
            "epoch 9 batch 19 loss 0.0454840213060379\n",
            "epoch 9 batch 20 loss 0.04811477288603783\n",
            "epoch 9 batch 21 loss 0.04500236362218857\n",
            "epoch 9 batch 22 loss 0.040231168270111084\n",
            "epoch 9 batch 23 loss 0.04765854775905609\n",
            "epoch 9 batch 24 loss 0.0455702543258667\n",
            "epoch 9 batch 25 loss 0.05499109625816345\n",
            "epoch 9 batch 26 loss 0.045384809374809265\n",
            "epoch 9 batch 27 loss 0.05324761942028999\n",
            "epoch 9 batch 28 loss 0.04025421664118767\n",
            "epoch 9 batch 29 loss 0.04597330465912819\n",
            "epoch 9 batch 30 loss 0.04744911566376686\n",
            "epoch 9 batch 31 loss 0.04458441585302353\n",
            "epoch 9 batch 32 loss 0.04077206552028656\n",
            "epoch 9 batch 33 loss 0.045031219720840454\n",
            "epoch 9 batch 34 loss 0.040951475501060486\n",
            "epoch 9 batch 35 loss 0.04309273883700371\n",
            "epoch 9 batch 36 loss 0.05473864823579788\n",
            "epoch 9 batch 37 loss 0.04845923185348511\n",
            "epoch 9 batch 38 loss 0.04453056678175926\n",
            "epoch 9 batch 39 loss 0.04389391839504242\n",
            "epoch 9 batch 40 loss 0.04596352577209473\n",
            "epoch 9 batch 41 loss 0.044307347387075424\n",
            "epoch 9 batch 42 loss 0.04464447870850563\n",
            "epoch 9 batch 43 loss 0.047878608107566833\n",
            "epoch 9 batch 44 loss 0.04939412325620651\n",
            "epoch 9 batch 45 loss 0.04289669543504715\n",
            "epoch 9 batch 46 loss 0.044217679649591446\n",
            "epoch 9 batch 47 loss 0.04812296852469444\n",
            "epoch 9 batch 48 loss 0.040578678250312805\n",
            "epoch 9 batch 49 loss 0.044322703033685684\n",
            "epoch 9 batch 50 loss 0.04693913832306862\n",
            "epoch 9 batch 51 loss 0.04685071110725403\n",
            "epoch 9 batch 52 loss 0.04537292569875717\n",
            "epoch 9 batch 53 loss 0.040054891258478165\n",
            "epoch 9 batch 54 loss 0.04566201567649841\n",
            "epoch 9 batch 55 loss 0.04412641376256943\n",
            "epoch 9 batch 56 loss 0.04179180786013603\n",
            "epoch 9 batch 57 loss 0.049998726695775986\n",
            "epoch 9 batch 58 loss 0.04696178808808327\n",
            "epoch 9 batch 59 loss 0.04337853565812111\n",
            "epoch 9 batch 60 loss 0.04514145106077194\n",
            "epoch 9 batch 61 loss 0.045413218438625336\n",
            "epoch 9 batch 62 loss 0.041846390813589096\n",
            "epoch 9 batch 63 loss 0.0454586036503315\n",
            "epoch 9 batch 64 loss 0.04968882352113724\n",
            "epoch 9 batch 65 loss 0.041474584490060806\n",
            "epoch 9 batch 66 loss 0.04669896885752678\n",
            "epoch 9 batch 67 loss 0.05096353963017464\n",
            "epoch 9 batch 68 loss 0.04530104622244835\n",
            "epoch 9 batch 69 loss 0.04587244987487793\n",
            "epoch 9 batch 70 loss 0.047279246151447296\n",
            "epoch 9 batch 71 loss 0.041438907384872437\n",
            "epoch 9 batch 72 loss 0.04276788607239723\n",
            "epoch 9 batch 73 loss 0.04820742458105087\n",
            "epoch 9 batch 74 loss 0.04343941807746887\n",
            "epoch 9 batch 75 loss 0.04775480926036835\n",
            "epoch 9 batch 76 loss 0.03783520683646202\n",
            "epoch 9 batch 77 loss 0.04559218883514404\n",
            "epoch 9 batch 78 loss 0.045887384563684464\n",
            "epoch 9 batch 79 loss 0.04268399626016617\n",
            "epoch 9 batch 80 loss 0.04601661115884781\n",
            "epoch 9 batch 81 loss 0.043615054339170456\n",
            "epoch 9 batch 82 loss 0.047609418630599976\n",
            "epoch 9 batch 83 loss 0.040720198303461075\n",
            "epoch 9 batch 84 loss 0.04291315749287605\n",
            "epoch 9 batch 85 loss 0.04576214402914047\n",
            "epoch 9 batch 86 loss 0.037567488849163055\n",
            "epoch 9 batch 87 loss 0.04326983541250229\n",
            "epoch 9 batch 88 loss 0.041905783116817474\n",
            "epoch 9 batch 89 loss 0.04835398122668266\n",
            "epoch 9 batch 90 loss 0.04382757097482681\n",
            "epoch 9 batch 91 loss 0.04591262713074684\n",
            "epoch 9 batch 92 loss 0.04548955708742142\n",
            "epoch 9 batch 93 loss 0.04575403779745102\n",
            "epoch 9 batch 94 loss 0.044016074389219284\n",
            "epoch 9 batch 95 loss 0.04779631271958351\n",
            "epoch 9 batch 96 loss 0.04343284294009209\n",
            "epoch 9 batch 97 loss 0.04443928971886635\n",
            "epoch 9 batch 98 loss 0.0427500456571579\n",
            "epoch 9 batch 99 loss 0.04668857902288437\n",
            "epoch 9 batch 100 loss 0.05112162232398987\n",
            "epoch 9 batch 101 loss 0.040825098752975464\n",
            "epoch 9 batch 102 loss 0.04290589690208435\n",
            "epoch 9 batch 103 loss 0.04719560220837593\n",
            "epoch 9 batch 104 loss 0.04149399697780609\n",
            "epoch 9 batch 105 loss 0.038639068603515625\n",
            "epoch 9 batch 106 loss 0.04001555219292641\n",
            "epoch 9 batch 107 loss 0.042791493237018585\n",
            "epoch 9 batch 108 loss 0.043842583894729614\n",
            "epoch 9 batch 109 loss 0.04510621353983879\n",
            "epoch 9 batch 110 loss 0.03947862982749939\n",
            "epoch 9 batch 111 loss 0.04372129589319229\n",
            "epoch 9 batch 112 loss 0.04994267597794533\n",
            "epoch 9 batch 113 loss 0.043824452906847\n",
            "epoch 9 batch 114 loss 0.04243621975183487\n",
            "epoch 9 batch 115 loss 0.044599633663892746\n",
            "epoch 9 batch 116 loss 0.044617798179388046\n",
            "epoch 9 batch 117 loss 0.04647558555006981\n",
            "epoch 9 batch 118 loss 0.04379904642701149\n",
            "epoch 9 batch 119 loss 0.045967575162649155\n",
            "epoch 9 batch 120 loss 0.042510099709033966\n",
            "epoch 9 batch 121 loss 0.04381555691361427\n",
            "epoch 9 batch 122 loss 0.04483499005436897\n",
            "epoch 9 batch 123 loss 0.0426890067756176\n",
            "epoch 9 batch 124 loss 0.04705308750271797\n",
            "epoch 9 batch 125 loss 0.04230298474431038\n",
            "epoch 9 batch 126 loss 0.03743455559015274\n",
            "epoch 9 batch 127 loss 0.04931812360882759\n",
            "epoch 9 batch 128 loss 0.04333280771970749\n",
            "epoch 9 batch 129 loss 0.04039822891354561\n",
            "epoch 9 batch 130 loss 0.052509721368551254\n",
            "epoch 9 batch 131 loss 0.04624256119132042\n",
            "epoch 9 batch 132 loss 0.04616691544651985\n",
            "epoch 9 batch 133 loss 0.041298653930425644\n",
            "epoch 9 batch 134 loss 0.047003719955682755\n",
            "epoch 9 batch 135 loss 0.04957959055900574\n",
            "epoch 9 batch 136 loss 0.043357010930776596\n",
            "epoch 9 batch 137 loss 0.04314999282360077\n",
            "epoch 9 batch 138 loss 0.04357987642288208\n",
            "epoch 9 batch 139 loss 0.043687500059604645\n",
            "epoch 9 batch 140 loss 0.045834023505449295\n",
            "epoch 9 batch 141 loss 0.04207892343401909\n",
            "epoch 9 batch 142 loss 0.038988858461380005\n",
            "epoch 9 batch 143 loss 0.035987723618745804\n",
            "epoch 9 batch 144 loss 0.04137655720114708\n",
            "epoch 9 batch 145 loss 0.04520212486386299\n",
            "epoch 9 batch 146 loss 0.049588676542043686\n",
            "epoch 9 batch 147 loss 0.046786483377218246\n",
            "epoch 9 batch 148 loss 0.042923975735902786\n",
            "epoch 9 batch 149 loss 0.04478447511792183\n",
            "epoch 9 batch 150 loss 0.04570717364549637\n",
            "epoch 9 batch 151 loss 0.04303828254342079\n",
            "epoch 9 batch 152 loss 0.039103247225284576\n",
            "epoch 9 batch 153 loss 0.04414254054427147\n",
            "epoch 9 batch 154 loss 0.04507055506110191\n",
            "epoch 9 batch 155 loss 0.044202446937561035\n",
            "epoch 9 batch 156 loss 0.04774465784430504\n",
            "epoch 9 batch 157 loss 0.04293792322278023\n",
            "epoch 9 batch 158 loss 0.0420750230550766\n",
            "epoch 9 batch 159 loss 0.03727151080965996\n",
            "epoch 9 batch 160 loss 0.04512505978345871\n",
            "epoch 9 batch 161 loss 0.04297461360692978\n",
            "epoch 9 batch 162 loss 0.04887450486421585\n",
            "epoch 9 batch 163 loss 0.04521256685256958\n",
            "epoch 9 batch 164 loss 0.04339689388871193\n",
            "epoch 9 batch 165 loss 0.042434390634298325\n",
            "epoch 9 batch 166 loss 0.048061639070510864\n",
            "epoch 9 batch 167 loss 0.043014928698539734\n",
            "epoch 9 batch 168 loss 0.04882395640015602\n",
            "epoch 9 batch 169 loss 0.044221799820661545\n",
            "epoch 9 batch 170 loss 0.043190862983465195\n",
            "epoch 9 batch 171 loss 0.045126546174287796\n",
            "epoch 9 batch 172 loss 0.04599882289767265\n",
            "epoch 9 batch 173 loss 0.042195405811071396\n",
            "epoch 9 batch 174 loss 0.04437895491719246\n",
            "epoch 9 batch 175 loss 0.04693200811743736\n",
            "epoch 9 batch 176 loss 0.0452800989151001\n",
            "epoch 9 batch 177 loss 0.0416523702442646\n",
            "epoch 9 batch 178 loss 0.04141944646835327\n",
            "epoch 9 batch 179 loss 0.040597833693027496\n",
            "epoch 9 batch 180 loss 0.044567372649908066\n",
            "epoch 9 batch 181 loss 0.04612964391708374\n",
            "epoch 9 batch 182 loss 0.044310443103313446\n",
            "epoch 9 batch 183 loss 0.04527668654918671\n",
            "epoch 9 batch 184 loss 0.05398626625537872\n",
            "epoch 9 batch 185 loss 0.04558812454342842\n",
            "epoch 9 batch 186 loss 0.05160418897867203\n",
            "epoch 9 batch 187 loss 0.03943125158548355\n",
            "epoch 9 batch 188 loss 0.03978845104575157\n",
            "epoch 9 batch 189 loss 0.050015226006507874\n",
            "epoch 9 batch 190 loss 0.040858566761016846\n",
            "epoch 9 batch 191 loss 0.04714181646704674\n",
            "epoch 9 batch 192 loss 0.03787560015916824\n",
            "epoch 9 batch 193 loss 0.039889007806777954\n",
            "epoch 9 batch 194 loss 0.04229985177516937\n",
            "epoch 9 batch 195 loss 0.043733902275562286\n",
            "epoch 9 batch 196 loss 0.0442921482026577\n",
            "epoch 9 batch 197 loss 0.04097330942749977\n",
            "epoch 9 batch 198 loss 0.040212664753198624\n",
            "epoch 9 batch 199 loss 0.05025311931967735\n",
            "epoch 9 batch 200 loss 0.04614195227622986\n",
            "epoch 9 batch 201 loss 0.045317377895116806\n",
            "epoch 9 batch 202 loss 0.04186980053782463\n",
            "epoch 9 batch 203 loss 0.04184867814183235\n",
            "epoch 9 batch 204 loss 0.036990776658058167\n",
            "epoch 9 batch 205 loss 0.04727838560938835\n",
            "epoch 9 batch 206 loss 0.03589092567563057\n",
            "epoch 9 batch 207 loss 0.039905522018671036\n",
            "epoch 9 batch 208 loss 0.04901475831866264\n",
            "epoch 9 batch 209 loss 0.04302963241934776\n",
            "epoch 9 batch 210 loss 0.05042177438735962\n",
            "epoch 9 batch 211 loss 0.045902594923973083\n",
            "epoch 9 batch 212 loss 0.04272904247045517\n",
            "epoch 9 batch 213 loss 0.04218427464365959\n",
            "epoch 9 batch 214 loss 0.0438944436609745\n",
            "epoch 9 batch 215 loss 0.03999968618154526\n",
            "epoch 9 batch 216 loss 0.042312391102313995\n",
            "epoch 9 batch 217 loss 0.035948652774095535\n",
            "epoch 9 batch 218 loss 0.042522236704826355\n",
            "epoch 9 batch 219 loss 0.044993575662374496\n",
            "epoch 9 batch 220 loss 0.04240357503294945\n",
            "epoch 9 batch 221 loss 0.04204511642456055\n",
            "epoch 9 batch 222 loss 0.04445820674300194\n",
            "epoch 9 batch 223 loss 0.04480120912194252\n",
            "epoch 9 batch 224 loss 0.0456884503364563\n",
            "epoch 9 batch 225 loss 0.044971536844968796\n",
            "epoch 9 batch 226 loss 0.04337739571928978\n",
            "epoch 9 batch 227 loss 0.04255470260977745\n",
            "epoch 9 batch 228 loss 0.042675599455833435\n",
            "epoch 9 batch 229 loss 0.0425347238779068\n",
            "epoch 9 batch 230 loss 0.03086184523999691\n",
            "epoch 10 batch 0 loss 0.04704087972640991\n",
            "epoch 10 batch 1 loss 0.04400491341948509\n",
            "epoch 10 batch 2 loss 0.049419473856687546\n",
            "epoch 10 batch 3 loss 0.04779166728258133\n",
            "epoch 10 batch 4 loss 0.04441151022911072\n",
            "epoch 10 batch 5 loss 0.04377216845750809\n",
            "epoch 10 batch 6 loss 0.04474285989999771\n",
            "epoch 10 batch 7 loss 0.052509162575006485\n",
            "epoch 10 batch 8 loss 0.04438464716076851\n",
            "epoch 10 batch 9 loss 0.043799109756946564\n",
            "epoch 10 batch 10 loss 0.05233694240450859\n",
            "epoch 10 batch 11 loss 0.041039131581783295\n",
            "epoch 10 batch 12 loss 0.04150319844484329\n",
            "epoch 10 batch 13 loss 0.048762865364551544\n",
            "epoch 10 batch 14 loss 0.04111475870013237\n",
            "epoch 10 batch 15 loss 0.044356539845466614\n",
            "epoch 10 batch 16 loss 0.04094862565398216\n",
            "epoch 10 batch 17 loss 0.0427609458565712\n",
            "epoch 10 batch 18 loss 0.04261489585042\n",
            "epoch 10 batch 19 loss 0.04384535178542137\n",
            "epoch 10 batch 20 loss 0.04546836018562317\n",
            "epoch 10 batch 21 loss 0.044088274240493774\n",
            "epoch 10 batch 22 loss 0.03857218846678734\n",
            "epoch 10 batch 23 loss 0.045406442135572433\n",
            "epoch 10 batch 24 loss 0.044250428676605225\n",
            "epoch 10 batch 25 loss 0.05310504883527756\n",
            "epoch 10 batch 26 loss 0.04471047222614288\n",
            "epoch 10 batch 27 loss 0.05004299059510231\n",
            "epoch 10 batch 28 loss 0.03896871954202652\n",
            "epoch 10 batch 29 loss 0.044849567115306854\n",
            "epoch 10 batch 30 loss 0.04531044885516167\n",
            "epoch 10 batch 31 loss 0.04250606149435043\n",
            "epoch 10 batch 32 loss 0.03983917832374573\n",
            "epoch 10 batch 33 loss 0.04296737536787987\n",
            "epoch 10 batch 34 loss 0.03942655399441719\n",
            "epoch 10 batch 35 loss 0.04129626229405403\n",
            "epoch 10 batch 36 loss 0.05138607695698738\n",
            "epoch 10 batch 37 loss 0.04663032293319702\n",
            "epoch 10 batch 38 loss 0.04314115270972252\n",
            "epoch 10 batch 39 loss 0.041599735617637634\n",
            "epoch 10 batch 40 loss 0.04465107247233391\n",
            "epoch 10 batch 41 loss 0.04307183623313904\n",
            "epoch 10 batch 42 loss 0.042229846119880676\n",
            "epoch 10 batch 43 loss 0.04633479192852974\n",
            "epoch 10 batch 44 loss 0.047814734280109406\n",
            "epoch 10 batch 45 loss 0.04170972481369972\n",
            "epoch 10 batch 46 loss 0.0424770824611187\n",
            "epoch 10 batch 47 loss 0.04726968705654144\n",
            "epoch 10 batch 48 loss 0.0388169102370739\n",
            "epoch 10 batch 49 loss 0.04250343143939972\n",
            "epoch 10 batch 50 loss 0.045580051839351654\n",
            "epoch 10 batch 51 loss 0.045645833015441895\n",
            "epoch 10 batch 52 loss 0.04386668652296066\n",
            "epoch 10 batch 53 loss 0.03796743229031563\n",
            "epoch 10 batch 54 loss 0.0441739559173584\n",
            "epoch 10 batch 55 loss 0.04192952811717987\n",
            "epoch 10 batch 56 loss 0.04001079127192497\n",
            "epoch 10 batch 57 loss 0.04838034138083458\n",
            "epoch 10 batch 58 loss 0.04522896558046341\n",
            "epoch 10 batch 59 loss 0.041512541472911835\n",
            "epoch 10 batch 60 loss 0.04308095574378967\n",
            "epoch 10 batch 61 loss 0.04401422664523125\n",
            "epoch 10 batch 62 loss 0.040499962866306305\n",
            "epoch 10 batch 63 loss 0.04358428344130516\n",
            "epoch 10 batch 64 loss 0.048277005553245544\n",
            "epoch 10 batch 65 loss 0.040105272084474564\n",
            "epoch 10 batch 66 loss 0.044942453503608704\n",
            "epoch 10 batch 67 loss 0.04930414631962776\n",
            "epoch 10 batch 68 loss 0.044442858546972275\n",
            "epoch 10 batch 69 loss 0.044082947075366974\n",
            "epoch 10 batch 70 loss 0.045195404440164566\n",
            "epoch 10 batch 71 loss 0.03954104334115982\n",
            "epoch 10 batch 72 loss 0.04156908392906189\n",
            "epoch 10 batch 73 loss 0.046185050159692764\n",
            "epoch 10 batch 74 loss 0.042672980576753616\n",
            "epoch 10 batch 75 loss 0.04641120508313179\n",
            "epoch 10 batch 76 loss 0.03741687163710594\n",
            "epoch 10 batch 77 loss 0.04400509595870972\n",
            "epoch 10 batch 78 loss 0.04307857155799866\n",
            "epoch 10 batch 79 loss 0.04083269461989403\n",
            "epoch 10 batch 80 loss 0.04509506747126579\n",
            "epoch 10 batch 81 loss 0.04194153472781181\n",
            "epoch 10 batch 82 loss 0.045051004737615585\n",
            "epoch 10 batch 83 loss 0.04023102670907974\n",
            "epoch 10 batch 84 loss 0.04149404168128967\n",
            "epoch 10 batch 85 loss 0.04417366161942482\n",
            "epoch 10 batch 86 loss 0.036796677857637405\n",
            "epoch 10 batch 87 loss 0.04208054021000862\n",
            "epoch 10 batch 88 loss 0.04179973900318146\n",
            "epoch 10 batch 89 loss 0.04699844494462013\n",
            "epoch 10 batch 90 loss 0.041987378150224686\n",
            "epoch 10 batch 91 loss 0.04512256756424904\n",
            "epoch 10 batch 92 loss 0.044431135058403015\n",
            "epoch 10 batch 93 loss 0.043687060475349426\n",
            "epoch 10 batch 94 loss 0.0423407256603241\n",
            "epoch 10 batch 95 loss 0.04503493756055832\n",
            "epoch 10 batch 96 loss 0.04286225885152817\n",
            "epoch 10 batch 97 loss 0.042856764048337936\n",
            "epoch 10 batch 98 loss 0.041043128818273544\n",
            "epoch 10 batch 99 loss 0.0465596504509449\n",
            "epoch 10 batch 100 loss 0.04886581376194954\n",
            "epoch 10 batch 101 loss 0.04066207632422447\n",
            "epoch 10 batch 102 loss 0.04273849353194237\n",
            "epoch 10 batch 103 loss 0.04521803557872772\n",
            "epoch 10 batch 104 loss 0.04019278287887573\n",
            "epoch 10 batch 105 loss 0.038555704057216644\n",
            "epoch 10 batch 106 loss 0.038536544889211655\n",
            "epoch 10 batch 107 loss 0.04208266735076904\n",
            "epoch 10 batch 108 loss 0.04308946430683136\n",
            "epoch 10 batch 109 loss 0.04299915209412575\n",
            "epoch 10 batch 110 loss 0.0394207127392292\n",
            "epoch 10 batch 111 loss 0.04320502653717995\n",
            "epoch 10 batch 112 loss 0.0486748106777668\n",
            "epoch 10 batch 113 loss 0.04238652437925339\n",
            "epoch 10 batch 114 loss 0.04128977656364441\n",
            "epoch 10 batch 115 loss 0.04352265223860741\n",
            "epoch 10 batch 116 loss 0.04488605633378029\n",
            "epoch 10 batch 117 loss 0.04618522897362709\n",
            "epoch 10 batch 118 loss 0.04203600063920021\n",
            "epoch 10 batch 119 loss 0.045211151242256165\n",
            "epoch 10 batch 120 loss 0.042287372052669525\n",
            "epoch 10 batch 121 loss 0.04170531406998634\n",
            "epoch 10 batch 122 loss 0.043025631457567215\n",
            "epoch 10 batch 123 loss 0.041869062930345535\n",
            "epoch 10 batch 124 loss 0.046244408935308456\n",
            "epoch 10 batch 125 loss 0.041235148906707764\n",
            "epoch 10 batch 126 loss 0.035890255123376846\n",
            "epoch 10 batch 127 loss 0.04679855331778526\n",
            "epoch 10 batch 128 loss 0.04128033667802811\n",
            "epoch 10 batch 129 loss 0.03937241807579994\n",
            "epoch 10 batch 130 loss 0.05098506808280945\n",
            "epoch 10 batch 131 loss 0.044363684952259064\n",
            "epoch 10 batch 132 loss 0.044955961406230927\n",
            "epoch 10 batch 133 loss 0.040137648582458496\n",
            "epoch 10 batch 134 loss 0.045143578201532364\n",
            "epoch 10 batch 135 loss 0.047332070767879486\n",
            "epoch 10 batch 136 loss 0.042246460914611816\n",
            "epoch 10 batch 137 loss 0.04206153005361557\n",
            "epoch 10 batch 138 loss 0.04254855960607529\n",
            "epoch 10 batch 139 loss 0.04183697700500488\n",
            "epoch 10 batch 140 loss 0.0446317158639431\n",
            "epoch 10 batch 141 loss 0.039727162569761276\n",
            "epoch 10 batch 142 loss 0.037941090762615204\n",
            "epoch 10 batch 143 loss 0.03512183576822281\n",
            "epoch 10 batch 144 loss 0.03963542357087135\n",
            "epoch 10 batch 145 loss 0.04351339116692543\n",
            "epoch 10 batch 146 loss 0.047977056354284286\n",
            "epoch 10 batch 147 loss 0.045233096927404404\n",
            "epoch 10 batch 148 loss 0.04151351377367973\n",
            "epoch 10 batch 149 loss 0.04335988685488701\n",
            "epoch 10 batch 150 loss 0.04373827204108238\n",
            "epoch 10 batch 151 loss 0.041874781250953674\n",
            "epoch 10 batch 152 loss 0.03860846534371376\n",
            "epoch 10 batch 153 loss 0.041756246238946915\n",
            "epoch 10 batch 154 loss 0.0433969646692276\n",
            "epoch 10 batch 155 loss 0.0433870330452919\n",
            "epoch 10 batch 156 loss 0.045194290578365326\n",
            "epoch 10 batch 157 loss 0.041284408420324326\n",
            "epoch 10 batch 158 loss 0.04014755040407181\n",
            "epoch 10 batch 159 loss 0.03607136756181717\n",
            "epoch 10 batch 160 loss 0.04371169954538345\n",
            "epoch 10 batch 161 loss 0.04193473607301712\n",
            "epoch 10 batch 162 loss 0.04676063358783722\n",
            "epoch 10 batch 163 loss 0.04390827566385269\n",
            "epoch 10 batch 164 loss 0.04207277297973633\n",
            "epoch 10 batch 165 loss 0.04175763204693794\n",
            "epoch 10 batch 166 loss 0.04568373039364815\n",
            "epoch 10 batch 167 loss 0.04119674116373062\n",
            "epoch 10 batch 168 loss 0.048847004771232605\n",
            "epoch 10 batch 169 loss 0.04329806566238403\n",
            "epoch 10 batch 170 loss 0.04110366106033325\n",
            "epoch 10 batch 171 loss 0.044352054595947266\n",
            "epoch 10 batch 172 loss 0.04495494067668915\n",
            "epoch 10 batch 173 loss 0.04149201139807701\n",
            "epoch 10 batch 174 loss 0.042533066123723984\n",
            "epoch 10 batch 175 loss 0.04540124163031578\n",
            "epoch 10 batch 176 loss 0.043043963611125946\n",
            "epoch 10 batch 177 loss 0.03987779840826988\n",
            "epoch 10 batch 178 loss 0.04168378561735153\n",
            "epoch 10 batch 179 loss 0.04039367660880089\n",
            "epoch 10 batch 180 loss 0.04328728839755058\n",
            "epoch 10 batch 181 loss 0.04485364258289337\n",
            "epoch 10 batch 182 loss 0.043537694960832596\n",
            "epoch 10 batch 183 loss 0.045291755348443985\n",
            "epoch 10 batch 184 loss 0.05201501399278641\n",
            "epoch 10 batch 185 loss 0.0440823920071125\n",
            "epoch 10 batch 186 loss 0.05039215832948685\n",
            "epoch 10 batch 187 loss 0.03863129764795303\n",
            "epoch 10 batch 188 loss 0.03947782516479492\n",
            "epoch 10 batch 189 loss 0.04758957773447037\n",
            "epoch 10 batch 190 loss 0.04042453318834305\n",
            "epoch 10 batch 191 loss 0.045665573328733444\n",
            "epoch 10 batch 192 loss 0.03730708360671997\n",
            "epoch 10 batch 193 loss 0.038786474615335464\n",
            "epoch 10 batch 194 loss 0.04099759832024574\n",
            "epoch 10 batch 195 loss 0.04265090823173523\n",
            "epoch 10 batch 196 loss 0.04411011561751366\n",
            "epoch 10 batch 197 loss 0.03957678750157356\n",
            "epoch 10 batch 198 loss 0.038419146090745926\n",
            "epoch 10 batch 199 loss 0.04887880012392998\n",
            "epoch 10 batch 200 loss 0.04511331394314766\n",
            "epoch 10 batch 201 loss 0.04371185973286629\n",
            "epoch 10 batch 202 loss 0.04107259586453438\n",
            "epoch 10 batch 203 loss 0.04061794653534889\n",
            "epoch 10 batch 204 loss 0.03579630330204964\n",
            "epoch 10 batch 205 loss 0.04516554996371269\n",
            "epoch 10 batch 206 loss 0.03500564396381378\n",
            "epoch 10 batch 207 loss 0.039742447435855865\n",
            "epoch 10 batch 208 loss 0.048296570777893066\n",
            "epoch 10 batch 209 loss 0.04172082990407944\n",
            "epoch 10 batch 210 loss 0.04932834580540657\n",
            "epoch 10 batch 211 loss 0.045259468257427216\n",
            "epoch 10 batch 212 loss 0.042240459471940994\n",
            "epoch 10 batch 213 loss 0.041242875158786774\n",
            "epoch 10 batch 214 loss 0.04356037825345993\n",
            "epoch 10 batch 215 loss 0.03869883343577385\n",
            "epoch 10 batch 216 loss 0.0413980707526207\n",
            "epoch 10 batch 217 loss 0.03599638119339943\n",
            "epoch 10 batch 218 loss 0.042552266269922256\n",
            "epoch 10 batch 219 loss 0.04390290379524231\n",
            "epoch 10 batch 220 loss 0.041498467326164246\n",
            "epoch 10 batch 221 loss 0.04101533070206642\n",
            "epoch 10 batch 222 loss 0.04317203536629677\n",
            "epoch 10 batch 223 loss 0.0435919351875782\n",
            "epoch 10 batch 224 loss 0.04347453638911247\n",
            "epoch 10 batch 225 loss 0.042989686131477356\n",
            "epoch 10 batch 226 loss 0.040889982134103775\n",
            "epoch 10 batch 227 loss 0.04105910286307335\n",
            "epoch 10 batch 228 loss 0.041621606796979904\n",
            "epoch 10 batch 229 loss 0.041072774678468704\n",
            "epoch 10 batch 230 loss 0.0292395930737257\n",
            "epoch 11 batch 0 loss 0.04425932466983795\n",
            "epoch 11 batch 1 loss 0.04184962064027786\n",
            "epoch 11 batch 2 loss 0.04666774719953537\n",
            "epoch 11 batch 3 loss 0.04470115900039673\n",
            "epoch 11 batch 4 loss 0.042281121015548706\n",
            "epoch 11 batch 5 loss 0.04330351576209068\n",
            "epoch 11 batch 6 loss 0.042728688567876816\n",
            "epoch 11 batch 7 loss 0.05031426623463631\n",
            "epoch 11 batch 8 loss 0.04229994863271713\n",
            "epoch 11 batch 9 loss 0.04239782691001892\n",
            "epoch 11 batch 10 loss 0.05078580603003502\n",
            "epoch 11 batch 11 loss 0.04015811160206795\n",
            "epoch 11 batch 12 loss 0.040771324187517166\n",
            "epoch 11 batch 13 loss 0.048182982951402664\n",
            "epoch 11 batch 14 loss 0.03986264765262604\n",
            "epoch 11 batch 15 loss 0.04502934217453003\n",
            "epoch 11 batch 16 loss 0.04063194617629051\n",
            "epoch 11 batch 17 loss 0.042046379297971725\n",
            "epoch 11 batch 18 loss 0.04074133187532425\n",
            "epoch 11 batch 19 loss 0.041923221200704575\n",
            "epoch 11 batch 20 loss 0.04346564784646034\n",
            "epoch 11 batch 21 loss 0.042316194623708725\n",
            "epoch 11 batch 22 loss 0.037196386605501175\n",
            "epoch 11 batch 23 loss 0.043308377265930176\n",
            "epoch 11 batch 24 loss 0.041705355048179626\n",
            "epoch 11 batch 25 loss 0.0523909330368042\n",
            "epoch 11 batch 26 loss 0.04379413649439812\n",
            "epoch 11 batch 27 loss 0.048733048141002655\n",
            "epoch 11 batch 28 loss 0.03739815577864647\n",
            "epoch 11 batch 29 loss 0.04364712908864021\n",
            "epoch 11 batch 30 loss 0.042900510132312775\n",
            "epoch 11 batch 31 loss 0.04139168933033943\n",
            "epoch 11 batch 32 loss 0.040009308606386185\n",
            "epoch 11 batch 33 loss 0.04161685332655907\n",
            "epoch 11 batch 34 loss 0.03827635198831558\n",
            "epoch 11 batch 35 loss 0.04078567773103714\n",
            "epoch 11 batch 36 loss 0.04913100227713585\n",
            "epoch 11 batch 37 loss 0.04492630809545517\n",
            "epoch 11 batch 38 loss 0.04200638830661774\n",
            "epoch 11 batch 39 loss 0.03939136490225792\n",
            "epoch 11 batch 40 loss 0.04290976747870445\n",
            "epoch 11 batch 41 loss 0.04141577333211899\n",
            "epoch 11 batch 42 loss 0.04060882702469826\n",
            "epoch 11 batch 43 loss 0.04494122788310051\n",
            "epoch 11 batch 44 loss 0.04606713354587555\n",
            "epoch 11 batch 45 loss 0.04029481112957001\n",
            "epoch 11 batch 46 loss 0.04277481511235237\n",
            "epoch 11 batch 47 loss 0.04513318091630936\n",
            "epoch 11 batch 48 loss 0.037590865045785904\n",
            "epoch 11 batch 49 loss 0.041945941746234894\n",
            "epoch 11 batch 50 loss 0.044484298676252365\n",
            "epoch 11 batch 51 loss 0.04355741664767265\n",
            "epoch 11 batch 52 loss 0.043160319328308105\n",
            "epoch 11 batch 53 loss 0.038004882633686066\n",
            "epoch 11 batch 54 loss 0.04231557250022888\n",
            "epoch 11 batch 55 loss 0.0407574363052845\n",
            "epoch 11 batch 56 loss 0.03943386673927307\n",
            "epoch 11 batch 57 loss 0.04699452593922615\n",
            "epoch 11 batch 58 loss 0.044023510068655014\n",
            "epoch 11 batch 59 loss 0.04066520556807518\n",
            "epoch 11 batch 60 loss 0.04193475842475891\n",
            "epoch 11 batch 61 loss 0.042688317596912384\n",
            "epoch 11 batch 62 loss 0.04080013930797577\n",
            "epoch 11 batch 63 loss 0.04166387394070625\n",
            "epoch 11 batch 64 loss 0.048322759568691254\n",
            "epoch 11 batch 65 loss 0.03984324634075165\n",
            "epoch 11 batch 66 loss 0.04495272785425186\n",
            "epoch 11 batch 67 loss 0.04917945712804794\n",
            "epoch 11 batch 68 loss 0.043854519724845886\n",
            "epoch 11 batch 69 loss 0.04310114309191704\n",
            "epoch 11 batch 70 loss 0.04527683183550835\n",
            "epoch 11 batch 71 loss 0.0392029769718647\n",
            "epoch 11 batch 72 loss 0.041109710931777954\n",
            "epoch 11 batch 73 loss 0.04577934741973877\n",
            "epoch 11 batch 74 loss 0.04224414378404617\n",
            "epoch 11 batch 75 loss 0.046182453632354736\n",
            "epoch 11 batch 76 loss 0.036715611815452576\n",
            "epoch 11 batch 77 loss 0.043620333075523376\n",
            "epoch 11 batch 78 loss 0.04309777170419693\n",
            "epoch 11 batch 79 loss 0.040382154285907745\n",
            "epoch 11 batch 80 loss 0.04422891139984131\n",
            "epoch 11 batch 81 loss 0.04221390187740326\n",
            "epoch 11 batch 82 loss 0.04453407600522041\n",
            "epoch 11 batch 83 loss 0.03928021714091301\n",
            "epoch 11 batch 84 loss 0.041633203625679016\n",
            "epoch 11 batch 85 loss 0.04435853287577629\n",
            "epoch 11 batch 86 loss 0.036955270916223526\n",
            "epoch 11 batch 87 loss 0.04172534495592117\n",
            "epoch 11 batch 88 loss 0.04034357890486717\n",
            "epoch 11 batch 89 loss 0.04773956909775734\n",
            "epoch 11 batch 90 loss 0.04129356890916824\n",
            "epoch 11 batch 91 loss 0.04318763688206673\n",
            "epoch 11 batch 92 loss 0.04308825731277466\n",
            "epoch 11 batch 93 loss 0.0434260331094265\n",
            "epoch 11 batch 94 loss 0.041756466031074524\n",
            "epoch 11 batch 95 loss 0.04601673781871796\n",
            "epoch 11 batch 96 loss 0.04100731015205383\n",
            "epoch 11 batch 97 loss 0.04193825274705887\n",
            "epoch 11 batch 98 loss 0.04184269532561302\n",
            "epoch 11 batch 99 loss 0.044046346098184586\n",
            "epoch 11 batch 100 loss 0.04779579117894173\n",
            "epoch 11 batch 101 loss 0.03933084383606911\n",
            "epoch 11 batch 102 loss 0.04086144268512726\n",
            "epoch 11 batch 103 loss 0.04392325505614281\n",
            "epoch 11 batch 104 loss 0.038478851318359375\n",
            "epoch 11 batch 105 loss 0.03680754452943802\n",
            "epoch 11 batch 106 loss 0.03875095769762993\n",
            "epoch 11 batch 107 loss 0.0402185283601284\n",
            "epoch 11 batch 108 loss 0.04208855703473091\n",
            "epoch 11 batch 109 loss 0.043151214718818665\n",
            "epoch 11 batch 110 loss 0.037135422229766846\n",
            "epoch 11 batch 111 loss 0.04137428104877472\n",
            "epoch 11 batch 112 loss 0.04864863306283951\n",
            "epoch 11 batch 113 loss 0.042182497680187225\n",
            "epoch 11 batch 114 loss 0.03938787803053856\n",
            "epoch 11 batch 115 loss 0.0430290624499321\n",
            "epoch 11 batch 116 loss 0.04364355653524399\n",
            "epoch 11 batch 117 loss 0.04417186975479126\n",
            "epoch 11 batch 118 loss 0.04138054698705673\n",
            "epoch 11 batch 119 loss 0.04418846592307091\n",
            "epoch 11 batch 120 loss 0.04044453054666519\n",
            "epoch 11 batch 121 loss 0.042713459581136703\n",
            "epoch 11 batch 122 loss 0.04255444183945656\n",
            "epoch 11 batch 123 loss 0.039511788636446\n",
            "epoch 11 batch 124 loss 0.04640050604939461\n",
            "epoch 11 batch 125 loss 0.041440561413764954\n",
            "epoch 11 batch 126 loss 0.03544457629323006\n",
            "epoch 11 batch 127 loss 0.04627678170800209\n",
            "epoch 11 batch 128 loss 0.04018509387969971\n",
            "epoch 11 batch 129 loss 0.03865756839513779\n",
            "epoch 11 batch 130 loss 0.04986979812383652\n",
            "epoch 11 batch 131 loss 0.043847471475601196\n",
            "epoch 11 batch 132 loss 0.043138112872838974\n",
            "epoch 11 batch 133 loss 0.039331983774900436\n",
            "epoch 11 batch 134 loss 0.043423935770988464\n",
            "epoch 11 batch 135 loss 0.04642182216048241\n",
            "epoch 11 batch 136 loss 0.040105290710926056\n",
            "epoch 11 batch 137 loss 0.04082648456096649\n",
            "epoch 11 batch 138 loss 0.04113500565290451\n",
            "epoch 11 batch 139 loss 0.041576456278562546\n",
            "epoch 11 batch 140 loss 0.04377321898937225\n",
            "epoch 11 batch 141 loss 0.040091585367918015\n",
            "epoch 11 batch 142 loss 0.03772442415356636\n",
            "epoch 11 batch 143 loss 0.03360435739159584\n",
            "epoch 11 batch 144 loss 0.03993227332830429\n",
            "epoch 11 batch 145 loss 0.04258427396416664\n",
            "epoch 11 batch 146 loss 0.04654752090573311\n",
            "epoch 11 batch 147 loss 0.04412401467561722\n",
            "epoch 11 batch 148 loss 0.04156430810689926\n",
            "epoch 11 batch 149 loss 0.04245424270629883\n",
            "epoch 11 batch 150 loss 0.04192391037940979\n",
            "epoch 11 batch 151 loss 0.040902893990278244\n",
            "epoch 11 batch 152 loss 0.037520989775657654\n",
            "epoch 11 batch 153 loss 0.04102427884936333\n",
            "epoch 11 batch 154 loss 0.0429789163172245\n",
            "epoch 11 batch 155 loss 0.04283342510461807\n",
            "epoch 11 batch 156 loss 0.043351758271455765\n",
            "epoch 11 batch 157 loss 0.04044641554355621\n",
            "epoch 11 batch 158 loss 0.03914039209485054\n",
            "epoch 11 batch 159 loss 0.034695446491241455\n",
            "epoch 11 batch 160 loss 0.0419171079993248\n",
            "epoch 11 batch 161 loss 0.041140321642160416\n",
            "epoch 11 batch 162 loss 0.04602822661399841\n",
            "epoch 11 batch 163 loss 0.04285137727856636\n",
            "epoch 11 batch 164 loss 0.04036065936088562\n",
            "epoch 11 batch 165 loss 0.0396050363779068\n",
            "epoch 11 batch 166 loss 0.04420981928706169\n",
            "epoch 11 batch 167 loss 0.0395689457654953\n",
            "epoch 11 batch 168 loss 0.04650937020778656\n",
            "epoch 11 batch 169 loss 0.04203801602125168\n",
            "epoch 11 batch 170 loss 0.04057612270116806\n",
            "epoch 11 batch 171 loss 0.04226236417889595\n",
            "epoch 11 batch 172 loss 0.04349421709775925\n",
            "epoch 11 batch 173 loss 0.041260894387960434\n",
            "epoch 11 batch 174 loss 0.04111511632800102\n",
            "epoch 11 batch 175 loss 0.04391118884086609\n",
            "epoch 11 batch 176 loss 0.042561765760183334\n",
            "epoch 11 batch 177 loss 0.03800485283136368\n",
            "epoch 11 batch 178 loss 0.038221538066864014\n",
            "epoch 11 batch 179 loss 0.03852393105626106\n",
            "epoch 11 batch 180 loss 0.04241425171494484\n",
            "epoch 11 batch 181 loss 0.0434599407017231\n",
            "epoch 11 batch 182 loss 0.042612604796886444\n",
            "epoch 11 batch 183 loss 0.04371374472975731\n",
            "epoch 11 batch 184 loss 0.05093703791499138\n",
            "epoch 11 batch 185 loss 0.04290240630507469\n",
            "epoch 11 batch 186 loss 0.04856951907277107\n",
            "epoch 11 batch 187 loss 0.037539441138505936\n",
            "epoch 11 batch 188 loss 0.04034873843193054\n",
            "epoch 11 batch 189 loss 0.04551498591899872\n",
            "epoch 11 batch 190 loss 0.03782074525952339\n",
            "epoch 11 batch 191 loss 0.04399246349930763\n",
            "epoch 11 batch 192 loss 0.036850571632385254\n",
            "epoch 11 batch 193 loss 0.03767615556716919\n",
            "epoch 11 batch 194 loss 0.03894208371639252\n",
            "epoch 11 batch 195 loss 0.04176699370145798\n",
            "epoch 11 batch 196 loss 0.0431399904191494\n",
            "epoch 11 batch 197 loss 0.037599701434373856\n",
            "epoch 11 batch 198 loss 0.03758351877331734\n",
            "epoch 11 batch 199 loss 0.047793466597795486\n",
            "epoch 11 batch 200 loss 0.04388544335961342\n",
            "epoch 11 batch 201 loss 0.04159006476402283\n",
            "epoch 11 batch 202 loss 0.039824943989515305\n",
            "epoch 11 batch 203 loss 0.04017574340105057\n",
            "epoch 11 batch 204 loss 0.035496972501277924\n",
            "epoch 11 batch 205 loss 0.04407508671283722\n",
            "epoch 11 batch 206 loss 0.03375531733036041\n",
            "epoch 11 batch 207 loss 0.03860019892454147\n",
            "epoch 11 batch 208 loss 0.047480154782533646\n",
            "epoch 11 batch 209 loss 0.04096410796046257\n",
            "epoch 11 batch 210 loss 0.0473669208586216\n",
            "epoch 11 batch 211 loss 0.04389870911836624\n",
            "epoch 11 batch 212 loss 0.041148826479911804\n",
            "epoch 11 batch 213 loss 0.04064882919192314\n",
            "epoch 11 batch 214 loss 0.04181366786360741\n",
            "epoch 11 batch 215 loss 0.03747741878032684\n",
            "epoch 11 batch 216 loss 0.040066737681627274\n",
            "epoch 11 batch 217 loss 0.0355825312435627\n",
            "epoch 11 batch 218 loss 0.04233851656317711\n",
            "epoch 11 batch 219 loss 0.042720645666122437\n",
            "epoch 11 batch 220 loss 0.04233008995652199\n",
            "epoch 11 batch 221 loss 0.041441984474658966\n",
            "epoch 11 batch 222 loss 0.04239660128951073\n",
            "epoch 11 batch 223 loss 0.04275468364357948\n",
            "epoch 11 batch 224 loss 0.04206208139657974\n",
            "epoch 11 batch 225 loss 0.042500026524066925\n",
            "epoch 11 batch 226 loss 0.04112452268600464\n",
            "epoch 11 batch 227 loss 0.040272824466228485\n",
            "epoch 11 batch 228 loss 0.04050068557262421\n",
            "epoch 11 batch 229 loss 0.039826005697250366\n",
            "epoch 11 batch 230 loss 0.02763524278998375\n",
            "epoch 12 batch 0 loss 0.04389721155166626\n",
            "epoch 12 batch 1 loss 0.04209965839982033\n",
            "epoch 12 batch 2 loss 0.04638746380805969\n",
            "epoch 12 batch 3 loss 0.043934471905231476\n",
            "epoch 12 batch 4 loss 0.04221150279045105\n",
            "epoch 12 batch 5 loss 0.04092976078391075\n",
            "epoch 12 batch 6 loss 0.040340591222047806\n",
            "epoch 12 batch 7 loss 0.04836114123463631\n",
            "epoch 12 batch 8 loss 0.04193815588951111\n",
            "epoch 12 batch 9 loss 0.04219994693994522\n",
            "epoch 12 batch 10 loss 0.04882800206542015\n",
            "epoch 12 batch 11 loss 0.03859006613492966\n",
            "epoch 12 batch 12 loss 0.04049452766776085\n",
            "epoch 12 batch 13 loss 0.04853786155581474\n",
            "epoch 12 batch 14 loss 0.039467133581638336\n",
            "epoch 12 batch 15 loss 0.04428200051188469\n",
            "epoch 12 batch 16 loss 0.04086007922887802\n",
            "epoch 12 batch 17 loss 0.04186176136136055\n",
            "epoch 12 batch 18 loss 0.039774879813194275\n",
            "epoch 12 batch 19 loss 0.04112699627876282\n",
            "epoch 12 batch 20 loss 0.04323041811585426\n",
            "epoch 12 batch 21 loss 0.042117051780223846\n",
            "epoch 12 batch 22 loss 0.03698166459798813\n",
            "epoch 12 batch 23 loss 0.04151507467031479\n",
            "epoch 12 batch 24 loss 0.04084444418549538\n",
            "epoch 12 batch 25 loss 0.05182872340083122\n",
            "epoch 12 batch 26 loss 0.041914474219083786\n",
            "epoch 12 batch 27 loss 0.04718305170536041\n",
            "epoch 12 batch 28 loss 0.03679646924138069\n",
            "epoch 12 batch 29 loss 0.04326890781521797\n",
            "epoch 12 batch 30 loss 0.04224313050508499\n",
            "epoch 12 batch 31 loss 0.0411265529692173\n",
            "epoch 12 batch 32 loss 0.03792401775717735\n",
            "epoch 12 batch 33 loss 0.04178515449166298\n",
            "epoch 12 batch 34 loss 0.03843897208571434\n",
            "epoch 12 batch 35 loss 0.04016345739364624\n",
            "epoch 12 batch 36 loss 0.04924418032169342\n",
            "epoch 12 batch 37 loss 0.044454291462898254\n",
            "epoch 12 batch 38 loss 0.042095720767974854\n",
            "epoch 12 batch 39 loss 0.04092613607645035\n",
            "epoch 12 batch 40 loss 0.043191004544496536\n",
            "epoch 12 batch 41 loss 0.04029872268438339\n",
            "epoch 12 batch 42 loss 0.040480952709913254\n",
            "epoch 12 batch 43 loss 0.045870982110500336\n",
            "epoch 12 batch 44 loss 0.04514610767364502\n",
            "epoch 12 batch 45 loss 0.03855910524725914\n",
            "epoch 12 batch 46 loss 0.0417821928858757\n",
            "epoch 12 batch 47 loss 0.04486257955431938\n",
            "epoch 12 batch 48 loss 0.036450523883104324\n",
            "epoch 12 batch 49 loss 0.0411209873855114\n",
            "epoch 12 batch 50 loss 0.045253265649080276\n",
            "epoch 12 batch 51 loss 0.04260282590985298\n",
            "epoch 12 batch 52 loss 0.04297926649451256\n",
            "epoch 12 batch 53 loss 0.03834497928619385\n",
            "epoch 12 batch 54 loss 0.04169807583093643\n",
            "epoch 12 batch 55 loss 0.04052537679672241\n",
            "epoch 12 batch 56 loss 0.04062516614794731\n",
            "epoch 12 batch 57 loss 0.047123976051807404\n",
            "epoch 12 batch 58 loss 0.04334746673703194\n",
            "epoch 12 batch 59 loss 0.04124685749411583\n",
            "epoch 12 batch 60 loss 0.04242650046944618\n",
            "epoch 12 batch 61 loss 0.041434746235609055\n",
            "epoch 12 batch 62 loss 0.04075169935822487\n",
            "epoch 12 batch 63 loss 0.04087621346116066\n",
            "epoch 12 batch 64 loss 0.04583738371729851\n",
            "epoch 12 batch 65 loss 0.03995393589138985\n",
            "epoch 12 batch 66 loss 0.044998738914728165\n",
            "epoch 12 batch 67 loss 0.04642345383763313\n",
            "epoch 12 batch 68 loss 0.04402368143200874\n",
            "epoch 12 batch 69 loss 0.044200047850608826\n",
            "epoch 12 batch 70 loss 0.04321019724011421\n",
            "epoch 12 batch 71 loss 0.038091666996479034\n",
            "epoch 12 batch 72 loss 0.04278359189629555\n",
            "epoch 12 batch 73 loss 0.044515013694763184\n",
            "epoch 12 batch 74 loss 0.04160446673631668\n",
            "epoch 12 batch 75 loss 0.04571846127510071\n",
            "epoch 12 batch 76 loss 0.03676679730415344\n",
            "epoch 12 batch 77 loss 0.042349644005298615\n",
            "epoch 12 batch 78 loss 0.044087085872888565\n",
            "epoch 12 batch 79 loss 0.04171022027730942\n",
            "epoch 12 batch 80 loss 0.04342694953083992\n",
            "epoch 12 batch 81 loss 0.04193384200334549\n",
            "epoch 12 batch 82 loss 0.04579849913716316\n",
            "epoch 12 batch 83 loss 0.03889907896518707\n",
            "epoch 12 batch 84 loss 0.04168939217925072\n",
            "epoch 12 batch 85 loss 0.044873159378767014\n",
            "epoch 12 batch 86 loss 0.0362001471221447\n",
            "epoch 12 batch 87 loss 0.04309815540909767\n",
            "epoch 12 batch 88 loss 0.03859815374016762\n",
            "epoch 12 batch 89 loss 0.04673633724451065\n",
            "epoch 12 batch 90 loss 0.039817094802856445\n",
            "epoch 12 batch 91 loss 0.04251861944794655\n",
            "epoch 12 batch 92 loss 0.04224557802081108\n",
            "epoch 12 batch 93 loss 0.04284147545695305\n",
            "epoch 12 batch 94 loss 0.04053010791540146\n",
            "epoch 12 batch 95 loss 0.04481907933950424\n",
            "epoch 12 batch 96 loss 0.03968711197376251\n",
            "epoch 12 batch 97 loss 0.04288467392325401\n",
            "epoch 12 batch 98 loss 0.040589164942502975\n",
            "epoch 12 batch 99 loss 0.04317218437790871\n",
            "epoch 12 batch 100 loss 0.04715346544981003\n",
            "epoch 12 batch 101 loss 0.0381455197930336\n",
            "epoch 12 batch 102 loss 0.040492553263902664\n",
            "epoch 12 batch 103 loss 0.04342081397771835\n",
            "epoch 12 batch 104 loss 0.03819509595632553\n",
            "epoch 12 batch 105 loss 0.03533100709319115\n",
            "epoch 12 batch 106 loss 0.03672213479876518\n",
            "epoch 12 batch 107 loss 0.03841077536344528\n",
            "epoch 12 batch 108 loss 0.04118267074227333\n",
            "epoch 12 batch 109 loss 0.04111267253756523\n",
            "epoch 12 batch 110 loss 0.035591963678598404\n",
            "epoch 12 batch 111 loss 0.0400034561753273\n",
            "epoch 12 batch 112 loss 0.04603198543190956\n",
            "epoch 12 batch 113 loss 0.04025612026453018\n",
            "epoch 12 batch 114 loss 0.03823205456137657\n",
            "epoch 12 batch 115 loss 0.041016772389411926\n",
            "epoch 12 batch 116 loss 0.04152391478419304\n",
            "epoch 12 batch 117 loss 0.04334626719355583\n",
            "epoch 12 batch 118 loss 0.03948163241147995\n",
            "epoch 12 batch 119 loss 0.042181555181741714\n",
            "epoch 12 batch 120 loss 0.03857095539569855\n",
            "epoch 12 batch 121 loss 0.04154925420880318\n",
            "epoch 12 batch 122 loss 0.040671516209840775\n",
            "epoch 12 batch 123 loss 0.03885605186223984\n",
            "epoch 12 batch 124 loss 0.04423580318689346\n",
            "epoch 12 batch 125 loss 0.03842230886220932\n",
            "epoch 12 batch 126 loss 0.03426607325673103\n",
            "epoch 12 batch 127 loss 0.044517625123262405\n",
            "epoch 12 batch 128 loss 0.03840997815132141\n",
            "epoch 12 batch 129 loss 0.03765963017940521\n",
            "epoch 12 batch 130 loss 0.048285480588674545\n",
            "epoch 12 batch 131 loss 0.04375582933425903\n",
            "epoch 12 batch 132 loss 0.043041955679655075\n",
            "epoch 12 batch 133 loss 0.03781137987971306\n",
            "epoch 12 batch 134 loss 0.04295698553323746\n",
            "epoch 12 batch 135 loss 0.046405233442783356\n",
            "epoch 12 batch 136 loss 0.040395889431238174\n",
            "epoch 12 batch 137 loss 0.0390782356262207\n",
            "epoch 12 batch 138 loss 0.040657706558704376\n",
            "epoch 12 batch 139 loss 0.039546508342027664\n",
            "epoch 12 batch 140 loss 0.04234280437231064\n",
            "epoch 12 batch 141 loss 0.039913762360811234\n",
            "epoch 12 batch 142 loss 0.036292385309934616\n",
            "epoch 12 batch 143 loss 0.031875960528850555\n",
            "epoch 12 batch 144 loss 0.038679808378219604\n",
            "epoch 12 batch 145 loss 0.04228992015123367\n",
            "epoch 12 batch 146 loss 0.04624422639608383\n",
            "epoch 12 batch 147 loss 0.04198189452290535\n",
            "epoch 12 batch 148 loss 0.03923633322119713\n",
            "epoch 12 batch 149 loss 0.042025111615657806\n",
            "epoch 12 batch 150 loss 0.040206022560596466\n",
            "epoch 12 batch 151 loss 0.03881523013114929\n",
            "epoch 12 batch 152 loss 0.036893993616104126\n",
            "epoch 12 batch 153 loss 0.039530664682388306\n",
            "epoch 12 batch 154 loss 0.041581444442272186\n",
            "epoch 12 batch 155 loss 0.04119480773806572\n",
            "epoch 12 batch 156 loss 0.04306209832429886\n",
            "epoch 12 batch 157 loss 0.040272168815135956\n",
            "epoch 12 batch 158 loss 0.038364142179489136\n",
            "epoch 12 batch 159 loss 0.035410020500421524\n",
            "epoch 12 batch 160 loss 0.04024466127157211\n",
            "epoch 12 batch 161 loss 0.03952813521027565\n",
            "epoch 12 batch 162 loss 0.04469174146652222\n",
            "epoch 12 batch 163 loss 0.042388033121824265\n",
            "epoch 12 batch 164 loss 0.04029330611228943\n",
            "epoch 12 batch 165 loss 0.0384550504386425\n",
            "epoch 12 batch 166 loss 0.043675344437360764\n",
            "epoch 12 batch 167 loss 0.03916633501648903\n",
            "epoch 12 batch 168 loss 0.04560829699039459\n",
            "epoch 12 batch 169 loss 0.039781808853149414\n",
            "epoch 12 batch 170 loss 0.04046495631337166\n",
            "epoch 12 batch 171 loss 0.04197273775935173\n",
            "epoch 12 batch 172 loss 0.04190952330827713\n",
            "epoch 12 batch 173 loss 0.03919478878378868\n",
            "epoch 12 batch 174 loss 0.040716350078582764\n",
            "epoch 12 batch 175 loss 0.04285617545247078\n",
            "epoch 12 batch 176 loss 0.040392566472291946\n",
            "epoch 12 batch 177 loss 0.03721120208501816\n",
            "epoch 12 batch 178 loss 0.037451062351465225\n",
            "epoch 12 batch 179 loss 0.037334371358156204\n",
            "epoch 12 batch 180 loss 0.04121222719550133\n",
            "epoch 12 batch 181 loss 0.04256829619407654\n",
            "epoch 12 batch 182 loss 0.041625600308179855\n",
            "epoch 12 batch 183 loss 0.042229391634464264\n",
            "epoch 12 batch 184 loss 0.04945513233542442\n",
            "epoch 12 batch 185 loss 0.04240876063704491\n",
            "epoch 12 batch 186 loss 0.04800537973642349\n",
            "epoch 12 batch 187 loss 0.035772670060396194\n",
            "epoch 12 batch 188 loss 0.039651401340961456\n",
            "epoch 12 batch 189 loss 0.04603320732712746\n",
            "epoch 12 batch 190 loss 0.03794863820075989\n",
            "epoch 12 batch 191 loss 0.0428132526576519\n",
            "epoch 12 batch 192 loss 0.03570033609867096\n",
            "epoch 12 batch 193 loss 0.03792287036776543\n",
            "epoch 12 batch 194 loss 0.03854372352361679\n",
            "epoch 12 batch 195 loss 0.04147934168577194\n",
            "epoch 12 batch 196 loss 0.04276376962661743\n",
            "epoch 12 batch 197 loss 0.03718658536672592\n",
            "epoch 12 batch 198 loss 0.037016529589891434\n",
            "epoch 12 batch 199 loss 0.047775402665138245\n",
            "epoch 12 batch 200 loss 0.044131163507699966\n",
            "epoch 12 batch 201 loss 0.04045403003692627\n",
            "epoch 12 batch 202 loss 0.03872551769018173\n",
            "epoch 12 batch 203 loss 0.03957892209291458\n",
            "epoch 12 batch 204 loss 0.03522248566150665\n",
            "epoch 12 batch 205 loss 0.043500978499650955\n",
            "epoch 12 batch 206 loss 0.03342317044734955\n",
            "epoch 12 batch 207 loss 0.03721100836992264\n",
            "epoch 12 batch 208 loss 0.04516099393367767\n",
            "epoch 12 batch 209 loss 0.040566857904195786\n",
            "epoch 12 batch 210 loss 0.048404306173324585\n",
            "epoch 12 batch 211 loss 0.04138806089758873\n",
            "epoch 12 batch 212 loss 0.04005534201860428\n",
            "epoch 12 batch 213 loss 0.04044133797287941\n",
            "epoch 12 batch 214 loss 0.03973108157515526\n",
            "epoch 12 batch 215 loss 0.035373151302337646\n",
            "epoch 12 batch 216 loss 0.039521511644124985\n",
            "epoch 12 batch 217 loss 0.03426500782370567\n",
            "epoch 12 batch 218 loss 0.04078906029462814\n",
            "epoch 12 batch 219 loss 0.04197899252176285\n",
            "epoch 12 batch 220 loss 0.04040030017495155\n",
            "epoch 12 batch 221 loss 0.040124230086803436\n",
            "epoch 12 batch 222 loss 0.04040931910276413\n",
            "epoch 12 batch 223 loss 0.04180491343140602\n",
            "epoch 12 batch 224 loss 0.04214143007993698\n",
            "epoch 12 batch 225 loss 0.040902029722929\n",
            "epoch 12 batch 226 loss 0.039396125823259354\n",
            "epoch 12 batch 227 loss 0.03970974311232567\n",
            "epoch 12 batch 228 loss 0.03897249326109886\n",
            "epoch 12 batch 229 loss 0.03853743150830269\n",
            "epoch 12 batch 230 loss 0.0262379702180624\n",
            "epoch 13 batch 0 loss 0.04229738563299179\n",
            "epoch 13 batch 1 loss 0.041257575154304504\n",
            "epoch 13 batch 2 loss 0.04651258513331413\n",
            "epoch 13 batch 3 loss 0.04319533333182335\n",
            "epoch 13 batch 4 loss 0.041878025978803635\n",
            "epoch 13 batch 5 loss 0.040397755801677704\n",
            "epoch 13 batch 6 loss 0.03978034481406212\n",
            "epoch 13 batch 7 loss 0.045922283083200455\n",
            "epoch 13 batch 8 loss 0.04066667705774307\n",
            "epoch 13 batch 9 loss 0.04106253385543823\n",
            "epoch 13 batch 10 loss 0.048288457095623016\n",
            "epoch 13 batch 11 loss 0.03732535243034363\n",
            "epoch 13 batch 12 loss 0.03839363530278206\n",
            "epoch 13 batch 13 loss 0.047335460782051086\n",
            "epoch 13 batch 14 loss 0.03893709182739258\n",
            "epoch 13 batch 15 loss 0.04167046770453453\n",
            "epoch 13 batch 16 loss 0.039663758128881454\n",
            "epoch 13 batch 17 loss 0.041872452944517136\n",
            "epoch 13 batch 18 loss 0.04173712059855461\n",
            "epoch 13 batch 19 loss 0.04063119739294052\n",
            "epoch 13 batch 20 loss 0.04196858033537865\n",
            "epoch 13 batch 21 loss 0.0413450188934803\n",
            "epoch 13 batch 22 loss 0.038001783192157745\n",
            "epoch 13 batch 23 loss 0.04265217110514641\n",
            "epoch 13 batch 24 loss 0.04178442433476448\n",
            "epoch 13 batch 25 loss 0.05021718144416809\n",
            "epoch 13 batch 26 loss 0.039801038801670074\n",
            "epoch 13 batch 27 loss 0.047065477818250656\n",
            "epoch 13 batch 28 loss 0.036437783390283585\n",
            "epoch 13 batch 29 loss 0.04168260470032692\n",
            "epoch 13 batch 30 loss 0.04144091159105301\n",
            "epoch 13 batch 31 loss 0.0408211275935173\n",
            "epoch 13 batch 32 loss 0.03676963225007057\n",
            "epoch 13 batch 33 loss 0.03926666080951691\n",
            "epoch 13 batch 34 loss 0.03718740493059158\n",
            "epoch 13 batch 35 loss 0.04066240414977074\n",
            "epoch 13 batch 36 loss 0.04782949388027191\n",
            "epoch 13 batch 37 loss 0.04332008212804794\n",
            "epoch 13 batch 38 loss 0.04033168777823448\n",
            "epoch 13 batch 39 loss 0.038626302033662796\n",
            "epoch 13 batch 40 loss 0.04213276505470276\n",
            "epoch 13 batch 41 loss 0.03981490433216095\n",
            "epoch 13 batch 42 loss 0.0390663780272007\n",
            "epoch 13 batch 43 loss 0.04426708072423935\n",
            "epoch 13 batch 44 loss 0.044092271476984024\n",
            "epoch 13 batch 45 loss 0.037816207855939865\n",
            "epoch 13 batch 46 loss 0.04042871668934822\n",
            "epoch 13 batch 47 loss 0.04464874044060707\n",
            "epoch 13 batch 48 loss 0.035635460168123245\n",
            "epoch 13 batch 49 loss 0.039618153125047684\n",
            "epoch 13 batch 50 loss 0.0435921736061573\n",
            "epoch 13 batch 51 loss 0.04292354732751846\n",
            "epoch 13 batch 52 loss 0.03976908326148987\n",
            "epoch 13 batch 53 loss 0.03773774579167366\n",
            "epoch 13 batch 54 loss 0.04288806766271591\n",
            "epoch 13 batch 55 loss 0.03902766853570938\n",
            "epoch 13 batch 56 loss 0.03895414248108864\n",
            "epoch 13 batch 57 loss 0.048639439046382904\n",
            "epoch 13 batch 58 loss 0.04064040631055832\n",
            "epoch 13 batch 59 loss 0.04027848690748215\n",
            "epoch 13 batch 60 loss 0.04436390474438667\n",
            "epoch 13 batch 61 loss 0.039337996393442154\n",
            "epoch 13 batch 62 loss 0.039658840745687485\n",
            "epoch 13 batch 63 loss 0.041770219802856445\n",
            "epoch 13 batch 64 loss 0.04511227086186409\n",
            "epoch 13 batch 65 loss 0.03684944659471512\n",
            "epoch 13 batch 66 loss 0.04435702785849571\n",
            "epoch 13 batch 67 loss 0.04582728445529938\n",
            "epoch 13 batch 68 loss 0.04006708785891533\n",
            "epoch 13 batch 69 loss 0.04343993961811066\n",
            "epoch 13 batch 70 loss 0.04354773461818695\n",
            "epoch 13 batch 71 loss 0.03744744136929512\n",
            "epoch 13 batch 72 loss 0.039555445313453674\n",
            "epoch 13 batch 73 loss 0.04589643329381943\n",
            "epoch 13 batch 74 loss 0.03988645598292351\n",
            "epoch 13 batch 75 loss 0.043681997805833817\n",
            "epoch 13 batch 76 loss 0.036370303481817245\n",
            "epoch 13 batch 77 loss 0.04104781523346901\n",
            "epoch 13 batch 78 loss 0.04125561565160751\n",
            "epoch 13 batch 79 loss 0.03990265727043152\n",
            "epoch 13 batch 80 loss 0.042958278208971024\n",
            "epoch 13 batch 81 loss 0.03838055580854416\n",
            "epoch 13 batch 82 loss 0.04426247999072075\n",
            "epoch 13 batch 83 loss 0.038518983870744705\n",
            "epoch 13 batch 84 loss 0.04022146388888359\n",
            "epoch 13 batch 85 loss 0.042845938354730606\n",
            "epoch 13 batch 86 loss 0.035055067390203476\n",
            "epoch 13 batch 87 loss 0.03856347128748894\n",
            "epoch 13 batch 88 loss 0.036721017211675644\n",
            "epoch 13 batch 89 loss 0.045023031532764435\n",
            "epoch 13 batch 90 loss 0.03936394304037094\n",
            "epoch 13 batch 91 loss 0.04127258062362671\n",
            "epoch 13 batch 92 loss 0.041820231825113297\n",
            "epoch 13 batch 93 loss 0.04260782524943352\n",
            "epoch 13 batch 94 loss 0.03989420086145401\n",
            "epoch 13 batch 95 loss 0.04413194581866264\n",
            "epoch 13 batch 96 loss 0.04042772203683853\n",
            "epoch 13 batch 97 loss 0.04142177477478981\n",
            "epoch 13 batch 98 loss 0.039356205612421036\n",
            "epoch 13 batch 99 loss 0.04367559030652046\n",
            "epoch 13 batch 100 loss 0.04638703167438507\n",
            "epoch 13 batch 101 loss 0.03691839426755905\n",
            "epoch 13 batch 102 loss 0.039589934051036835\n",
            "epoch 13 batch 103 loss 0.04276755824685097\n",
            "epoch 13 batch 104 loss 0.039066415280103683\n",
            "epoch 13 batch 105 loss 0.034934818744659424\n",
            "epoch 13 batch 106 loss 0.03700193017721176\n",
            "epoch 13 batch 107 loss 0.03730406612157822\n",
            "epoch 13 batch 108 loss 0.04083578661084175\n",
            "epoch 13 batch 109 loss 0.04063915088772774\n",
            "epoch 13 batch 110 loss 0.034743085503578186\n",
            "epoch 13 batch 111 loss 0.0392928309738636\n",
            "epoch 13 batch 112 loss 0.04576817527413368\n",
            "epoch 13 batch 113 loss 0.04096926748752594\n",
            "epoch 13 batch 114 loss 0.037598878145217896\n",
            "epoch 13 batch 115 loss 0.04048962891101837\n",
            "epoch 13 batch 116 loss 0.040444452315568924\n",
            "epoch 13 batch 117 loss 0.041600871831178665\n",
            "epoch 13 batch 118 loss 0.038518186658620834\n",
            "epoch 13 batch 119 loss 0.04136525094509125\n",
            "epoch 13 batch 120 loss 0.03803327679634094\n",
            "epoch 13 batch 121 loss 0.03889511153101921\n",
            "epoch 13 batch 122 loss 0.03937625139951706\n",
            "epoch 13 batch 123 loss 0.03791673481464386\n",
            "epoch 13 batch 124 loss 0.04235224425792694\n",
            "epoch 13 batch 125 loss 0.03780190274119377\n",
            "epoch 13 batch 126 loss 0.033028148114681244\n",
            "epoch 13 batch 127 loss 0.04375427961349487\n",
            "epoch 13 batch 128 loss 0.03730466589331627\n",
            "epoch 13 batch 129 loss 0.0361456461250782\n",
            "epoch 13 batch 130 loss 0.04812104254961014\n",
            "epoch 13 batch 131 loss 0.04155363887548447\n",
            "epoch 13 batch 132 loss 0.043529704213142395\n",
            "epoch 13 batch 133 loss 0.03735848516225815\n",
            "epoch 13 batch 134 loss 0.04114999622106552\n",
            "epoch 13 batch 135 loss 0.04406232759356499\n",
            "epoch 13 batch 136 loss 0.039674438536167145\n",
            "epoch 13 batch 137 loss 0.037708085030317307\n",
            "epoch 13 batch 138 loss 0.03990894928574562\n",
            "epoch 13 batch 139 loss 0.03861815109848976\n",
            "epoch 13 batch 140 loss 0.04161236062645912\n",
            "epoch 13 batch 141 loss 0.03844931721687317\n",
            "epoch 13 batch 142 loss 0.03555009514093399\n",
            "epoch 13 batch 143 loss 0.03123335912823677\n",
            "epoch 13 batch 144 loss 0.03686593100428581\n",
            "epoch 13 batch 145 loss 0.040405210107564926\n",
            "epoch 13 batch 146 loss 0.04555895924568176\n",
            "epoch 13 batch 147 loss 0.04030170664191246\n",
            "epoch 13 batch 148 loss 0.03800263628363609\n",
            "epoch 13 batch 149 loss 0.040484149008989334\n",
            "epoch 13 batch 150 loss 0.03896094858646393\n",
            "epoch 13 batch 151 loss 0.0389215312898159\n",
            "epoch 13 batch 152 loss 0.03671078756451607\n",
            "epoch 13 batch 153 loss 0.039316486567258835\n",
            "epoch 13 batch 154 loss 0.03908107802271843\n",
            "epoch 13 batch 155 loss 0.03905986621975899\n",
            "epoch 13 batch 156 loss 0.04091430827975273\n",
            "epoch 13 batch 157 loss 0.03963448107242584\n",
            "epoch 13 batch 158 loss 0.037244655191898346\n",
            "epoch 13 batch 159 loss 0.03266996145248413\n",
            "epoch 13 batch 160 loss 0.03892393410205841\n",
            "epoch 13 batch 161 loss 0.038351546972990036\n",
            "epoch 13 batch 162 loss 0.04304107278585434\n",
            "epoch 13 batch 163 loss 0.03925194591283798\n",
            "epoch 13 batch 164 loss 0.039176229387521744\n",
            "epoch 13 batch 165 loss 0.0372990146279335\n",
            "epoch 13 batch 166 loss 0.04117899015545845\n",
            "epoch 13 batch 167 loss 0.0379079170525074\n",
            "epoch 13 batch 168 loss 0.04367007315158844\n",
            "epoch 13 batch 169 loss 0.03872540593147278\n",
            "epoch 13 batch 170 loss 0.038921017199754715\n",
            "epoch 13 batch 171 loss 0.039935532957315445\n",
            "epoch 13 batch 172 loss 0.040812645107507706\n",
            "epoch 13 batch 173 loss 0.0380844809114933\n",
            "epoch 13 batch 174 loss 0.03901738300919533\n",
            "epoch 13 batch 175 loss 0.04295340180397034\n",
            "epoch 13 batch 176 loss 0.03987022489309311\n",
            "epoch 13 batch 177 loss 0.036270271986722946\n",
            "epoch 13 batch 178 loss 0.03663381189107895\n",
            "epoch 13 batch 179 loss 0.0357893668115139\n",
            "epoch 13 batch 180 loss 0.038834214210510254\n",
            "epoch 13 batch 181 loss 0.04099390655755997\n",
            "epoch 13 batch 182 loss 0.040754955261945724\n",
            "epoch 13 batch 183 loss 0.04184243455529213\n",
            "epoch 13 batch 184 loss 0.04727984219789505\n",
            "epoch 13 batch 185 loss 0.04018945246934891\n",
            "epoch 13 batch 186 loss 0.04623636230826378\n",
            "epoch 13 batch 187 loss 0.0353347510099411\n",
            "epoch 13 batch 188 loss 0.03719810023903847\n",
            "epoch 13 batch 189 loss 0.045766349881887436\n",
            "epoch 13 batch 190 loss 0.03757643699645996\n",
            "epoch 13 batch 191 loss 0.041435882449150085\n",
            "epoch 13 batch 192 loss 0.035345762968063354\n",
            "epoch 13 batch 193 loss 0.03754870966076851\n",
            "epoch 13 batch 194 loss 0.03778553754091263\n",
            "epoch 13 batch 195 loss 0.03960319980978966\n",
            "epoch 13 batch 196 loss 0.042771048843860626\n",
            "epoch 13 batch 197 loss 0.03621429577469826\n",
            "epoch 13 batch 198 loss 0.03583519905805588\n",
            "epoch 13 batch 199 loss 0.04698649421334267\n",
            "epoch 13 batch 200 loss 0.04370534047484398\n",
            "epoch 13 batch 201 loss 0.04031209275126457\n",
            "epoch 13 batch 202 loss 0.03729019686579704\n",
            "epoch 13 batch 203 loss 0.03847930580377579\n",
            "epoch 13 batch 204 loss 0.033721648156642914\n",
            "epoch 13 batch 205 loss 0.042518068104982376\n",
            "epoch 13 batch 206 loss 0.03244462609291077\n",
            "epoch 13 batch 207 loss 0.037118081003427505\n",
            "epoch 13 batch 208 loss 0.04344543069601059\n",
            "epoch 13 batch 209 loss 0.03863871097564697\n",
            "epoch 13 batch 210 loss 0.046458650380373\n",
            "epoch 13 batch 211 loss 0.0412231907248497\n",
            "epoch 13 batch 212 loss 0.0381966270506382\n",
            "epoch 13 batch 213 loss 0.03873308375477791\n",
            "epoch 13 batch 214 loss 0.03840143606066704\n",
            "epoch 13 batch 215 loss 0.03458647057414055\n",
            "epoch 13 batch 216 loss 0.03742656856775284\n",
            "epoch 13 batch 217 loss 0.03335755690932274\n",
            "epoch 13 batch 218 loss 0.038677625358104706\n",
            "epoch 13 batch 219 loss 0.040190503001213074\n",
            "epoch 13 batch 220 loss 0.03934570774435997\n",
            "epoch 13 batch 221 loss 0.03936496004462242\n",
            "epoch 13 batch 222 loss 0.03965727984905243\n",
            "epoch 13 batch 223 loss 0.04008084908127785\n",
            "epoch 13 batch 224 loss 0.0403076596558094\n",
            "epoch 13 batch 225 loss 0.03974677249789238\n",
            "epoch 13 batch 226 loss 0.03772979602217674\n",
            "epoch 13 batch 227 loss 0.03817810118198395\n",
            "epoch 13 batch 228 loss 0.038926929235458374\n",
            "epoch 13 batch 229 loss 0.03864006698131561\n",
            "epoch 13 batch 230 loss 0.024448739364743233\n",
            "epoch 14 batch 0 loss 0.040332019329071045\n",
            "epoch 14 batch 1 loss 0.04119017720222473\n",
            "epoch 14 batch 2 loss 0.04507937282323837\n",
            "epoch 14 batch 3 loss 0.0416497103869915\n",
            "epoch 14 batch 4 loss 0.04055986553430557\n",
            "epoch 14 batch 5 loss 0.0399010144174099\n",
            "epoch 14 batch 6 loss 0.03882855921983719\n",
            "epoch 14 batch 7 loss 0.04500114917755127\n",
            "epoch 14 batch 8 loss 0.04002324119210243\n",
            "epoch 14 batch 9 loss 0.040462199598550797\n",
            "epoch 14 batch 10 loss 0.04559709504246712\n",
            "epoch 14 batch 11 loss 0.03774544224143028\n",
            "epoch 14 batch 12 loss 0.03862094134092331\n",
            "epoch 14 batch 13 loss 0.044169627130031586\n",
            "epoch 14 batch 14 loss 0.03684956952929497\n",
            "epoch 14 batch 15 loss 0.04195516183972359\n",
            "epoch 14 batch 16 loss 0.03869561851024628\n",
            "epoch 14 batch 17 loss 0.03996305167675018\n",
            "epoch 14 batch 18 loss 0.039834752678871155\n",
            "epoch 14 batch 19 loss 0.042057618498802185\n",
            "epoch 14 batch 20 loss 0.04001399874687195\n",
            "epoch 14 batch 21 loss 0.03962288424372673\n",
            "epoch 14 batch 22 loss 0.03456758335232735\n",
            "epoch 14 batch 23 loss 0.04154498875141144\n",
            "epoch 14 batch 24 loss 0.039212193340063095\n",
            "epoch 14 batch 25 loss 0.048958685249090195\n",
            "epoch 14 batch 26 loss 0.03878232464194298\n",
            "epoch 14 batch 27 loss 0.04469195753335953\n",
            "epoch 14 batch 28 loss 0.0355420745909214\n",
            "epoch 14 batch 29 loss 0.04067161679267883\n",
            "epoch 14 batch 30 loss 0.04107046127319336\n",
            "epoch 14 batch 31 loss 0.03944546356797218\n",
            "epoch 14 batch 32 loss 0.03493794426321983\n",
            "epoch 14 batch 33 loss 0.03825344517827034\n",
            "epoch 14 batch 34 loss 0.03624251112341881\n",
            "epoch 14 batch 35 loss 0.03916702792048454\n",
            "epoch 14 batch 36 loss 0.04696253687143326\n",
            "epoch 14 batch 37 loss 0.04170225188136101\n",
            "epoch 14 batch 38 loss 0.038447801023721695\n",
            "epoch 14 batch 39 loss 0.03797449544072151\n",
            "epoch 14 batch 40 loss 0.040249068289995193\n",
            "epoch 14 batch 41 loss 0.038964562118053436\n",
            "epoch 14 batch 42 loss 0.038816116750240326\n",
            "epoch 14 batch 43 loss 0.043374862521886826\n",
            "epoch 14 batch 44 loss 0.04281558468937874\n",
            "epoch 14 batch 45 loss 0.03715643659234047\n",
            "epoch 14 batch 46 loss 0.03834117576479912\n",
            "epoch 14 batch 47 loss 0.04330354556441307\n",
            "epoch 14 batch 48 loss 0.03688704967498779\n",
            "epoch 14 batch 49 loss 0.03856777772307396\n",
            "epoch 14 batch 50 loss 0.04161263257265091\n",
            "epoch 14 batch 51 loss 0.04106896370649338\n",
            "epoch 14 batch 52 loss 0.039345260709524155\n",
            "epoch 14 batch 53 loss 0.03515022248029709\n",
            "epoch 14 batch 54 loss 0.04124816134572029\n",
            "epoch 14 batch 55 loss 0.03848647326231003\n",
            "epoch 14 batch 56 loss 0.036835890263319016\n",
            "epoch 14 batch 57 loss 0.04629509150981903\n",
            "epoch 14 batch 58 loss 0.04100828990340233\n",
            "epoch 14 batch 59 loss 0.03824242949485779\n",
            "epoch 14 batch 60 loss 0.040813688188791275\n",
            "epoch 14 batch 61 loss 0.0387803316116333\n",
            "epoch 14 batch 62 loss 0.037504952400922775\n",
            "epoch 14 batch 63 loss 0.03946726396679878\n",
            "epoch 14 batch 64 loss 0.04314809292554855\n",
            "epoch 14 batch 65 loss 0.03602844476699829\n",
            "epoch 14 batch 66 loss 0.04170652851462364\n",
            "epoch 14 batch 67 loss 0.04284702613949776\n",
            "epoch 14 batch 68 loss 0.038781192153692245\n",
            "epoch 14 batch 69 loss 0.03977369889616966\n",
            "epoch 14 batch 70 loss 0.04180195555090904\n",
            "epoch 14 batch 71 loss 0.03479189798235893\n",
            "epoch 14 batch 72 loss 0.03806909918785095\n",
            "epoch 14 batch 73 loss 0.04218519479036331\n",
            "epoch 14 batch 74 loss 0.038988884538412094\n",
            "epoch 14 batch 75 loss 0.040448714047670364\n",
            "epoch 14 batch 76 loss 0.034279439598321915\n",
            "epoch 14 batch 77 loss 0.03908289596438408\n",
            "epoch 14 batch 78 loss 0.03845551609992981\n",
            "epoch 14 batch 79 loss 0.03625541552901268\n",
            "epoch 14 batch 80 loss 0.04133937507867813\n",
            "epoch 14 batch 81 loss 0.037922631949186325\n",
            "epoch 14 batch 82 loss 0.0409344807267189\n",
            "epoch 14 batch 83 loss 0.03645888715982437\n",
            "epoch 14 batch 84 loss 0.03790709003806114\n",
            "epoch 14 batch 85 loss 0.04078285023570061\n",
            "epoch 14 batch 86 loss 0.03258785232901573\n",
            "epoch 14 batch 87 loss 0.03724276274442673\n",
            "epoch 14 batch 88 loss 0.03543967381119728\n",
            "epoch 14 batch 89 loss 0.04236933961510658\n",
            "epoch 14 batch 90 loss 0.03776898980140686\n",
            "epoch 14 batch 91 loss 0.04079112783074379\n",
            "epoch 14 batch 92 loss 0.03898260369896889\n",
            "epoch 14 batch 93 loss 0.04058225452899933\n",
            "epoch 14 batch 94 loss 0.038339342921972275\n",
            "epoch 14 batch 95 loss 0.041940487921237946\n",
            "epoch 14 batch 96 loss 0.03738865256309509\n",
            "epoch 14 batch 97 loss 0.04084126651287079\n",
            "epoch 14 batch 98 loss 0.037719883024692535\n",
            "epoch 14 batch 99 loss 0.04062934219837189\n",
            "epoch 14 batch 100 loss 0.04410407319664955\n",
            "epoch 14 batch 101 loss 0.03581113740801811\n",
            "epoch 14 batch 102 loss 0.03755728900432587\n",
            "epoch 14 batch 103 loss 0.039868853986263275\n",
            "epoch 14 batch 104 loss 0.037550996989011765\n",
            "epoch 14 batch 105 loss 0.03458985313773155\n",
            "epoch 14 batch 106 loss 0.035349391400814056\n",
            "epoch 14 batch 107 loss 0.03688645735383034\n",
            "epoch 14 batch 108 loss 0.03927704691886902\n",
            "epoch 14 batch 109 loss 0.038229767233133316\n",
            "epoch 14 batch 110 loss 0.03469939902424812\n",
            "epoch 14 batch 111 loss 0.03820287436246872\n",
            "epoch 14 batch 112 loss 0.04335293918848038\n",
            "epoch 14 batch 113 loss 0.039549726992845535\n",
            "epoch 14 batch 114 loss 0.037297382950782776\n",
            "epoch 14 batch 115 loss 0.04001300036907196\n",
            "epoch 14 batch 116 loss 0.039365559816360474\n",
            "epoch 14 batch 117 loss 0.04107390344142914\n",
            "epoch 14 batch 118 loss 0.038117412477731705\n",
            "epoch 14 batch 119 loss 0.040205296128988266\n",
            "epoch 14 batch 120 loss 0.037658389657735825\n",
            "epoch 14 batch 121 loss 0.03890671581029892\n",
            "epoch 14 batch 122 loss 0.03853343799710274\n",
            "epoch 14 batch 123 loss 0.037362512201070786\n",
            "epoch 14 batch 124 loss 0.041761364787817\n",
            "epoch 14 batch 125 loss 0.03658885136246681\n",
            "epoch 14 batch 126 loss 0.032644122838974\n",
            "epoch 14 batch 127 loss 0.042155180126428604\n",
            "epoch 14 batch 128 loss 0.03743898496031761\n",
            "epoch 14 batch 129 loss 0.0349930115044117\n",
            "epoch 14 batch 130 loss 0.04703238233923912\n",
            "epoch 14 batch 131 loss 0.040057696402072906\n",
            "epoch 14 batch 132 loss 0.041436567902565\n",
            "epoch 14 batch 133 loss 0.03644827380776405\n",
            "epoch 14 batch 134 loss 0.041011206805706024\n",
            "epoch 14 batch 135 loss 0.04336967319250107\n",
            "epoch 14 batch 136 loss 0.03878307342529297\n",
            "epoch 14 batch 137 loss 0.03650970384478569\n",
            "epoch 14 batch 138 loss 0.03951239585876465\n",
            "epoch 14 batch 139 loss 0.038087960332632065\n",
            "epoch 14 batch 140 loss 0.041252829134464264\n",
            "epoch 14 batch 141 loss 0.038312170654535294\n",
            "epoch 14 batch 142 loss 0.034880463033914566\n",
            "epoch 14 batch 143 loss 0.03168771415948868\n",
            "epoch 14 batch 144 loss 0.03636445477604866\n",
            "epoch 14 batch 145 loss 0.03971181437373161\n",
            "epoch 14 batch 146 loss 0.04432155191898346\n",
            "epoch 14 batch 147 loss 0.03996792063117027\n",
            "epoch 14 batch 148 loss 0.037203073501586914\n",
            "epoch 14 batch 149 loss 0.03994302824139595\n",
            "epoch 14 batch 150 loss 0.037367723882198334\n",
            "epoch 14 batch 151 loss 0.036316968500614166\n",
            "epoch 14 batch 152 loss 0.03513890877366066\n",
            "epoch 14 batch 153 loss 0.039594732224941254\n",
            "epoch 14 batch 154 loss 0.03876727446913719\n",
            "epoch 14 batch 155 loss 0.03792280703783035\n",
            "epoch 14 batch 156 loss 0.03994901478290558\n",
            "epoch 14 batch 157 loss 0.03777807950973511\n",
            "epoch 14 batch 158 loss 0.03585226833820343\n",
            "epoch 14 batch 159 loss 0.03225855901837349\n",
            "epoch 14 batch 160 loss 0.037637583911418915\n",
            "epoch 14 batch 161 loss 0.03693874180316925\n",
            "epoch 14 batch 162 loss 0.04252129793167114\n",
            "epoch 14 batch 163 loss 0.038755737245082855\n",
            "epoch 14 batch 164 loss 0.03766017407178879\n",
            "epoch 14 batch 165 loss 0.03666362166404724\n",
            "epoch 14 batch 166 loss 0.03973226621747017\n",
            "epoch 14 batch 167 loss 0.036820799112319946\n",
            "epoch 14 batch 168 loss 0.042213935405015945\n",
            "epoch 14 batch 169 loss 0.03793812170624733\n",
            "epoch 14 batch 170 loss 0.037409067153930664\n",
            "epoch 14 batch 171 loss 0.03829289972782135\n",
            "epoch 14 batch 172 loss 0.03972296044230461\n",
            "epoch 14 batch 173 loss 0.036772727966308594\n",
            "epoch 14 batch 174 loss 0.037737857550382614\n",
            "epoch 14 batch 175 loss 0.04182390123605728\n",
            "epoch 14 batch 176 loss 0.03837105259299278\n",
            "epoch 14 batch 177 loss 0.03542859107255936\n",
            "epoch 14 batch 178 loss 0.03638601303100586\n",
            "epoch 14 batch 179 loss 0.03506091609597206\n",
            "epoch 14 batch 180 loss 0.03773343563079834\n",
            "epoch 14 batch 181 loss 0.039057694375514984\n",
            "epoch 14 batch 182 loss 0.03842335566878319\n",
            "epoch 14 batch 183 loss 0.040759116411209106\n",
            "epoch 14 batch 184 loss 0.04518118500709534\n",
            "epoch 14 batch 185 loss 0.038260579109191895\n",
            "epoch 14 batch 186 loss 0.04455949366092682\n",
            "epoch 14 batch 187 loss 0.034305140376091\n",
            "epoch 14 batch 188 loss 0.03592301532626152\n",
            "epoch 14 batch 189 loss 0.04359222948551178\n",
            "epoch 14 batch 190 loss 0.03634369745850563\n",
            "epoch 14 batch 191 loss 0.04050135239958763\n",
            "epoch 14 batch 192 loss 0.033912528306245804\n",
            "epoch 14 batch 193 loss 0.03625926747918129\n",
            "epoch 14 batch 194 loss 0.03596055135130882\n",
            "epoch 14 batch 195 loss 0.037274863570928574\n",
            "epoch 14 batch 196 loss 0.03946546092629433\n",
            "epoch 14 batch 197 loss 0.03533906117081642\n",
            "epoch 14 batch 198 loss 0.03535103425383568\n",
            "epoch 14 batch 199 loss 0.04460102319717407\n",
            "epoch 14 batch 200 loss 0.04196750745177269\n",
            "epoch 14 batch 201 loss 0.040216878056526184\n",
            "epoch 14 batch 202 loss 0.03667230159044266\n",
            "epoch 14 batch 203 loss 0.037023499608039856\n",
            "epoch 14 batch 204 loss 0.033384304493665695\n",
            "epoch 14 batch 205 loss 0.041441384702920914\n",
            "epoch 14 batch 206 loss 0.03185826167464256\n",
            "epoch 14 batch 207 loss 0.03690117225050926\n",
            "epoch 14 batch 208 loss 0.04228316247463226\n",
            "epoch 14 batch 209 loss 0.038372889161109924\n",
            "epoch 14 batch 210 loss 0.044122517108917236\n",
            "epoch 14 batch 211 loss 0.040711745619773865\n",
            "epoch 14 batch 212 loss 0.03638547286391258\n",
            "epoch 14 batch 213 loss 0.03707917779684067\n",
            "epoch 14 batch 214 loss 0.03787161037325859\n",
            "epoch 14 batch 215 loss 0.03384943678975105\n",
            "epoch 14 batch 216 loss 0.03641294315457344\n",
            "epoch 14 batch 217 loss 0.031565167009830475\n",
            "epoch 14 batch 218 loss 0.03810751438140869\n",
            "epoch 14 batch 219 loss 0.04151904955506325\n",
            "epoch 14 batch 220 loss 0.03884046897292137\n",
            "epoch 14 batch 221 loss 0.03765158727765083\n",
            "epoch 14 batch 222 loss 0.037593137472867966\n",
            "epoch 14 batch 223 loss 0.03941675275564194\n",
            "epoch 14 batch 224 loss 0.039215266704559326\n",
            "epoch 14 batch 225 loss 0.038170743733644485\n",
            "epoch 14 batch 226 loss 0.03816855326294899\n",
            "epoch 14 batch 227 loss 0.03858514130115509\n",
            "epoch 14 batch 228 loss 0.037998300045728683\n",
            "epoch 14 batch 229 loss 0.03824372962117195\n",
            "epoch 14 batch 230 loss 0.023210633546113968\n",
            "epoch 15 batch 0 loss 0.04001297056674957\n",
            "epoch 15 batch 1 loss 0.0387796089053154\n",
            "epoch 15 batch 2 loss 0.04336247220635414\n",
            "epoch 15 batch 3 loss 0.040820423513650894\n",
            "epoch 15 batch 4 loss 0.040237657725811005\n",
            "epoch 15 batch 5 loss 0.03751794621348381\n",
            "epoch 15 batch 6 loss 0.03830994665622711\n",
            "epoch 15 batch 7 loss 0.04349218308925629\n",
            "epoch 15 batch 8 loss 0.03861823305487633\n",
            "epoch 15 batch 9 loss 0.038452714681625366\n",
            "epoch 15 batch 10 loss 0.04382188618183136\n",
            "epoch 15 batch 11 loss 0.036324385553598404\n",
            "epoch 15 batch 12 loss 0.036516088992357254\n",
            "epoch 15 batch 13 loss 0.04386235028505325\n",
            "epoch 15 batch 14 loss 0.03560233116149902\n",
            "epoch 15 batch 15 loss 0.03950107470154762\n",
            "epoch 15 batch 16 loss 0.038040123879909515\n",
            "epoch 15 batch 17 loss 0.03891047090291977\n",
            "epoch 15 batch 18 loss 0.03718102350831032\n",
            "epoch 15 batch 19 loss 0.03925696760416031\n",
            "epoch 15 batch 20 loss 0.039710499346256256\n",
            "epoch 15 batch 21 loss 0.03847262263298035\n",
            "epoch 15 batch 22 loss 0.032956287264823914\n",
            "epoch 15 batch 23 loss 0.03909316286444664\n",
            "epoch 15 batch 24 loss 0.03717224672436714\n",
            "epoch 15 batch 25 loss 0.046628426760435104\n",
            "epoch 15 batch 26 loss 0.03736285865306854\n",
            "epoch 15 batch 27 loss 0.04269406571984291\n",
            "epoch 15 batch 28 loss 0.03492434695363045\n",
            "epoch 15 batch 29 loss 0.038930706679821014\n",
            "epoch 15 batch 30 loss 0.03933659568428993\n",
            "epoch 15 batch 31 loss 0.037676408886909485\n",
            "epoch 15 batch 32 loss 0.03379008173942566\n",
            "epoch 15 batch 33 loss 0.0370258130133152\n",
            "epoch 15 batch 34 loss 0.03490287438035011\n",
            "epoch 15 batch 35 loss 0.03702392056584358\n",
            "epoch 15 batch 36 loss 0.04662979394197464\n",
            "epoch 15 batch 37 loss 0.03912678733468056\n",
            "epoch 15 batch 38 loss 0.03713585063815117\n",
            "epoch 15 batch 39 loss 0.03585227578878403\n",
            "epoch 15 batch 40 loss 0.03969503939151764\n",
            "epoch 15 batch 41 loss 0.037594910711050034\n",
            "epoch 15 batch 42 loss 0.036879390478134155\n",
            "epoch 15 batch 43 loss 0.04146745800971985\n",
            "epoch 15 batch 44 loss 0.04058680683374405\n",
            "epoch 15 batch 45 loss 0.03604288771748543\n",
            "epoch 15 batch 46 loss 0.03680599108338356\n",
            "epoch 15 batch 47 loss 0.04188060760498047\n",
            "epoch 15 batch 48 loss 0.03573378175497055\n",
            "epoch 15 batch 49 loss 0.03761497512459755\n",
            "epoch 15 batch 50 loss 0.03957604616880417\n",
            "epoch 15 batch 51 loss 0.040845178067684174\n",
            "epoch 15 batch 52 loss 0.03720163181424141\n",
            "epoch 15 batch 53 loss 0.033789850771427155\n",
            "epoch 15 batch 54 loss 0.03935236856341362\n",
            "epoch 15 batch 55 loss 0.037347376346588135\n",
            "epoch 15 batch 56 loss 0.03472699224948883\n",
            "epoch 15 batch 57 loss 0.04463765770196915\n",
            "epoch 15 batch 58 loss 0.03997093439102173\n",
            "epoch 15 batch 59 loss 0.03783847764134407\n",
            "epoch 15 batch 60 loss 0.038028061389923096\n",
            "epoch 15 batch 61 loss 0.03758320212364197\n",
            "epoch 15 batch 62 loss 0.03759907931089401\n",
            "epoch 15 batch 63 loss 0.037965938448905945\n",
            "epoch 15 batch 64 loss 0.041123196482658386\n",
            "epoch 15 batch 65 loss 0.03584165871143341\n",
            "epoch 15 batch 66 loss 0.03945395350456238\n",
            "epoch 15 batch 67 loss 0.04263126850128174\n",
            "epoch 15 batch 68 loss 0.03853835538029671\n",
            "epoch 15 batch 69 loss 0.03912836313247681\n",
            "epoch 15 batch 70 loss 0.03958448767662048\n",
            "epoch 15 batch 71 loss 0.034311871975660324\n",
            "epoch 15 batch 72 loss 0.03811577335000038\n",
            "epoch 15 batch 73 loss 0.04090322554111481\n",
            "epoch 15 batch 74 loss 0.03765002265572548\n",
            "epoch 15 batch 75 loss 0.04066259413957596\n",
            "epoch 15 batch 76 loss 0.032863568514585495\n",
            "epoch 15 batch 77 loss 0.03850998729467392\n",
            "epoch 15 batch 78 loss 0.03774833679199219\n",
            "epoch 15 batch 79 loss 0.03520471230149269\n",
            "epoch 15 batch 80 loss 0.03848561644554138\n",
            "epoch 15 batch 81 loss 0.03650255501270294\n",
            "epoch 15 batch 82 loss 0.03955722600221634\n",
            "epoch 15 batch 83 loss 0.03480039909482002\n",
            "epoch 15 batch 84 loss 0.03583872690796852\n",
            "epoch 15 batch 85 loss 0.03993825986981392\n",
            "epoch 15 batch 86 loss 0.03278464823961258\n",
            "epoch 15 batch 87 loss 0.03662116825580597\n",
            "epoch 15 batch 88 loss 0.034562788903713226\n",
            "epoch 15 batch 89 loss 0.04040180891752243\n",
            "epoch 15 batch 90 loss 0.03621622920036316\n",
            "epoch 15 batch 91 loss 0.04022753983736038\n",
            "epoch 15 batch 92 loss 0.03844635188579559\n",
            "epoch 15 batch 93 loss 0.039965130388736725\n",
            "epoch 15 batch 94 loss 0.03665792942047119\n",
            "epoch 15 batch 95 loss 0.040296588093042374\n",
            "epoch 15 batch 96 loss 0.035903450101614\n",
            "epoch 15 batch 97 loss 0.038505081087350845\n",
            "epoch 15 batch 98 loss 0.03696036711335182\n",
            "epoch 15 batch 99 loss 0.040316030383110046\n",
            "epoch 15 batch 100 loss 0.04248688369989395\n",
            "epoch 15 batch 101 loss 0.034619513899087906\n",
            "epoch 15 batch 102 loss 0.03701052442193031\n",
            "epoch 15 batch 103 loss 0.04009607434272766\n",
            "epoch 15 batch 104 loss 0.03598721697926521\n",
            "epoch 15 batch 105 loss 0.03472292050719261\n",
            "epoch 15 batch 106 loss 0.03509412705898285\n",
            "epoch 15 batch 107 loss 0.03638569265604019\n",
            "epoch 15 batch 108 loss 0.038387980312108994\n",
            "epoch 15 batch 109 loss 0.039292629808187485\n",
            "epoch 15 batch 110 loss 0.03283948823809624\n",
            "epoch 15 batch 111 loss 0.03638811036944389\n",
            "epoch 15 batch 112 loss 0.041956476867198944\n",
            "epoch 15 batch 113 loss 0.03927288204431534\n",
            "epoch 15 batch 114 loss 0.03745265305042267\n",
            "epoch 15 batch 115 loss 0.039152149111032486\n",
            "epoch 15 batch 116 loss 0.038137633353471756\n",
            "epoch 15 batch 117 loss 0.041097067296504974\n",
            "epoch 15 batch 118 loss 0.03713983669877052\n",
            "epoch 15 batch 119 loss 0.0395071767270565\n",
            "epoch 15 batch 120 loss 0.036874085664749146\n",
            "epoch 15 batch 121 loss 0.03822298347949982\n",
            "epoch 15 batch 122 loss 0.03654041886329651\n",
            "epoch 15 batch 123 loss 0.03647623956203461\n",
            "epoch 15 batch 124 loss 0.040080226957798004\n",
            "epoch 15 batch 125 loss 0.03590547293424606\n",
            "epoch 15 batch 126 loss 0.032067738473415375\n",
            "epoch 15 batch 127 loss 0.042715754359960556\n",
            "epoch 15 batch 128 loss 0.03523113951086998\n",
            "epoch 15 batch 129 loss 0.03402514383196831\n",
            "epoch 15 batch 130 loss 0.04698263108730316\n",
            "epoch 15 batch 131 loss 0.03930696099996567\n",
            "epoch 15 batch 132 loss 0.040337175130844116\n",
            "epoch 15 batch 133 loss 0.03636964038014412\n",
            "epoch 15 batch 134 loss 0.03944186866283417\n",
            "epoch 15 batch 135 loss 0.04247656837105751\n",
            "epoch 15 batch 136 loss 0.038018740713596344\n",
            "epoch 15 batch 137 loss 0.03734684735536575\n",
            "epoch 15 batch 138 loss 0.037659551948308945\n",
            "epoch 15 batch 139 loss 0.03668106719851494\n",
            "epoch 15 batch 140 loss 0.04050521180033684\n",
            "epoch 15 batch 141 loss 0.03692793473601341\n",
            "epoch 15 batch 142 loss 0.03500664606690407\n",
            "epoch 15 batch 143 loss 0.030580513179302216\n",
            "epoch 15 batch 144 loss 0.035803113132715225\n",
            "epoch 15 batch 145 loss 0.03866066783666611\n",
            "epoch 15 batch 146 loss 0.04392983391880989\n",
            "epoch 15 batch 147 loss 0.041005756705999374\n",
            "epoch 15 batch 148 loss 0.037224724888801575\n",
            "epoch 15 batch 149 loss 0.03961707279086113\n",
            "epoch 15 batch 150 loss 0.03738308697938919\n",
            "epoch 15 batch 151 loss 0.03635944798588753\n",
            "epoch 15 batch 152 loss 0.03406113386154175\n",
            "epoch 15 batch 153 loss 0.038553621619939804\n",
            "epoch 15 batch 154 loss 0.03807017207145691\n",
            "epoch 15 batch 155 loss 0.038633085787296295\n",
            "epoch 15 batch 156 loss 0.04024890437722206\n",
            "epoch 15 batch 157 loss 0.0372815802693367\n",
            "epoch 15 batch 158 loss 0.034873224794864655\n",
            "epoch 15 batch 159 loss 0.03152192756533623\n",
            "epoch 15 batch 160 loss 0.03820088878273964\n",
            "epoch 15 batch 161 loss 0.036367375403642654\n",
            "epoch 15 batch 162 loss 0.041292738169431686\n",
            "epoch 15 batch 163 loss 0.04007411375641823\n",
            "epoch 15 batch 164 loss 0.03761966526508331\n",
            "epoch 15 batch 165 loss 0.0362255834043026\n",
            "epoch 15 batch 166 loss 0.039794351905584335\n",
            "epoch 15 batch 167 loss 0.03831116110086441\n",
            "epoch 15 batch 168 loss 0.04242211952805519\n",
            "epoch 15 batch 169 loss 0.03749040141701698\n",
            "epoch 15 batch 170 loss 0.03659512475132942\n",
            "epoch 15 batch 171 loss 0.039848264306783676\n",
            "epoch 15 batch 172 loss 0.03887079283595085\n",
            "epoch 15 batch 173 loss 0.03771983087062836\n",
            "epoch 15 batch 174 loss 0.038768280297517776\n",
            "epoch 15 batch 175 loss 0.04114237427711487\n",
            "epoch 15 batch 176 loss 0.037660304456949234\n",
            "epoch 15 batch 177 loss 0.03574764356017113\n",
            "epoch 15 batch 178 loss 0.03653162345290184\n",
            "epoch 15 batch 179 loss 0.034976113587617874\n",
            "epoch 15 batch 180 loss 0.037658218294382095\n",
            "epoch 15 batch 181 loss 0.03996962681412697\n",
            "epoch 15 batch 182 loss 0.037100911140441895\n",
            "epoch 15 batch 183 loss 0.038637880235910416\n",
            "epoch 15 batch 184 loss 0.046027448028326035\n",
            "epoch 15 batch 185 loss 0.038196396082639694\n",
            "epoch 15 batch 186 loss 0.04406061768531799\n",
            "epoch 15 batch 187 loss 0.034399211406707764\n",
            "epoch 15 batch 188 loss 0.03618636354804039\n",
            "epoch 15 batch 189 loss 0.042220354080200195\n",
            "epoch 15 batch 190 loss 0.03506488725543022\n",
            "epoch 15 batch 191 loss 0.03930789977312088\n",
            "epoch 15 batch 192 loss 0.03390239551663399\n",
            "epoch 15 batch 193 loss 0.03492435812950134\n",
            "epoch 15 batch 194 loss 0.03480876609683037\n",
            "epoch 15 batch 195 loss 0.03739171102643013\n",
            "epoch 15 batch 196 loss 0.03879985213279724\n",
            "epoch 15 batch 197 loss 0.03492992743849754\n",
            "epoch 15 batch 198 loss 0.035521406680345535\n",
            "epoch 15 batch 199 loss 0.04359260946512222\n",
            "epoch 15 batch 200 loss 0.041595134884119034\n",
            "epoch 15 batch 201 loss 0.04018736630678177\n",
            "epoch 15 batch 202 loss 0.03579774498939514\n",
            "epoch 15 batch 203 loss 0.03664472699165344\n",
            "epoch 15 batch 204 loss 0.032847486436367035\n",
            "epoch 15 batch 205 loss 0.04196733981370926\n",
            "epoch 15 batch 206 loss 0.031015830114483833\n",
            "epoch 15 batch 207 loss 0.03590725362300873\n",
            "epoch 15 batch 208 loss 0.04081147164106369\n",
            "epoch 15 batch 209 loss 0.038327280431985855\n",
            "epoch 15 batch 210 loss 0.04354440048336983\n",
            "epoch 15 batch 211 loss 0.04042258486151695\n",
            "epoch 15 batch 212 loss 0.036298565566539764\n",
            "epoch 15 batch 213 loss 0.036298152059316635\n",
            "epoch 15 batch 214 loss 0.03799615800380707\n",
            "epoch 15 batch 215 loss 0.033820804208517075\n",
            "epoch 15 batch 216 loss 0.036410022526979446\n",
            "epoch 15 batch 217 loss 0.030982602387666702\n",
            "epoch 15 batch 218 loss 0.037794265896081924\n",
            "epoch 15 batch 219 loss 0.03948754072189331\n",
            "epoch 15 batch 220 loss 0.037903331220149994\n",
            "epoch 15 batch 221 loss 0.03808407858014107\n",
            "epoch 15 batch 222 loss 0.03735798969864845\n",
            "epoch 15 batch 223 loss 0.03955511003732681\n",
            "epoch 15 batch 224 loss 0.03941250219941139\n",
            "epoch 15 batch 225 loss 0.03837534412741661\n",
            "epoch 15 batch 226 loss 0.03749105706810951\n",
            "epoch 15 batch 227 loss 0.03793424367904663\n",
            "epoch 15 batch 228 loss 0.03854651749134064\n",
            "epoch 15 batch 229 loss 0.037488751113414764\n",
            "epoch 15 batch 230 loss 0.0219291839748621\n",
            "epoch 16 batch 0 loss 0.040759600698947906\n",
            "epoch 16 batch 1 loss 0.038522686809301376\n",
            "epoch 16 batch 2 loss 0.043478842824697495\n",
            "epoch 16 batch 3 loss 0.039031803607940674\n",
            "epoch 16 batch 4 loss 0.039380498230457306\n",
            "epoch 16 batch 5 loss 0.03714528679847717\n",
            "epoch 16 batch 6 loss 0.0384678989648819\n",
            "epoch 16 batch 7 loss 0.0441250316798687\n",
            "epoch 16 batch 8 loss 0.03803281486034393\n",
            "epoch 16 batch 9 loss 0.038763873279094696\n",
            "epoch 16 batch 10 loss 0.04311148449778557\n",
            "epoch 16 batch 11 loss 0.03646348416805267\n",
            "epoch 16 batch 12 loss 0.03584636375308037\n",
            "epoch 16 batch 13 loss 0.042632464319467545\n",
            "epoch 16 batch 14 loss 0.03533885255455971\n",
            "epoch 16 batch 15 loss 0.03926781192421913\n",
            "epoch 16 batch 16 loss 0.03656100481748581\n",
            "epoch 16 batch 17 loss 0.03820912167429924\n",
            "epoch 16 batch 18 loss 0.036183495074510574\n",
            "epoch 16 batch 19 loss 0.03916867822408676\n",
            "epoch 16 batch 20 loss 0.037611424922943115\n",
            "epoch 16 batch 21 loss 0.03765750303864479\n",
            "epoch 16 batch 22 loss 0.03311499208211899\n",
            "epoch 16 batch 23 loss 0.03885502740740776\n",
            "epoch 16 batch 24 loss 0.03626954182982445\n",
            "epoch 16 batch 25 loss 0.04638142138719559\n",
            "epoch 16 batch 26 loss 0.0374421589076519\n",
            "epoch 16 batch 27 loss 0.04140782356262207\n",
            "epoch 16 batch 28 loss 0.03464368358254433\n",
            "epoch 16 batch 29 loss 0.03797343000769615\n",
            "epoch 16 batch 30 loss 0.038941122591495514\n",
            "epoch 16 batch 31 loss 0.037444572895765305\n",
            "epoch 16 batch 32 loss 0.032150477170944214\n",
            "epoch 16 batch 33 loss 0.03598364442586899\n",
            "epoch 16 batch 34 loss 0.0340946801006794\n",
            "epoch 16 batch 35 loss 0.036047257483005524\n",
            "epoch 16 batch 36 loss 0.04550568759441376\n",
            "epoch 16 batch 37 loss 0.03950891271233559\n",
            "epoch 16 batch 38 loss 0.03568184748291969\n",
            "epoch 16 batch 39 loss 0.03469892963767052\n",
            "epoch 16 batch 40 loss 0.03754102811217308\n",
            "epoch 16 batch 41 loss 0.03749930113554001\n",
            "epoch 16 batch 42 loss 0.035257767885923386\n",
            "epoch 16 batch 43 loss 0.04048428311944008\n",
            "epoch 16 batch 44 loss 0.041207876056432724\n",
            "epoch 16 batch 45 loss 0.03581928834319115\n",
            "epoch 16 batch 46 loss 0.03594109043478966\n",
            "epoch 16 batch 47 loss 0.03877125307917595\n",
            "epoch 16 batch 48 loss 0.03385787084698677\n",
            "epoch 16 batch 49 loss 0.03607536107301712\n",
            "epoch 16 batch 50 loss 0.03977907449007034\n",
            "epoch 16 batch 51 loss 0.03994416818022728\n",
            "epoch 16 batch 52 loss 0.03639213368296623\n",
            "epoch 16 batch 53 loss 0.03384499251842499\n",
            "epoch 16 batch 54 loss 0.03739020228385925\n",
            "epoch 16 batch 55 loss 0.036927200853824615\n",
            "epoch 16 batch 56 loss 0.03516252338886261\n",
            "epoch 16 batch 57 loss 0.04182245582342148\n",
            "epoch 16 batch 58 loss 0.03880557790398598\n",
            "epoch 16 batch 59 loss 0.03675674647092819\n",
            "epoch 16 batch 60 loss 0.037388626486063004\n",
            "epoch 16 batch 61 loss 0.03616298735141754\n",
            "epoch 16 batch 62 loss 0.035953253507614136\n",
            "epoch 16 batch 63 loss 0.03740302845835686\n",
            "epoch 16 batch 64 loss 0.04078982397913933\n",
            "epoch 16 batch 65 loss 0.035840798169374466\n",
            "epoch 16 batch 66 loss 0.03926956653594971\n",
            "epoch 16 batch 67 loss 0.042653147131204605\n",
            "epoch 16 batch 68 loss 0.037594884634017944\n",
            "epoch 16 batch 69 loss 0.0388980507850647\n",
            "epoch 16 batch 70 loss 0.03896447271108627\n",
            "epoch 16 batch 71 loss 0.03313424065709114\n",
            "epoch 16 batch 72 loss 0.036358173936605453\n",
            "epoch 16 batch 73 loss 0.04000794515013695\n",
            "epoch 16 batch 74 loss 0.03621538355946541\n",
            "epoch 16 batch 75 loss 0.03980332240462303\n",
            "epoch 16 batch 76 loss 0.03161850944161415\n",
            "epoch 16 batch 77 loss 0.03831947594881058\n",
            "epoch 16 batch 78 loss 0.03664051368832588\n",
            "epoch 16 batch 79 loss 0.03450879827141762\n",
            "epoch 16 batch 80 loss 0.03708667308092117\n",
            "epoch 16 batch 81 loss 0.03598771616816521\n",
            "epoch 16 batch 82 loss 0.0393366701900959\n",
            "epoch 16 batch 83 loss 0.03312881290912628\n",
            "epoch 16 batch 84 loss 0.03454487398266792\n",
            "epoch 16 batch 85 loss 0.038067031651735306\n",
            "epoch 16 batch 86 loss 0.031834956258535385\n",
            "epoch 16 batch 87 loss 0.035762179642915726\n",
            "epoch 16 batch 88 loss 0.034426335245370865\n",
            "epoch 16 batch 89 loss 0.03976961597800255\n",
            "epoch 16 batch 90 loss 0.034525420516729355\n",
            "epoch 16 batch 91 loss 0.037577252835035324\n",
            "epoch 16 batch 92 loss 0.03676650673151016\n",
            "epoch 16 batch 93 loss 0.03809775412082672\n",
            "epoch 16 batch 94 loss 0.03646545484662056\n",
            "epoch 16 batch 95 loss 0.039991896599531174\n",
            "epoch 16 batch 96 loss 0.03584462031722069\n",
            "epoch 16 batch 97 loss 0.037566863000392914\n",
            "epoch 16 batch 98 loss 0.03656560182571411\n",
            "epoch 16 batch 99 loss 0.03976254537701607\n",
            "epoch 16 batch 100 loss 0.04167769476771355\n",
            "epoch 16 batch 101 loss 0.034143175929784775\n",
            "epoch 16 batch 102 loss 0.03642940893769264\n",
            "epoch 16 batch 103 loss 0.03802790865302086\n",
            "epoch 16 batch 104 loss 0.03556584566831589\n",
            "epoch 16 batch 105 loss 0.0336390919983387\n",
            "epoch 16 batch 106 loss 0.03481512516736984\n",
            "epoch 16 batch 107 loss 0.036449775099754333\n",
            "epoch 16 batch 108 loss 0.037553928792476654\n",
            "epoch 16 batch 109 loss 0.0394764319062233\n",
            "epoch 16 batch 110 loss 0.03458744287490845\n",
            "epoch 16 batch 111 loss 0.03612435236573219\n",
            "epoch 16 batch 112 loss 0.040247268974781036\n",
            "epoch 16 batch 113 loss 0.03797096386551857\n",
            "epoch 16 batch 114 loss 0.03575662896037102\n",
            "epoch 16 batch 115 loss 0.03823342174291611\n",
            "epoch 16 batch 116 loss 0.03847533091902733\n",
            "epoch 16 batch 117 loss 0.03977629542350769\n",
            "epoch 16 batch 118 loss 0.03734666109085083\n",
            "epoch 16 batch 119 loss 0.03777403011918068\n",
            "epoch 16 batch 120 loss 0.0356462262570858\n",
            "epoch 16 batch 121 loss 0.03836748003959656\n",
            "epoch 16 batch 122 loss 0.03699532151222229\n",
            "epoch 16 batch 123 loss 0.034599389880895615\n",
            "epoch 16 batch 124 loss 0.03948942944407463\n",
            "epoch 16 batch 125 loss 0.03527029976248741\n",
            "epoch 16 batch 126 loss 0.03207874670624733\n",
            "epoch 16 batch 127 loss 0.04098782315850258\n",
            "epoch 16 batch 128 loss 0.03626520186662674\n",
            "epoch 16 batch 129 loss 0.03413965180516243\n",
            "epoch 16 batch 130 loss 0.04690954461693764\n",
            "epoch 16 batch 131 loss 0.03937755897641182\n",
            "epoch 16 batch 132 loss 0.03987980633974075\n",
            "epoch 16 batch 133 loss 0.03551860898733139\n",
            "epoch 16 batch 134 loss 0.04012134671211243\n",
            "epoch 16 batch 135 loss 0.04191480204463005\n",
            "epoch 16 batch 136 loss 0.03680316358804703\n",
            "epoch 16 batch 137 loss 0.03737669438123703\n",
            "epoch 16 batch 138 loss 0.038515426218509674\n",
            "epoch 16 batch 139 loss 0.03665909171104431\n",
            "epoch 16 batch 140 loss 0.04065035656094551\n",
            "epoch 16 batch 141 loss 0.037680983543395996\n",
            "epoch 16 batch 142 loss 0.03488655760884285\n",
            "epoch 16 batch 143 loss 0.03139569237828255\n",
            "epoch 16 batch 144 loss 0.035446226596832275\n",
            "epoch 16 batch 145 loss 0.03854683041572571\n",
            "epoch 16 batch 146 loss 0.04362386837601662\n",
            "epoch 16 batch 147 loss 0.041755981743335724\n",
            "epoch 16 batch 148 loss 0.03935259208083153\n",
            "epoch 16 batch 149 loss 0.038258619606494904\n",
            "epoch 16 batch 150 loss 0.03611687198281288\n",
            "epoch 16 batch 151 loss 0.037089359015226364\n",
            "epoch 16 batch 152 loss 0.0341169536113739\n",
            "epoch 16 batch 153 loss 0.03769567236304283\n",
            "epoch 16 batch 154 loss 0.03836378827691078\n",
            "epoch 16 batch 155 loss 0.03770250454545021\n",
            "epoch 16 batch 156 loss 0.03958456218242645\n",
            "epoch 16 batch 157 loss 0.037371065467596054\n",
            "epoch 16 batch 158 loss 0.03489004075527191\n",
            "epoch 16 batch 159 loss 0.03084925003349781\n",
            "epoch 16 batch 160 loss 0.036921169608831406\n",
            "epoch 16 batch 161 loss 0.03510018438100815\n",
            "epoch 16 batch 162 loss 0.04030095413327217\n",
            "epoch 16 batch 163 loss 0.037566497921943665\n",
            "epoch 16 batch 164 loss 0.035835884511470795\n",
            "epoch 16 batch 165 loss 0.0364493653178215\n",
            "epoch 16 batch 166 loss 0.03860050067305565\n",
            "epoch 16 batch 167 loss 0.03611313924193382\n",
            "epoch 16 batch 168 loss 0.041422706097364426\n",
            "epoch 16 batch 169 loss 0.036374252289533615\n",
            "epoch 16 batch 170 loss 0.03634116053581238\n",
            "epoch 16 batch 171 loss 0.04043460264801979\n",
            "epoch 16 batch 172 loss 0.039770785719156265\n",
            "epoch 16 batch 173 loss 0.03622927516698837\n",
            "epoch 16 batch 174 loss 0.037548940628767014\n",
            "epoch 16 batch 175 loss 0.044279251247644424\n",
            "epoch 16 batch 176 loss 0.03780723735690117\n",
            "epoch 16 batch 177 loss 0.03479703515768051\n",
            "epoch 16 batch 178 loss 0.03688680753111839\n",
            "epoch 16 batch 179 loss 0.03602396696805954\n",
            "epoch 16 batch 180 loss 0.03694044053554535\n",
            "epoch 16 batch 181 loss 0.03922654315829277\n",
            "epoch 16 batch 182 loss 0.03928075358271599\n",
            "epoch 16 batch 183 loss 0.03952544182538986\n",
            "epoch 16 batch 184 loss 0.04513627663254738\n",
            "epoch 16 batch 185 loss 0.039818380028009415\n",
            "epoch 16 batch 186 loss 0.043645091354846954\n",
            "epoch 16 batch 187 loss 0.031585875898599625\n",
            "epoch 16 batch 188 loss 0.03547395020723343\n",
            "epoch 16 batch 189 loss 0.04285426810383797\n",
            "epoch 16 batch 190 loss 0.0356425978243351\n",
            "epoch 16 batch 191 loss 0.039082448929548264\n",
            "epoch 16 batch 192 loss 0.03422490134835243\n",
            "epoch 16 batch 193 loss 0.03388926014304161\n",
            "epoch 16 batch 194 loss 0.03443295881152153\n",
            "epoch 16 batch 195 loss 0.03619522228837013\n",
            "epoch 16 batch 196 loss 0.03941332548856735\n",
            "epoch 16 batch 197 loss 0.0348321907222271\n",
            "epoch 16 batch 198 loss 0.034623805433511734\n",
            "epoch 16 batch 199 loss 0.04342575743794441\n",
            "epoch 16 batch 200 loss 0.04151621460914612\n",
            "epoch 16 batch 201 loss 0.037669699639081955\n",
            "epoch 16 batch 202 loss 0.03546856716275215\n",
            "epoch 16 batch 203 loss 0.03651856631040573\n",
            "epoch 16 batch 204 loss 0.031957946717739105\n",
            "epoch 16 batch 205 loss 0.04132222384214401\n",
            "epoch 16 batch 206 loss 0.03019266575574875\n",
            "epoch 16 batch 207 loss 0.03426360711455345\n",
            "epoch 16 batch 208 loss 0.04007241129875183\n",
            "epoch 16 batch 209 loss 0.0363243967294693\n",
            "epoch 16 batch 210 loss 0.043941088020801544\n",
            "epoch 16 batch 211 loss 0.03940802067518234\n",
            "epoch 16 batch 212 loss 0.03653011471033096\n",
            "epoch 16 batch 213 loss 0.03536702319979668\n",
            "epoch 16 batch 214 loss 0.03757331520318985\n",
            "epoch 16 batch 215 loss 0.03206568583846092\n",
            "epoch 16 batch 216 loss 0.03507912904024124\n",
            "epoch 16 batch 217 loss 0.030408548191189766\n",
            "epoch 16 batch 218 loss 0.03751246631145477\n",
            "epoch 16 batch 219 loss 0.038833606988191605\n",
            "epoch 16 batch 220 loss 0.03687472641468048\n",
            "epoch 16 batch 221 loss 0.037459369748830795\n",
            "epoch 16 batch 222 loss 0.037019480019807816\n",
            "epoch 16 batch 223 loss 0.03968958184123039\n",
            "epoch 16 batch 224 loss 0.03860458731651306\n",
            "epoch 16 batch 225 loss 0.03773975372314453\n",
            "epoch 16 batch 226 loss 0.0380622036755085\n",
            "epoch 16 batch 227 loss 0.036762725561857224\n",
            "epoch 16 batch 228 loss 0.0371444933116436\n",
            "epoch 16 batch 229 loss 0.03634851425886154\n",
            "epoch 16 batch 230 loss 0.02247951366007328\n",
            "epoch 17 batch 0 loss 0.03879464790225029\n",
            "epoch 17 batch 1 loss 0.038947105407714844\n",
            "epoch 17 batch 2 loss 0.043500080704689026\n",
            "epoch 17 batch 3 loss 0.03925994038581848\n",
            "epoch 17 batch 4 loss 0.03886265680193901\n",
            "epoch 17 batch 5 loss 0.03678903728723526\n",
            "epoch 17 batch 6 loss 0.03763522952795029\n",
            "epoch 17 batch 7 loss 0.04320518672466278\n",
            "epoch 17 batch 8 loss 0.038411203771829605\n",
            "epoch 17 batch 9 loss 0.03910441696643829\n",
            "epoch 17 batch 10 loss 0.04365982115268707\n",
            "epoch 17 batch 11 loss 0.036227330565452576\n",
            "epoch 17 batch 12 loss 0.03534688428044319\n",
            "epoch 17 batch 13 loss 0.042974259704351425\n",
            "epoch 17 batch 14 loss 0.035750776529312134\n",
            "epoch 17 batch 15 loss 0.03989430144429207\n",
            "epoch 17 batch 16 loss 0.03641539439558983\n",
            "epoch 17 batch 17 loss 0.03766804561018944\n",
            "epoch 17 batch 18 loss 0.037083081901073456\n",
            "epoch 17 batch 19 loss 0.03893079236149788\n",
            "epoch 17 batch 20 loss 0.0384233184158802\n",
            "epoch 17 batch 21 loss 0.037440042942762375\n",
            "epoch 17 batch 22 loss 0.03330601006746292\n",
            "epoch 17 batch 23 loss 0.03934221714735031\n",
            "epoch 17 batch 24 loss 0.03647446259856224\n",
            "epoch 17 batch 25 loss 0.04508455842733383\n",
            "epoch 17 batch 26 loss 0.037187959998846054\n",
            "epoch 17 batch 27 loss 0.04114893823862076\n",
            "epoch 17 batch 28 loss 0.03431089222431183\n",
            "epoch 17 batch 29 loss 0.03668193891644478\n",
            "epoch 17 batch 30 loss 0.03809824585914612\n",
            "epoch 17 batch 31 loss 0.03897786885499954\n",
            "epoch 17 batch 32 loss 0.03194158151745796\n",
            "epoch 17 batch 33 loss 0.035392701625823975\n",
            "epoch 17 batch 34 loss 0.03428906202316284\n",
            "epoch 17 batch 35 loss 0.03546362742781639\n",
            "epoch 17 batch 36 loss 0.04389423504471779\n",
            "epoch 17 batch 37 loss 0.0394478403031826\n",
            "epoch 17 batch 38 loss 0.036557648330926895\n",
            "epoch 17 batch 39 loss 0.03543825447559357\n",
            "epoch 17 batch 40 loss 0.03700572997331619\n",
            "epoch 17 batch 41 loss 0.03687519580125809\n",
            "epoch 17 batch 42 loss 0.03586350381374359\n",
            "epoch 17 batch 43 loss 0.040177155286073685\n",
            "epoch 17 batch 44 loss 0.04000968486070633\n",
            "epoch 17 batch 45 loss 0.035578854382038116\n",
            "epoch 17 batch 46 loss 0.03623822703957558\n",
            "epoch 17 batch 47 loss 0.03794935718178749\n",
            "epoch 17 batch 48 loss 0.033561207354068756\n",
            "epoch 17 batch 49 loss 0.036709580570459366\n",
            "epoch 17 batch 50 loss 0.03857456147670746\n",
            "epoch 17 batch 51 loss 0.039262816309928894\n",
            "epoch 17 batch 52 loss 0.03748409077525139\n",
            "epoch 17 batch 53 loss 0.032453183084726334\n",
            "epoch 17 batch 54 loss 0.03608890250325203\n",
            "epoch 17 batch 55 loss 0.03480790928006172\n",
            "epoch 17 batch 56 loss 0.034519538283348083\n",
            "epoch 17 batch 57 loss 0.041131339967250824\n",
            "epoch 17 batch 58 loss 0.037067919969558716\n",
            "epoch 17 batch 59 loss 0.0358436219394207\n",
            "epoch 17 batch 60 loss 0.03850475698709488\n",
            "epoch 17 batch 61 loss 0.03708229586482048\n",
            "epoch 17 batch 62 loss 0.03538178652524948\n",
            "epoch 17 batch 63 loss 0.036736175417900085\n",
            "epoch 17 batch 64 loss 0.04130275547504425\n",
            "epoch 17 batch 65 loss 0.03512205556035042\n",
            "epoch 17 batch 66 loss 0.03773059695959091\n",
            "epoch 17 batch 67 loss 0.042965006083250046\n",
            "epoch 17 batch 68 loss 0.03819941729307175\n",
            "epoch 17 batch 69 loss 0.03773632273077965\n",
            "epoch 17 batch 70 loss 0.038277868181467056\n",
            "epoch 17 batch 71 loss 0.03340381011366844\n",
            "epoch 17 batch 72 loss 0.036610569804906845\n",
            "epoch 17 batch 73 loss 0.03907444328069687\n",
            "epoch 17 batch 74 loss 0.036223988980054855\n",
            "epoch 17 batch 75 loss 0.03896912559866905\n",
            "epoch 17 batch 76 loss 0.031512100249528885\n",
            "epoch 17 batch 77 loss 0.03683166950941086\n",
            "epoch 17 batch 78 loss 0.03612474724650383\n",
            "epoch 17 batch 79 loss 0.03363288193941116\n",
            "epoch 17 batch 80 loss 0.03673643246293068\n",
            "epoch 17 batch 81 loss 0.03569479286670685\n",
            "epoch 17 batch 82 loss 0.03874783590435982\n",
            "epoch 17 batch 83 loss 0.0340646468102932\n",
            "epoch 17 batch 84 loss 0.03379974141716957\n",
            "epoch 17 batch 85 loss 0.03829043731093407\n",
            "epoch 17 batch 86 loss 0.03177708014845848\n",
            "epoch 17 batch 87 loss 0.03470901399850845\n",
            "epoch 17 batch 88 loss 0.03417825326323509\n",
            "epoch 17 batch 89 loss 0.03909780830144882\n",
            "epoch 17 batch 90 loss 0.03466210514307022\n",
            "epoch 17 batch 91 loss 0.03662234917283058\n",
            "epoch 17 batch 92 loss 0.03551463410258293\n",
            "epoch 17 batch 93 loss 0.03840970993041992\n",
            "epoch 17 batch 94 loss 0.036106664687395096\n",
            "epoch 17 batch 95 loss 0.039474908262491226\n",
            "epoch 17 batch 96 loss 0.034578192979097366\n",
            "epoch 17 batch 97 loss 0.03636879846453667\n",
            "epoch 17 batch 98 loss 0.03583746403455734\n",
            "epoch 17 batch 99 loss 0.03780607506632805\n",
            "epoch 17 batch 100 loss 0.04057485610246658\n",
            "epoch 17 batch 101 loss 0.032940689474344254\n",
            "epoch 17 batch 102 loss 0.03566042706370354\n",
            "epoch 17 batch 103 loss 0.03857230767607689\n",
            "epoch 17 batch 104 loss 0.03488310053944588\n",
            "epoch 17 batch 105 loss 0.0324719212949276\n",
            "epoch 17 batch 106 loss 0.03482908383011818\n",
            "epoch 17 batch 107 loss 0.03528745844960213\n",
            "epoch 17 batch 108 loss 0.037113841623067856\n",
            "epoch 17 batch 109 loss 0.037121452391147614\n",
            "epoch 17 batch 110 loss 0.03194242715835571\n",
            "epoch 17 batch 111 loss 0.03759784623980522\n",
            "epoch 17 batch 112 loss 0.04059474915266037\n",
            "epoch 17 batch 113 loss 0.035890866070985794\n",
            "epoch 17 batch 114 loss 0.03500901162624359\n",
            "epoch 17 batch 115 loss 0.03748258575797081\n",
            "epoch 17 batch 116 loss 0.03908652439713478\n",
            "epoch 17 batch 117 loss 0.03846209868788719\n",
            "epoch 17 batch 118 loss 0.03576764464378357\n",
            "epoch 17 batch 119 loss 0.03894706070423126\n",
            "epoch 17 batch 120 loss 0.03564084321260452\n",
            "epoch 17 batch 121 loss 0.03673190996050835\n",
            "epoch 17 batch 122 loss 0.03555845841765404\n",
            "epoch 17 batch 123 loss 0.03487825021147728\n",
            "epoch 17 batch 124 loss 0.0386793315410614\n",
            "epoch 17 batch 125 loss 0.03486005216836929\n",
            "epoch 17 batch 126 loss 0.03032999485731125\n",
            "epoch 17 batch 127 loss 0.04019103944301605\n",
            "epoch 17 batch 128 loss 0.035432592034339905\n",
            "epoch 17 batch 129 loss 0.03306073322892189\n",
            "epoch 17 batch 130 loss 0.04558846354484558\n",
            "epoch 17 batch 131 loss 0.039629291743040085\n",
            "epoch 17 batch 132 loss 0.038574621081352234\n",
            "epoch 17 batch 133 loss 0.03420798480510712\n",
            "epoch 17 batch 134 loss 0.0388234443962574\n",
            "epoch 17 batch 135 loss 0.04147415980696678\n",
            "epoch 17 batch 136 loss 0.03615190461277962\n",
            "epoch 17 batch 137 loss 0.03641994670033455\n",
            "epoch 17 batch 138 loss 0.03806426748633385\n",
            "epoch 17 batch 139 loss 0.035098880529403687\n",
            "epoch 17 batch 140 loss 0.037507064640522\n",
            "epoch 17 batch 141 loss 0.03607594221830368\n",
            "epoch 17 batch 142 loss 0.034417759627103806\n",
            "epoch 17 batch 143 loss 0.030631769448518753\n",
            "epoch 17 batch 144 loss 0.03490554168820381\n",
            "epoch 17 batch 145 loss 0.03806798905134201\n",
            "epoch 17 batch 146 loss 0.04237479344010353\n",
            "epoch 17 batch 147 loss 0.03956872597336769\n",
            "epoch 17 batch 148 loss 0.03763582557439804\n",
            "epoch 17 batch 149 loss 0.038232650607824326\n",
            "epoch 17 batch 150 loss 0.03727739304304123\n",
            "epoch 17 batch 151 loss 0.035730667412281036\n",
            "epoch 17 batch 152 loss 0.033993903547525406\n",
            "epoch 17 batch 153 loss 0.03864959999918938\n",
            "epoch 17 batch 154 loss 0.03678639233112335\n",
            "epoch 17 batch 155 loss 0.03662083297967911\n",
            "epoch 17 batch 156 loss 0.03992913290858269\n",
            "epoch 17 batch 157 loss 0.035721782594919205\n",
            "epoch 17 batch 158 loss 0.03508422523736954\n",
            "epoch 17 batch 159 loss 0.030825935304164886\n",
            "epoch 17 batch 160 loss 0.037185173481702805\n",
            "epoch 17 batch 161 loss 0.03506150096654892\n",
            "epoch 17 batch 162 loss 0.0410381555557251\n",
            "epoch 17 batch 163 loss 0.03783130645751953\n",
            "epoch 17 batch 164 loss 0.03666221722960472\n",
            "epoch 17 batch 165 loss 0.03520684689283371\n",
            "epoch 17 batch 166 loss 0.03862779214978218\n",
            "epoch 17 batch 167 loss 0.03529456630349159\n",
            "epoch 17 batch 168 loss 0.04126855358481407\n",
            "epoch 17 batch 169 loss 0.03582962974905968\n",
            "epoch 17 batch 170 loss 0.035304613411426544\n",
            "epoch 17 batch 171 loss 0.03755156323313713\n",
            "epoch 17 batch 172 loss 0.039312511682510376\n",
            "epoch 17 batch 173 loss 0.03593773394823074\n",
            "epoch 17 batch 174 loss 0.03524834290146828\n",
            "epoch 17 batch 175 loss 0.04031824320554733\n",
            "epoch 17 batch 176 loss 0.03623027354478836\n",
            "epoch 17 batch 177 loss 0.033563271164894104\n",
            "epoch 17 batch 178 loss 0.034092243760824203\n",
            "epoch 17 batch 179 loss 0.035009123384952545\n",
            "epoch 17 batch 180 loss 0.03811722993850708\n",
            "epoch 17 batch 181 loss 0.037882428616285324\n",
            "epoch 17 batch 182 loss 0.03797224164009094\n",
            "epoch 17 batch 183 loss 0.04010474681854248\n",
            "epoch 17 batch 184 loss 0.04452142491936684\n",
            "epoch 17 batch 185 loss 0.03731635585427284\n",
            "epoch 17 batch 186 loss 0.04392450302839279\n",
            "epoch 17 batch 187 loss 0.03291908651590347\n",
            "epoch 17 batch 188 loss 0.03456347435712814\n",
            "epoch 17 batch 189 loss 0.04113873094320297\n",
            "epoch 17 batch 190 loss 0.033805277198553085\n",
            "epoch 17 batch 191 loss 0.04017120227217674\n",
            "epoch 17 batch 192 loss 0.032899219542741776\n",
            "epoch 17 batch 193 loss 0.0327754020690918\n",
            "epoch 17 batch 194 loss 0.0345049723982811\n",
            "epoch 17 batch 195 loss 0.036071088165044785\n",
            "epoch 17 batch 196 loss 0.037797097116708755\n",
            "epoch 17 batch 197 loss 0.03373343497514725\n",
            "epoch 17 batch 198 loss 0.033842429518699646\n",
            "epoch 17 batch 199 loss 0.04189837723970413\n",
            "epoch 17 batch 200 loss 0.040304239839315414\n",
            "epoch 17 batch 201 loss 0.037294816225767136\n",
            "epoch 17 batch 202 loss 0.035074036568403244\n",
            "epoch 17 batch 203 loss 0.03470982611179352\n",
            "epoch 17 batch 204 loss 0.030165694653987885\n",
            "epoch 17 batch 205 loss 0.03978487849235535\n",
            "epoch 17 batch 206 loss 0.029758740216493607\n",
            "epoch 17 batch 207 loss 0.03312378004193306\n",
            "epoch 17 batch 208 loss 0.0395045168697834\n",
            "epoch 17 batch 209 loss 0.03530186414718628\n",
            "epoch 17 batch 210 loss 0.04252546280622482\n",
            "epoch 17 batch 211 loss 0.03737824037671089\n",
            "epoch 17 batch 212 loss 0.03524000570178032\n",
            "epoch 17 batch 213 loss 0.03611086308956146\n",
            "epoch 17 batch 214 loss 0.035682715475559235\n",
            "epoch 17 batch 215 loss 0.032605431973934174\n",
            "epoch 17 batch 216 loss 0.03306829556822777\n",
            "epoch 17 batch 217 loss 0.030108559876680374\n",
            "epoch 17 batch 218 loss 0.03629793971776962\n",
            "epoch 17 batch 219 loss 0.036821287125349045\n",
            "epoch 17 batch 220 loss 0.0351879820227623\n",
            "epoch 17 batch 221 loss 0.035089217126369476\n",
            "epoch 17 batch 222 loss 0.034635186195373535\n",
            "epoch 17 batch 223 loss 0.037577830255031586\n",
            "epoch 17 batch 224 loss 0.037425216287374496\n",
            "epoch 17 batch 225 loss 0.0350305549800396\n",
            "epoch 17 batch 226 loss 0.03451407328248024\n",
            "epoch 17 batch 227 loss 0.036191828548908234\n",
            "epoch 17 batch 228 loss 0.035358380526304245\n",
            "epoch 17 batch 229 loss 0.033865977078676224\n",
            "epoch 17 batch 230 loss 0.020957253873348236\n",
            "epoch 18 batch 0 loss 0.037962451577186584\n",
            "epoch 18 batch 1 loss 0.03721898794174194\n",
            "epoch 18 batch 2 loss 0.04118964448571205\n",
            "epoch 18 batch 3 loss 0.03731190785765648\n",
            "epoch 18 batch 4 loss 0.036890916526317596\n",
            "epoch 18 batch 5 loss 0.03526189178228378\n",
            "epoch 18 batch 6 loss 0.0353391095995903\n",
            "epoch 18 batch 7 loss 0.04124420881271362\n",
            "epoch 18 batch 8 loss 0.03498325124382973\n",
            "epoch 18 batch 9 loss 0.038120999932289124\n",
            "epoch 18 batch 10 loss 0.0426204577088356\n",
            "epoch 18 batch 11 loss 0.03411981835961342\n",
            "epoch 18 batch 12 loss 0.033863961696624756\n",
            "epoch 18 batch 13 loss 0.04260129854083061\n",
            "epoch 18 batch 14 loss 0.03445922210812569\n",
            "epoch 18 batch 15 loss 0.03688737750053406\n",
            "epoch 18 batch 16 loss 0.03457963839173317\n",
            "epoch 18 batch 17 loss 0.03615669906139374\n",
            "epoch 18 batch 18 loss 0.03461151197552681\n",
            "epoch 18 batch 19 loss 0.03738565742969513\n",
            "epoch 18 batch 20 loss 0.03770912066102028\n",
            "epoch 18 batch 21 loss 0.035632092505693436\n",
            "epoch 18 batch 22 loss 0.031336694955825806\n",
            "epoch 18 batch 23 loss 0.03718946874141693\n",
            "epoch 18 batch 24 loss 0.036503490060567856\n",
            "epoch 18 batch 25 loss 0.04592441767454147\n",
            "epoch 18 batch 26 loss 0.035390112549066544\n",
            "epoch 18 batch 27 loss 0.039420973509550095\n",
            "epoch 18 batch 28 loss 0.0339985229074955\n",
            "epoch 18 batch 29 loss 0.03574738651514053\n",
            "epoch 18 batch 30 loss 0.035642385482788086\n",
            "epoch 18 batch 31 loss 0.037476472556591034\n",
            "epoch 18 batch 32 loss 0.03202865272760391\n",
            "epoch 18 batch 33 loss 0.03433169797062874\n",
            "epoch 18 batch 34 loss 0.032513346523046494\n",
            "epoch 18 batch 35 loss 0.0346585288643837\n",
            "epoch 18 batch 36 loss 0.043369174003601074\n",
            "epoch 18 batch 37 loss 0.038092952221632004\n",
            "epoch 18 batch 38 loss 0.03560721129179001\n",
            "epoch 18 batch 39 loss 0.03443082794547081\n",
            "epoch 18 batch 40 loss 0.03687085211277008\n",
            "epoch 18 batch 41 loss 0.03515547513961792\n",
            "epoch 18 batch 42 loss 0.034930795431137085\n",
            "epoch 18 batch 43 loss 0.03950430080294609\n",
            "epoch 18 batch 44 loss 0.038648560643196106\n",
            "epoch 18 batch 45 loss 0.03405372053384781\n",
            "epoch 18 batch 46 loss 0.035438813269138336\n",
            "epoch 18 batch 47 loss 0.03883466124534607\n",
            "epoch 18 batch 48 loss 0.03217384219169617\n",
            "epoch 18 batch 49 loss 0.03647786006331444\n",
            "epoch 18 batch 50 loss 0.03833076357841492\n",
            "epoch 18 batch 51 loss 0.03808243200182915\n",
            "epoch 18 batch 52 loss 0.03632683679461479\n",
            "epoch 18 batch 53 loss 0.032801032066345215\n",
            "epoch 18 batch 54 loss 0.036053553223609924\n",
            "epoch 18 batch 55 loss 0.0348292738199234\n",
            "epoch 18 batch 56 loss 0.03420815244317055\n",
            "epoch 18 batch 57 loss 0.04035880044102669\n",
            "epoch 18 batch 58 loss 0.03761672228574753\n",
            "epoch 18 batch 59 loss 0.03514258563518524\n",
            "epoch 18 batch 60 loss 0.03754338622093201\n",
            "epoch 18 batch 61 loss 0.038299381732940674\n",
            "epoch 18 batch 62 loss 0.03524671867489815\n",
            "epoch 18 batch 63 loss 0.03549996018409729\n",
            "epoch 18 batch 64 loss 0.04060635715723038\n",
            "epoch 18 batch 65 loss 0.03446144610643387\n",
            "epoch 18 batch 66 loss 0.037917133420705795\n",
            "epoch 18 batch 67 loss 0.041926369071006775\n",
            "epoch 18 batch 68 loss 0.03735460340976715\n",
            "epoch 18 batch 69 loss 0.037686508148908615\n",
            "epoch 18 batch 70 loss 0.03894370049238205\n",
            "epoch 18 batch 71 loss 0.0325968936085701\n",
            "epoch 18 batch 72 loss 0.03776686638593674\n",
            "epoch 18 batch 73 loss 0.03793098032474518\n",
            "epoch 18 batch 74 loss 0.03481494262814522\n",
            "epoch 18 batch 75 loss 0.038241006433963776\n",
            "epoch 18 batch 76 loss 0.03150291368365288\n",
            "epoch 18 batch 77 loss 0.036050278693437576\n",
            "epoch 18 batch 78 loss 0.03577028214931488\n",
            "epoch 18 batch 79 loss 0.03414948284626007\n",
            "epoch 18 batch 80 loss 0.037394437938928604\n",
            "epoch 18 batch 81 loss 0.034290216863155365\n",
            "epoch 18 batch 82 loss 0.037860628217458725\n",
            "epoch 18 batch 83 loss 0.034122515469789505\n",
            "epoch 18 batch 84 loss 0.035087235271930695\n",
            "epoch 18 batch 85 loss 0.038079869002103806\n",
            "epoch 18 batch 86 loss 0.032426800578832626\n",
            "epoch 18 batch 87 loss 0.03476972505450249\n",
            "epoch 18 batch 88 loss 0.03444945067167282\n",
            "epoch 18 batch 89 loss 0.03887182101607323\n",
            "epoch 18 batch 90 loss 0.03385362774133682\n",
            "epoch 18 batch 91 loss 0.037628620862960815\n",
            "epoch 18 batch 92 loss 0.03446042165160179\n",
            "epoch 18 batch 93 loss 0.036523886024951935\n",
            "epoch 18 batch 94 loss 0.03526667132973671\n",
            "epoch 18 batch 95 loss 0.03922848030924797\n",
            "epoch 18 batch 96 loss 0.03429413214325905\n",
            "epoch 18 batch 97 loss 0.03599833697080612\n",
            "epoch 18 batch 98 loss 0.0350266695022583\n",
            "epoch 18 batch 99 loss 0.037973057478666306\n",
            "epoch 18 batch 100 loss 0.03997638449072838\n",
            "epoch 18 batch 101 loss 0.03325524926185608\n",
            "epoch 18 batch 102 loss 0.034621868282556534\n",
            "epoch 18 batch 103 loss 0.037573590874671936\n",
            "epoch 18 batch 104 loss 0.03371068462729454\n",
            "epoch 18 batch 105 loss 0.03178490325808525\n",
            "epoch 18 batch 106 loss 0.03434082865715027\n",
            "epoch 18 batch 107 loss 0.03433367237448692\n",
            "epoch 18 batch 108 loss 0.0367412269115448\n",
            "epoch 18 batch 109 loss 0.03680228441953659\n",
            "epoch 18 batch 110 loss 0.031176527962088585\n",
            "epoch 18 batch 111 loss 0.03639132156968117\n",
            "epoch 18 batch 112 loss 0.041744720190763474\n",
            "epoch 18 batch 113 loss 0.03564545139670372\n",
            "epoch 18 batch 114 loss 0.03497058153152466\n",
            "epoch 18 batch 115 loss 0.03744609281420708\n",
            "epoch 18 batch 116 loss 0.03787201642990112\n",
            "epoch 18 batch 117 loss 0.03788847476243973\n",
            "epoch 18 batch 118 loss 0.03444169834256172\n",
            "epoch 18 batch 119 loss 0.0372253879904747\n",
            "epoch 18 batch 120 loss 0.03612373024225235\n",
            "epoch 18 batch 121 loss 0.036394406110048294\n",
            "epoch 18 batch 122 loss 0.035485513508319855\n",
            "epoch 18 batch 123 loss 0.03354499489068985\n",
            "epoch 18 batch 124 loss 0.03935291990637779\n",
            "epoch 18 batch 125 loss 0.03390340134501457\n",
            "epoch 18 batch 126 loss 0.02974409982562065\n",
            "epoch 18 batch 127 loss 0.03984399884939194\n",
            "epoch 18 batch 128 loss 0.034653667360544205\n",
            "epoch 18 batch 129 loss 0.03334749862551689\n",
            "epoch 18 batch 130 loss 0.04299145191907883\n",
            "epoch 18 batch 131 loss 0.03878409415483475\n",
            "epoch 18 batch 132 loss 0.03850441798567772\n",
            "epoch 18 batch 133 loss 0.03360535204410553\n",
            "epoch 18 batch 134 loss 0.03700403869152069\n",
            "epoch 18 batch 135 loss 0.040165938436985016\n",
            "epoch 18 batch 136 loss 0.03493102267384529\n",
            "epoch 18 batch 137 loss 0.03674451261758804\n",
            "epoch 18 batch 138 loss 0.03651841729879379\n",
            "epoch 18 batch 139 loss 0.035884588956832886\n",
            "epoch 18 batch 140 loss 0.03778780996799469\n",
            "epoch 18 batch 141 loss 0.03488600626587868\n",
            "epoch 18 batch 142 loss 0.03419024497270584\n",
            "epoch 18 batch 143 loss 0.03011520579457283\n",
            "epoch 18 batch 144 loss 0.033989470452070236\n",
            "epoch 18 batch 145 loss 0.03663220256567001\n",
            "epoch 18 batch 146 loss 0.041422586888074875\n",
            "epoch 18 batch 147 loss 0.038286641240119934\n",
            "epoch 18 batch 148 loss 0.03692284971475601\n",
            "epoch 18 batch 149 loss 0.03650107607245445\n",
            "epoch 18 batch 150 loss 0.03757966682314873\n",
            "epoch 18 batch 151 loss 0.03561554476618767\n",
            "epoch 18 batch 152 loss 0.03270700201392174\n",
            "epoch 18 batch 153 loss 0.03717905282974243\n",
            "epoch 18 batch 154 loss 0.03590862825512886\n",
            "epoch 18 batch 155 loss 0.0365019291639328\n",
            "epoch 18 batch 156 loss 0.0377681590616703\n",
            "epoch 18 batch 157 loss 0.03552616387605667\n",
            "epoch 18 batch 158 loss 0.03555678576231003\n",
            "epoch 18 batch 159 loss 0.03072921745479107\n",
            "epoch 18 batch 160 loss 0.03629150986671448\n",
            "epoch 18 batch 161 loss 0.03417157754302025\n",
            "epoch 18 batch 162 loss 0.04002106189727783\n",
            "epoch 18 batch 163 loss 0.03701552376151085\n",
            "epoch 18 batch 164 loss 0.036457620561122894\n",
            "epoch 18 batch 165 loss 0.03399293124675751\n",
            "epoch 18 batch 166 loss 0.039427220821380615\n",
            "epoch 18 batch 167 loss 0.03572697192430496\n",
            "epoch 18 batch 168 loss 0.040326543152332306\n",
            "epoch 18 batch 169 loss 0.0349191389977932\n",
            "epoch 18 batch 170 loss 0.034130677580833435\n",
            "epoch 18 batch 171 loss 0.03649067506194115\n",
            "epoch 18 batch 172 loss 0.03788964822888374\n",
            "epoch 18 batch 173 loss 0.034960661083459854\n",
            "epoch 18 batch 174 loss 0.03499054163694382\n",
            "epoch 18 batch 175 loss 0.03882521390914917\n",
            "epoch 18 batch 176 loss 0.03608657419681549\n",
            "epoch 18 batch 177 loss 0.032879579812288284\n",
            "epoch 18 batch 178 loss 0.033119719475507736\n",
            "epoch 18 batch 179 loss 0.03287573531270027\n",
            "epoch 18 batch 180 loss 0.03773963078856468\n",
            "epoch 18 batch 181 loss 0.037178654223680496\n",
            "epoch 18 batch 182 loss 0.03546961024403572\n",
            "epoch 18 batch 183 loss 0.03829261660575867\n",
            "epoch 18 batch 184 loss 0.04390104115009308\n",
            "epoch 18 batch 185 loss 0.0359048955142498\n",
            "epoch 18 batch 186 loss 0.04175794497132301\n",
            "epoch 18 batch 187 loss 0.03157506138086319\n",
            "epoch 18 batch 188 loss 0.03500938042998314\n",
            "epoch 18 batch 189 loss 0.04096096009016037\n",
            "epoch 18 batch 190 loss 0.03359831124544144\n",
            "epoch 18 batch 191 loss 0.038048554211854935\n",
            "epoch 18 batch 192 loss 0.03181515261530876\n",
            "epoch 18 batch 193 loss 0.03272123262286186\n",
            "epoch 18 batch 194 loss 0.03335277736186981\n",
            "epoch 18 batch 195 loss 0.03484130650758743\n",
            "epoch 18 batch 196 loss 0.03690023347735405\n",
            "epoch 18 batch 197 loss 0.03307913616299629\n",
            "epoch 18 batch 198 loss 0.032969195395708084\n",
            "epoch 18 batch 199 loss 0.04123883694410324\n",
            "epoch 18 batch 200 loss 0.03965333476662636\n",
            "epoch 18 batch 201 loss 0.036082569509744644\n",
            "epoch 18 batch 202 loss 0.034519534558057785\n",
            "epoch 18 batch 203 loss 0.03349381685256958\n",
            "epoch 18 batch 204 loss 0.03029155731201172\n",
            "epoch 18 batch 205 loss 0.03959720954298973\n",
            "epoch 18 batch 206 loss 0.029052650555968285\n",
            "epoch 18 batch 207 loss 0.03267458453774452\n",
            "epoch 18 batch 208 loss 0.03793296217918396\n",
            "epoch 18 batch 209 loss 0.03444503992795944\n",
            "epoch 18 batch 210 loss 0.040591198951005936\n",
            "epoch 18 batch 211 loss 0.035446055233478546\n",
            "epoch 18 batch 212 loss 0.03363587334752083\n",
            "epoch 18 batch 213 loss 0.03453358635306358\n",
            "epoch 18 batch 214 loss 0.03480369970202446\n",
            "epoch 18 batch 215 loss 0.0320977121591568\n",
            "epoch 18 batch 216 loss 0.03273360803723335\n",
            "epoch 18 batch 217 loss 0.028552668169140816\n",
            "epoch 18 batch 218 loss 0.034625232219696045\n",
            "epoch 18 batch 219 loss 0.03679272159934044\n",
            "epoch 18 batch 220 loss 0.03332554176449776\n",
            "epoch 18 batch 221 loss 0.03392236679792404\n",
            "epoch 18 batch 222 loss 0.03408295661211014\n",
            "epoch 18 batch 223 loss 0.03693697601556778\n",
            "epoch 18 batch 224 loss 0.035994336009025574\n",
            "epoch 18 batch 225 loss 0.03481508418917656\n",
            "epoch 18 batch 226 loss 0.034661177545785904\n",
            "epoch 18 batch 227 loss 0.035827334970235825\n",
            "epoch 18 batch 228 loss 0.03495102375745773\n",
            "epoch 18 batch 229 loss 0.03310537710785866\n",
            "epoch 18 batch 230 loss 0.01995125599205494\n",
            "epoch 19 batch 0 loss 0.037097442895174026\n",
            "epoch 19 batch 1 loss 0.03590099513530731\n",
            "epoch 19 batch 2 loss 0.039655935019254684\n",
            "epoch 19 batch 3 loss 0.03744126111268997\n",
            "epoch 19 batch 4 loss 0.03657327964901924\n",
            "epoch 19 batch 5 loss 0.03507957234978676\n",
            "epoch 19 batch 6 loss 0.034749630838632584\n",
            "epoch 19 batch 7 loss 0.04074513167142868\n",
            "epoch 19 batch 8 loss 0.0358646884560585\n",
            "epoch 19 batch 9 loss 0.0372573621571064\n",
            "epoch 19 batch 10 loss 0.04127373546361923\n",
            "epoch 19 batch 11 loss 0.033525850623846054\n",
            "epoch 19 batch 12 loss 0.033329930156469345\n",
            "epoch 19 batch 13 loss 0.040331896394491196\n",
            "epoch 19 batch 14 loss 0.03374648466706276\n",
            "epoch 19 batch 15 loss 0.03590665012598038\n",
            "epoch 19 batch 16 loss 0.03425437957048416\n",
            "epoch 19 batch 17 loss 0.03537575900554657\n",
            "epoch 19 batch 18 loss 0.03477836400270462\n",
            "epoch 19 batch 19 loss 0.036787066608667374\n",
            "epoch 19 batch 20 loss 0.0362127423286438\n",
            "epoch 19 batch 21 loss 0.035654935985803604\n",
            "epoch 19 batch 22 loss 0.031628623604774475\n",
            "epoch 19 batch 23 loss 0.03712069243192673\n",
            "epoch 19 batch 24 loss 0.03612538054585457\n",
            "epoch 19 batch 25 loss 0.0451480858027935\n",
            "epoch 19 batch 26 loss 0.03426528722047806\n",
            "epoch 19 batch 27 loss 0.03951150178909302\n",
            "epoch 19 batch 28 loss 0.033130258321762085\n",
            "epoch 19 batch 29 loss 0.035712163895368576\n",
            "epoch 19 batch 30 loss 0.035463545471429825\n",
            "epoch 19 batch 31 loss 0.03786793723702431\n",
            "epoch 19 batch 32 loss 0.031617920845746994\n",
            "epoch 19 batch 33 loss 0.03365856409072876\n",
            "epoch 19 batch 34 loss 0.03184881806373596\n",
            "epoch 19 batch 35 loss 0.033492133021354675\n",
            "epoch 19 batch 36 loss 0.04222123697400093\n",
            "epoch 19 batch 37 loss 0.0383240282535553\n",
            "epoch 19 batch 38 loss 0.03589225560426712\n",
            "epoch 19 batch 39 loss 0.0335964672267437\n",
            "epoch 19 batch 40 loss 0.03554680943489075\n",
            "epoch 19 batch 41 loss 0.03502825275063515\n",
            "epoch 19 batch 42 loss 0.033545948565006256\n",
            "epoch 19 batch 43 loss 0.03859832510352135\n",
            "epoch 19 batch 44 loss 0.0375513881444931\n",
            "epoch 19 batch 45 loss 0.034341827034950256\n",
            "epoch 19 batch 46 loss 0.035586629062891006\n",
            "epoch 19 batch 47 loss 0.03835669532418251\n",
            "epoch 19 batch 48 loss 0.03210464119911194\n",
            "epoch 19 batch 49 loss 0.03713832050561905\n",
            "epoch 19 batch 50 loss 0.03653658181428909\n",
            "epoch 19 batch 51 loss 0.03769511356949806\n",
            "epoch 19 batch 52 loss 0.03416693955659866\n",
            "epoch 19 batch 53 loss 0.03179887309670448\n",
            "epoch 19 batch 54 loss 0.035641882568597794\n",
            "epoch 19 batch 55 loss 0.03325210511684418\n",
            "epoch 19 batch 56 loss 0.0332183763384819\n",
            "epoch 19 batch 57 loss 0.03847052901983261\n",
            "epoch 19 batch 58 loss 0.037793222814798355\n",
            "epoch 19 batch 59 loss 0.034503109753131866\n",
            "epoch 19 batch 60 loss 0.036508072167634964\n",
            "epoch 19 batch 61 loss 0.03504959121346474\n",
            "epoch 19 batch 62 loss 0.03496396541595459\n",
            "epoch 19 batch 63 loss 0.03439703583717346\n",
            "epoch 19 batch 64 loss 0.038969241082668304\n",
            "epoch 19 batch 65 loss 0.0345868319272995\n",
            "epoch 19 batch 66 loss 0.037154655903577805\n",
            "epoch 19 batch 67 loss 0.04166916385293007\n",
            "epoch 19 batch 68 loss 0.03672601655125618\n",
            "epoch 19 batch 69 loss 0.03612257167696953\n",
            "epoch 19 batch 70 loss 0.03771737962961197\n",
            "epoch 19 batch 71 loss 0.0329865887761116\n",
            "epoch 19 batch 72 loss 0.035338304936885834\n",
            "epoch 19 batch 73 loss 0.03781100735068321\n",
            "epoch 19 batch 74 loss 0.03463666886091232\n",
            "epoch 19 batch 75 loss 0.03821055218577385\n",
            "epoch 19 batch 76 loss 0.03096761368215084\n",
            "epoch 19 batch 77 loss 0.035029828548431396\n",
            "epoch 19 batch 78 loss 0.03566334769129753\n",
            "epoch 19 batch 79 loss 0.03317893669009209\n",
            "epoch 19 batch 80 loss 0.037530966103076935\n",
            "epoch 19 batch 81 loss 0.03398462384939194\n",
            "epoch 19 batch 82 loss 0.03784254565834999\n",
            "epoch 19 batch 83 loss 0.03367858752608299\n",
            "epoch 19 batch 84 loss 0.03386865183711052\n",
            "epoch 19 batch 85 loss 0.03801232576370239\n",
            "epoch 19 batch 86 loss 0.031246475875377655\n",
            "epoch 19 batch 87 loss 0.033865854144096375\n",
            "epoch 19 batch 88 loss 0.034181807190179825\n",
            "epoch 19 batch 89 loss 0.03830109164118767\n",
            "epoch 19 batch 90 loss 0.0337810143828392\n",
            "epoch 19 batch 91 loss 0.03644007816910744\n",
            "epoch 19 batch 92 loss 0.03479381650686264\n",
            "epoch 19 batch 93 loss 0.03654993698000908\n",
            "epoch 19 batch 94 loss 0.035154398530721664\n",
            "epoch 19 batch 95 loss 0.03882715106010437\n",
            "epoch 19 batch 96 loss 0.033309128135442734\n",
            "epoch 19 batch 97 loss 0.03450813144445419\n",
            "epoch 19 batch 98 loss 0.03436125069856644\n",
            "epoch 19 batch 99 loss 0.037942226976156235\n",
            "epoch 19 batch 100 loss 0.039707109332084656\n",
            "epoch 19 batch 101 loss 0.03273551166057587\n",
            "epoch 19 batch 102 loss 0.034550439566373825\n",
            "epoch 19 batch 103 loss 0.03695279732346535\n",
            "epoch 19 batch 104 loss 0.03338533639907837\n",
            "epoch 19 batch 105 loss 0.03257152810692787\n",
            "epoch 19 batch 106 loss 0.03379737585783005\n",
            "epoch 19 batch 107 loss 0.03400929272174835\n",
            "epoch 19 batch 108 loss 0.036214679479599\n",
            "epoch 19 batch 109 loss 0.03687926381826401\n",
            "epoch 19 batch 110 loss 0.030434556305408478\n",
            "epoch 19 batch 111 loss 0.03555230796337128\n",
            "epoch 19 batch 112 loss 0.039557211101055145\n",
            "epoch 19 batch 113 loss 0.034953903406858444\n",
            "epoch 19 batch 114 loss 0.034414198249578476\n",
            "epoch 19 batch 115 loss 0.03577030822634697\n",
            "epoch 19 batch 116 loss 0.03676999360322952\n",
            "epoch 19 batch 117 loss 0.03838224336504936\n",
            "epoch 19 batch 118 loss 0.034800585359334946\n",
            "epoch 19 batch 119 loss 0.03667246550321579\n",
            "epoch 19 batch 120 loss 0.03551594913005829\n",
            "epoch 19 batch 121 loss 0.03679683804512024\n",
            "epoch 19 batch 122 loss 0.03587024658918381\n",
            "epoch 19 batch 123 loss 0.03344745561480522\n",
            "epoch 19 batch 124 loss 0.03717527538537979\n",
            "epoch 19 batch 125 loss 0.034637339413166046\n",
            "epoch 19 batch 126 loss 0.028915712609887123\n",
            "epoch 19 batch 127 loss 0.03943457454442978\n",
            "epoch 19 batch 128 loss 0.034369755536317825\n",
            "epoch 19 batch 129 loss 0.03258807584643364\n",
            "epoch 19 batch 130 loss 0.04372367262840271\n",
            "epoch 19 batch 131 loss 0.037024542689323425\n",
            "epoch 19 batch 132 loss 0.03746851533651352\n",
            "epoch 19 batch 133 loss 0.033395916223526\n",
            "epoch 19 batch 134 loss 0.0369054339826107\n",
            "epoch 19 batch 135 loss 0.03866817057132721\n",
            "epoch 19 batch 136 loss 0.034839726984500885\n",
            "epoch 19 batch 137 loss 0.03525734692811966\n",
            "epoch 19 batch 138 loss 0.03576664626598358\n",
            "epoch 19 batch 139 loss 0.034145958721637726\n",
            "epoch 19 batch 140 loss 0.03809881582856178\n",
            "epoch 19 batch 141 loss 0.033465806394815445\n",
            "epoch 19 batch 142 loss 0.0315055288374424\n",
            "epoch 19 batch 143 loss 0.02888421155512333\n",
            "epoch 19 batch 144 loss 0.03366959095001221\n",
            "epoch 19 batch 145 loss 0.03652949631214142\n",
            "epoch 19 batch 146 loss 0.04165301471948624\n",
            "epoch 19 batch 147 loss 0.037939753383398056\n",
            "epoch 19 batch 148 loss 0.03532350808382034\n",
            "epoch 19 batch 149 loss 0.037317436188459396\n",
            "epoch 19 batch 150 loss 0.03574187308549881\n",
            "epoch 19 batch 151 loss 0.034044716507196426\n",
            "epoch 19 batch 152 loss 0.03151967003941536\n",
            "epoch 19 batch 153 loss 0.03576279431581497\n",
            "epoch 19 batch 154 loss 0.03426972031593323\n",
            "epoch 19 batch 155 loss 0.03531084209680557\n",
            "epoch 19 batch 156 loss 0.03752065449953079\n",
            "epoch 19 batch 157 loss 0.03424916788935661\n",
            "epoch 19 batch 158 loss 0.033599283546209335\n",
            "epoch 19 batch 159 loss 0.030362218618392944\n",
            "epoch 19 batch 160 loss 0.03556648641824722\n",
            "epoch 19 batch 161 loss 0.0335337296128273\n",
            "epoch 19 batch 162 loss 0.038995444774627686\n",
            "epoch 19 batch 163 loss 0.03621993586421013\n",
            "epoch 19 batch 164 loss 0.036125607788562775\n",
            "epoch 19 batch 165 loss 0.03351105749607086\n",
            "epoch 19 batch 166 loss 0.037928514182567596\n",
            "epoch 19 batch 167 loss 0.03459428250789642\n",
            "epoch 19 batch 168 loss 0.039147671312093735\n",
            "epoch 19 batch 169 loss 0.034121084958314896\n",
            "epoch 19 batch 170 loss 0.03422119468450546\n",
            "epoch 19 batch 171 loss 0.035856910049915314\n",
            "epoch 19 batch 172 loss 0.03730638697743416\n",
            "epoch 19 batch 173 loss 0.03448079526424408\n",
            "epoch 19 batch 174 loss 0.0341053307056427\n",
            "epoch 19 batch 175 loss 0.03733120113611221\n",
            "epoch 19 batch 176 loss 0.03425105661153793\n",
            "epoch 19 batch 177 loss 0.032763730734586716\n",
            "epoch 19 batch 178 loss 0.03307300806045532\n",
            "epoch 19 batch 179 loss 0.0331924632191658\n",
            "epoch 19 batch 180 loss 0.035994455218315125\n",
            "epoch 19 batch 181 loss 0.036260947585105896\n",
            "epoch 19 batch 182 loss 0.03473309800028801\n",
            "epoch 19 batch 183 loss 0.03737308830022812\n",
            "epoch 19 batch 184 loss 0.044269364327192307\n",
            "epoch 19 batch 185 loss 0.03587227314710617\n",
            "epoch 19 batch 186 loss 0.0419602207839489\n",
            "epoch 19 batch 187 loss 0.03055407479405403\n",
            "epoch 19 batch 188 loss 0.0340329185128212\n",
            "epoch 19 batch 189 loss 0.040596798062324524\n",
            "epoch 19 batch 190 loss 0.03284190222620964\n",
            "epoch 19 batch 191 loss 0.03729667887091637\n",
            "epoch 19 batch 192 loss 0.031946148723363876\n",
            "epoch 19 batch 193 loss 0.032486382871866226\n",
            "epoch 19 batch 194 loss 0.03217426687479019\n",
            "epoch 19 batch 195 loss 0.034649595618247986\n",
            "epoch 19 batch 196 loss 0.03670402243733406\n",
            "epoch 19 batch 197 loss 0.03287191689014435\n",
            "epoch 19 batch 198 loss 0.032885290682315826\n",
            "epoch 19 batch 199 loss 0.04176471009850502\n",
            "epoch 19 batch 200 loss 0.03840678930282593\n",
            "epoch 19 batch 201 loss 0.03571315482258797\n",
            "epoch 19 batch 202 loss 0.03425221145153046\n",
            "epoch 19 batch 203 loss 0.034089360386133194\n",
            "epoch 19 batch 204 loss 0.0311335027217865\n",
            "epoch 19 batch 205 loss 0.03719078376889229\n",
            "epoch 19 batch 206 loss 0.029209356755018234\n",
            "epoch 19 batch 207 loss 0.03233225643634796\n",
            "epoch 19 batch 208 loss 0.03856717422604561\n",
            "epoch 19 batch 209 loss 0.03388163074851036\n",
            "epoch 19 batch 210 loss 0.039651405066251755\n",
            "epoch 19 batch 211 loss 0.03681987524032593\n",
            "epoch 19 batch 212 loss 0.03354493901133537\n",
            "epoch 19 batch 213 loss 0.03399290516972542\n",
            "epoch 19 batch 214 loss 0.03329071030020714\n",
            "epoch 19 batch 215 loss 0.030638039112091064\n",
            "epoch 19 batch 216 loss 0.03388925641775131\n",
            "epoch 19 batch 217 loss 0.02871220000088215\n",
            "epoch 19 batch 218 loss 0.03456658124923706\n",
            "epoch 19 batch 219 loss 0.03611939772963524\n",
            "epoch 19 batch 220 loss 0.03425108641386032\n",
            "epoch 19 batch 221 loss 0.034589312970638275\n",
            "epoch 19 batch 222 loss 0.03314102068543434\n",
            "epoch 19 batch 223 loss 0.03659052029252052\n",
            "epoch 19 batch 224 loss 0.035338111221790314\n",
            "epoch 19 batch 225 loss 0.034481629729270935\n",
            "epoch 19 batch 226 loss 0.03413626179099083\n",
            "epoch 19 batch 227 loss 0.03423083573579788\n",
            "epoch 19 batch 228 loss 0.0341479629278183\n",
            "epoch 19 batch 229 loss 0.03392194211483002\n",
            "epoch 19 batch 230 loss 0.019357619807124138\n",
            "Loading data...\n",
            "2034\n",
            "first sententence : قوله ولا تكره ضيافته                                                                                                                                                                                                                                                                                        \n",
            "Extracting features...\n",
            "torch.Size([2967, 300, 37])\n",
            "Creating dataloader...\n",
            "Done data creation !\n",
            "-------------------start evaluating-------------------\n",
            "Accuracy:  96.33169555664062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test case\n",
        "data=\"ذهب علي الي الشاطئ\"\n",
        "enc = torch.empty(0, len(arabic_letters),dtype=torch.float32).to(device)\n",
        "for letter in data:\n",
        "    x = encoding(letter).unsqueeze(0).to(device)\n",
        "    enc = torch.cat((enc, x), 0)\n",
        "# print(encoded_data.shape)\n",
        "encoding_labels=torch.tensor([],dtype=torch.long).to(device)\n",
        "predictions=[]\n",
        "# for i in range(len(enc)):\n",
        "yhat = model(enc.to(device))\n",
        "yhat = yhat.detach().cpu().numpy()\n",
        "# reshape the outputs to [batch_size * sequence_length, num_classes]\n",
        "yhat = yhat.reshape(-1, yhat.shape[-1])\n",
        "# get predicted classes\n",
        "predicted_classes = np.argmax(yhat, axis=1)\n",
        "# store predictions and actuals\n",
        "predictions.extend(predicted_classes.tolist())\n",
        "res=\"\"\n",
        "inverted_classes = {v: k for k, v in classes.items()}\n",
        "\n",
        "for i in range(len(data)):\n",
        "  res+=data[i]\n",
        "  res+=inverted_classes[predictions[i]]\n",
        "print(res)\n",
        "\n"
      ],
      "metadata": {
        "id": "6svTOWYF255z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "inp_vocab_size = 37\n",
        "hidden_dim = 128\n",
        "seq_len = 400\n",
        "num_classes = 16\n",
        "\n",
        "# model = LSTM(inp_vocab_size, hidden_dim, seq_len, num_classes)\n",
        "# model.load_state_dict(torch.load(\"/content/drive/MyDrive/NLP project/drive/files/model_zeyd.pth\"))\n",
        "# model.to(device)\n",
        "validatindata=DataSet(\"/content/drive/MyDrive/NLP project/sample_test_no_diacritics.txt\",batch_size=1)\n",
        "validatindataloader=validatindata.getdata()\n",
        "print(\"-------------------start evaluating-------------------\")\n",
        "\n",
        "\n",
        "predictions = []\n",
        "actuals=[]\n",
        "data=validatindata.getx()\n",
        "statments=[state for state in data]\n",
        "\n",
        "def evaluate_model(test_dl, model):\n",
        "    predicted_classes = []\n",
        "    model.to(device)\n",
        "    for i, (inputs, targets) in enumerate(test_dl):\n",
        "        yhat = model(inputs.to(device))\n",
        "        yhat = yhat.detach().cpu().numpy()\n",
        "        # reshape the outputs to [batch_size * sequence_length, num_classes]\n",
        "        yhat = yhat.reshape(-1, yhat.shape[-1])\n",
        "        # get predicted classes\n",
        "        predicted_classes = np.argmax(yhat, axis=1)\n",
        "        # convert targets to numpy array and reshape\n",
        "        targets = targets.cpu().numpy().reshape(-1)\n",
        "        while len(targets)>0 and targets[-1]==15:\n",
        "            targets=targets[:-1]\n",
        "            predicted_classes=predicted_classes[:-1]\n",
        "            statments[i]=statments[i][:-1]\n",
        "        # store predictions and actuals\n",
        "        predictions.extend(predicted_classes)\n",
        "        actuals.extend(targets.tolist())\n",
        "\n",
        "\n",
        "\n",
        "evaluate_model(validatindataloader,model)\n",
        "result = \"\"\n",
        "c=0\n",
        "# for i in range(len(statments)):\n",
        "#     for j in range(len(statments[i])):\n",
        "#         result += statments[i][j]\n",
        "#         result += inverted_classes[predictions[i*10+j]]\n",
        "#         if (statments[i][j]==\" \"):\n",
        "#           c+=1\n",
        "\n",
        "# print(\"Reversed string: \", result)\n",
        "\n",
        "# print(\"Predicted classes: \", predictions)\n",
        "actuals_res=[]\n",
        "predictions_res=[]\n",
        "print (len(actuals))\n",
        "\n",
        "for i in range(len(actuals)):\n",
        "  if actuals[i]!=15:\n",
        "    actuals_res.append(actuals[i])\n",
        "    predictions_res.append(predictions[i])\n",
        "print(len(actuals_res))\n",
        "predicted_classes_csv = [(i, label) for i, label in enumerate(predictions_res)]\n",
        "\n",
        "id_line_letter_df_after = pd.DataFrame(\n",
        "    {\n",
        "        \"ID\": [info[0] for info in predicted_classes_csv],\n",
        "        \"label\": [info[1] for info in predicted_classes_csv],\n",
        "    }\n",
        ")\n",
        "id_line_letter_df_after.to_csv(\"testsample.csv\", index=False)\n",
        "\n",
        "\n",
        "\n",
        "# def calculate_DER(actual_labels, predicted_labels):\n",
        "#     # Convert lists to PyTorch tensors if they are not already\n",
        "#     if not isinstance(actual_labels, torch.Tensor):\n",
        "#         actual_labels = torch.tensor(actual_labels)\n",
        "#     if not isinstance(predicted_labels, torch.Tensor):\n",
        "#         predicted_labels = torch.tensor(predicted_labels)\n",
        "\n",
        "#     # Check if the lengths of both label sequences match\n",
        "#     if len(actual_labels) != len(predicted_labels):\n",
        "#         raise ValueError(\"Lengths of actual and predicted labels should match.\")\n",
        "\n",
        "#     total_errors = torch.sum(actual_labels != predicted_labels)\n",
        "#     total_frames = len(actual_labels)\n",
        "\n",
        "#     # DER calculation\n",
        "#     DER = (1-(total_errors / total_frames)) * 100.0\n",
        "#     return DER.item()  # Convert PyTorch scalar to Python float\n",
        "\n",
        "# predictions=[x for x in predictions if x!=15]\n",
        "# actuals=[x for x in actuals if x!=15]\n",
        "\n",
        "# acc = calculate_DER(np.array(actuals_res), np.array(predictions_res))\n",
        "# print(\"Accuracy: \", acc)"
      ],
      "metadata": {
        "id": "t2_eYAAnCslV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "907e8334-1635-4bc7-ae73-5bfe64c188f7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "2\n",
            "Extracting features...\n",
            "torch.Size([1, 300, 37])\n",
            "Creating dataloader...\n",
            "Done data creation !\n",
            "-------------------start evaluating-------------------\n",
            "226\n",
            "182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# inp_vocab_size = 37\n",
        "# hidden_dim = 128\n",
        "# seq_len = 400\n",
        "# num_classes = 16\n",
        "\n",
        "# model = LSTM(inp_vocab_size, hidden_dim, seq_len, num_classes)\n",
        "# model.load_state_dict(torch.load(\"/content/model.pth\", map_location=torch.device(\"cpu\")))\n",
        "\n",
        "validatindata=DataSet(\"/content/test_no_diacritics.txt\",batch_size=1)\n",
        "validatindataloader=validatindata.getdata()\n",
        "print(\"-------------------start evaluating-------------------\")\n",
        "\n",
        "\n",
        "predictions = []\n",
        "actuals=[]\n",
        "# data=validatindata.getx()\n",
        "statments=[state for state in data]\n",
        "# sub=validatindata.getx()\n",
        "# print(sub[:10])\n",
        "def evaluate_model(test_dl, model):\n",
        "    predicted_classes = []\n",
        "    model.to(device)\n",
        "    for i, (inputs, targets) in enumerate(test_dl):\n",
        "        if (i==0):\n",
        "          print(targets)\n",
        "        yhat = model(inputs.to(device))\n",
        "        yhat = yhat.detach().cpu().numpy()\n",
        "        # reshape the outputs to [batch_size * sequence_length, num_classes]\n",
        "        yhat = yhat.reshape(-1, yhat.shape[-1])\n",
        "        # get predicted classes\n",
        "        predicted_classes = np.argmax(yhat, axis=1)\n",
        "        # convert targets to numpy array and reshape\n",
        "        targets = targets.cpu().numpy().reshape(-1)\n",
        "        while len(targets)>0 and targets[-1]==15:\n",
        "            targets=targets[:-1]\n",
        "            predicted_classes=predicted_classes[:-1]\n",
        "            # statments[i]=statments[i][:-1]\n",
        "        # store predictions and actuals\n",
        "        predictions.extend(predicted_classes)\n",
        "        actuals.extend(targets.tolist())\n",
        "\n",
        "evaluate_model(validatindataloader,model)\n",
        "result = \"\"\n",
        "c=0\n",
        "# for i in range(len(statments)):\n",
        "#     for j in range(len(statments[i])):\n",
        "#         result += statments[i][j]\n",
        "#         result += inverted_classes[predictions[i*10+j]]\n",
        "#         if (statments[i][j]==\" \"):\n",
        "#           c+=1\n",
        "\n",
        "# print(\"Reversed string: \", result)\n",
        "\n",
        "# print(\"Predicted classes: \", predictions)\n",
        "actuals_res=[]\n",
        "predictions_res=[]\n",
        "# print (len(actuals))\n",
        "\n",
        "for i in range(len(actuals)):\n",
        "  if actuals[i]!=15:\n",
        "    actuals_res.append(actuals[i])\n",
        "    predictions_res.append(predictions[i])\n",
        "# print(len(actuals_res))\n",
        "predicted_classes_csv = [(i, label) for i, label in enumerate(predictions_res)]\n",
        "\n",
        "id_line_letter_df_after = pd.DataFrame(\n",
        "    {\n",
        "        \"ID\": [info[0] for info in predicted_classes_csv],\n",
        "        \"label\": [info[1] for info in predicted_classes_csv],\n",
        "    }\n",
        ")\n",
        "id_line_letter_df_after.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "\n",
        "\n",
        "# def calculate_DER(actual_labels, predicted_labels):\n",
        "#     # Convert lists to PyTorch tensors if they are not already\n",
        "#     if not isinstance(actual_labels, torch.Tensor):\n",
        "#         actual_labels = torch.tensor(actual_labels)\n",
        "#     if not isinstance(predicted_labels, torch.Tensor):\n",
        "#         predicted_labels = torch.tensor(predicted_labels)\n",
        "\n",
        "#     # Check if the lengths of both label sequences match\n",
        "#     if len(actual_labels) != len(predicted_labels):\n",
        "#         raise ValueError(\"Lengths of actual and predicted labels should match.\")\n",
        "\n",
        "#     total_errors = torch.sum(actual_labels != predicted_labels)\n",
        "#     total_frames = len(actual_labels)\n",
        "\n",
        "#     # DER calculation\n",
        "#     DER = (1-(total_errors / total_frames)) * 100.0\n",
        "#     return DER.item()  # Convert PyTorch scalar to Python float\n",
        "\n",
        "# predictions=[x for x in predictions if x!=15]\n",
        "# actuals=[x for x in actuals if x!=15]\n",
        "\n",
        "# acc = calculate_DER(np.array(actuals_res), np.array(predictions_res))\n",
        "print(\"Accuracy: \", acc)"
      ],
      "metadata": {
        "id": "SA8vdWh96fUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b539f591-eb83-4992-e804-6a8611844cf8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "2065\n",
            "first sententence : ليس للوكيل بالقبض أن يبرأ المدين أو يهب الدين له أو يأخذ رهنا من المدين في مقابل الدين أو يقبل إحالته على شخص آخر لكن له أن يأخذ كفيلا لكن ليس له أن يأخذ كفيلا بشرط براءة الأصيل انظر المادة الأنقروي الطحطاوي وصرة الفتاوى البحر                                                                          \n",
            "Extracting features...\n",
            "torch.Size([2974, 300, 37])\n",
            "Creating dataloader...\n",
            "Done data creation !\n",
            "-------------------start evaluating-------------------\n",
            "tensor([[14, 14, 14, 15, 14, 14, 14, 14, 14, 14, 15, 14, 14, 14, 14, 14, 14, 15,\n",
            "         14, 14, 15, 14, 14, 14, 14, 15, 14, 14, 14, 14, 14, 14, 15, 14, 14, 15,\n",
            "         14, 14, 14, 15, 14, 14, 14, 14, 14, 15, 14, 14, 15, 14, 14, 15, 14, 14,\n",
            "         14, 14, 15, 14, 14, 14, 14, 15, 14, 14, 15, 14, 14, 14, 14, 14, 14, 15,\n",
            "         14, 14, 15, 14, 14, 14, 14, 14, 15, 14, 14, 14, 14, 14, 15, 14, 14, 15,\n",
            "         14, 14, 14, 14, 15, 14, 14, 14, 14, 14, 14, 15, 14, 14, 14, 15, 14, 14,\n",
            "         14, 15, 14, 14, 14, 15, 14, 14, 14, 15, 14, 14, 15, 14, 14, 15, 14, 14,\n",
            "         14, 14, 15, 14, 14, 14, 14, 14, 15, 14, 14, 14, 15, 14, 14, 14, 15, 14,\n",
            "         14, 15, 14, 14, 15, 14, 14, 14, 14, 15, 14, 14, 14, 14, 14, 15, 14, 14,\n",
            "         14, 14, 15, 14, 14, 14, 14, 14, 15, 14, 14, 14, 14, 14, 14, 15, 14, 14,\n",
            "         14, 14, 15, 14, 14, 14, 14, 14, 14, 15, 14, 14, 14, 14, 14, 14, 14, 14,\n",
            "         15, 14, 14, 14, 14, 14, 14, 14, 14, 15, 14, 14, 14, 14, 15, 14, 14, 14,\n",
            "         14, 14, 14, 14, 15, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15,\n",
            "         15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
            "         15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
            "         15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
            "         15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15]], device='cuda:0')\n",
            "Accuracy:  17.717599868774414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# actuals_res=[]\n",
        "# print(len(actuals))\n",
        "# for i in range(len(actuals)):\n",
        "#   if actuals[i]!=15:\n",
        "#     actuals_res.append(actuals[i])\n",
        "#     predictions_res.append(predictions[i])\n",
        "# print(len(actuals_res))"
      ],
      "metadata": {
        "id": "t4o1KQ6wZYmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# validatindata=DataSet(\"/content/drive/MyDrive/NLP project/test_no_diacritics.txt\",batch_size=1)\n",
        "# validatindataloader=validatindata.getdata()"
      ],
      "metadata": {
        "id": "Up-QjXZdaiMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "inverted_classes = {v: k for k, v in classes.items()}\n",
        "\n",
        "\n",
        "inp_vocab_size = 37\n",
        "hidden_dim = 128\n",
        "seq_len = 400\n",
        "num_classes = 16\n",
        "\n",
        "model = LSTM(inp_vocab_size, hidden_dim, seq_len, num_classes)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/NLP project/drive/finalOne.pth\", map_location=torch.device(\"cuda\")))\n",
        "# model.eval()\n",
        "\n",
        "# Traindata = DataSet( \"/content/drive/MyDrive/NLP project/Arabic-text-to-diacritics/Dataset/train.txt\", batch_size = 1 )\n",
        "# Traindataloader = Traindata.getdata()\n",
        "\n",
        "# model = LSTM(inp_vocab_size, hidden_dim, seq_len, num_classes)\n",
        "print(\"-------------------start training-------------------\")\n",
        "# train(Traindataloader, model)\n",
        "\n",
        "\n",
        "validatindata=DataSet(\"/content/drive/MyDrive/NLP project/test_no_diacritics.txt\",batch_size=1)\n",
        "validatindataloader=validatindata.getdata()\n",
        "print(\"-------------------start evaluating-------------------\")\n",
        "\n",
        "# test_sentence = \"ذهب علي الى الشاطئ\"\n",
        "# print(\"Test sentence: \", test_sentence)\n",
        "\n",
        "# enc_before = torch.empty(0, inp_vocab_size, dtype=torch.float32)\n",
        "# list_of_letters_before = []\n",
        "# numb_of_lines = 0\n",
        "# for letter in test_sentence:\n",
        "#     if letter == \" \":\n",
        "#         continue\n",
        "#     list_of_letters_before.append((numb_of_lines, letter))\n",
        "#     x = encoding(letter).unsqueeze(0)\n",
        "#     enc_before = torch.cat((enc_before, x), 0)\n",
        "#     if letter == \"\\n\":\n",
        "#         numb_of_lines += 1\n",
        "# list_of_letters_before=[]\n",
        "# Xdata=validatindata.getx()\n",
        "# for i,sent in enumerate(Xdata):\n",
        "#     for letter in sent:\n",
        "#         list_of_letters_before.append((i,letter))\n",
        "\n",
        "# print(\"List of arabic letters before predictions: \", list_of_letters_before)\n",
        "# id_line_letter_data_before = [\n",
        "#     (i, line, label) for i, (line, label) in enumerate(list_of_letters_before)\n",
        "# ]\n",
        "\n",
        "\n",
        "# print(\"id, line, letter before predictions: \", id_line_letter_data_before)\n",
        "# id_line_letter_df_before = pd.DataFrame(\n",
        "#     {\n",
        "#         \"id\": [info[0] for info in id_line_letter_data_before],\n",
        "#         \"line\": [info[1] for info in id_line_letter_data_before],\n",
        "#         \"letter\": [info[2] for info in id_line_letter_data_before],\n",
        "#     }\n",
        "# )\n",
        "# id_line_letter_df_before.to_csv(\"submission_id_line_letter.csv\", index=False)\n",
        "\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     yhat = model(enc_before)\n",
        "#     yhat = yhat.detach().cpu().numpy()\n",
        "#     yhat = yhat.reshape(-1, yhat.shape[-1])\n",
        "#     predicted_classes = torch.argmax(torch.from_numpy(yhat), axis=1).tolist()\n",
        "# print(\"Predicted classes: \", predicted_classes)\n",
        "data=validatindata.getx()\n",
        "\n",
        "\n",
        "predictions = []\n",
        "actuals=[]\n",
        "def evaluate_model(test_dl, model):\n",
        "    predicted_classes = []\n",
        "    model.to(device)\n",
        "    for i, (inputs, targets) in enumerate(test_dl):\n",
        "        yhat = model(inputs.to(device))\n",
        "        yhat = yhat.detach().cpu().numpy()\n",
        "        # reshape the outputs to [batch_size * sequence_length, num_classes]\n",
        "        yhat = yhat.reshape(-1, yhat.shape[-1])\n",
        "        # get predicted classes\n",
        "        predicted_classes = np.argmax(yhat, axis=1)\n",
        "        # convert targets to numpy array and reshape\n",
        "        targets = targets.cpu().numpy().reshape(-1)\n",
        "        # print(\"length of predicted classes: \",len(predicted_classes))\n",
        "        # print(\"length of targets: \",len(targets))\n",
        "        while len(targets)>0 and targets[-1]==15:\n",
        "            targets=targets[:-1]\n",
        "            data[i]=data[i][:-1]\n",
        "            predicted_classes=predicted_classes[:-1]\n",
        "        # store predictions and actuals\n",
        "        predictions.extend(predicted_classes.tolist())\n",
        "        actuals.extend(targets.tolist())\n",
        "        print(\"===============================================\")\n",
        "        print(\"length of predicted classes: \",len(predicted_classes))\n",
        "        print(\"length of targets: \",len(targets))\n",
        "        print(\"length of data: \",len(data[i]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "evaluate_model(validatindataloader,model)\n",
        "result = \"\"\n",
        "for i in range(len(data)):\n",
        "    for j in range(len(data[i])):\n",
        "        result += data[i][j]\n",
        "        result += inverted_classes[predictions[i*10+j]]\n",
        "\n",
        "# write_to_file_string(\"test\",\"submission.txt\", result[:100])\n",
        "print(result[:100])\n",
        "\n",
        "print(\"Length of data: \",len(data))\n",
        "print(\"Length of predictions: \",len(predictions))\n",
        "\n",
        "# print(\"Reversed string: \", result[:100])\n",
        "\n",
        "# print(\"Predicted classes: \", predictions)\n",
        "predicted_classes_csv = [(i, label) for i, label in enumerate(predictions)]\n",
        "\n",
        "id_line_letter_df_after = pd.DataFrame(\n",
        "    {\n",
        "        \"id\": [info[0] for info in predicted_classes_csv],\n",
        "        \"label\": [info[1] for info in predicted_classes_csv],\n",
        "    }\n",
        ")\n",
        "id_line_letter_df_after.to_csv(\"predicted_chars.csv\", index=False)\n",
        "\n",
        "\n",
        "\n",
        "def calculate_DER(actual_labels, predicted_labels):\n",
        "    # Convert lists to PyTorch tensors if they are not already\n",
        "    if not isinstance(actual_labels, torch.Tensor):\n",
        "        actual_labels = torch.tensor(actual_labels)\n",
        "    if not isinstance(predicted_labels, torch.Tensor):\n",
        "        predicted_labels = torch.tensor(predicted_labels)\n",
        "\n",
        "    # Check if the lengths of both label sequences match\n",
        "    if len(actual_labels) != len(predicted_labels):\n",
        "        raise ValueError(\"Lengths of actual and predicted labels should match.\")\n",
        "\n",
        "    total_errors = torch.sum(actual_labels != predicted_labels)\n",
        "    total_frames = len(actual_labels)\n",
        "\n",
        "    # DER calculation\n",
        "    DER = (1-(total_errors / total_frames)) * 100.0\n",
        "    return DER.item()  # Convert PyTorch scalar to Python float\n",
        "\n",
        "# predictions=[x for x in predictions if x!=15]\n",
        "# actuals=[x for x in actuals if x!=15]\n",
        "\n",
        "acc = calculate_DER(np.array(actuals), np.array(predictions))\n",
        "print(\"Accuracy: \", acc)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4piey-u4A_yA",
        "outputId": "cd18b2f5-a4a6-42f9-fbed-c3f6c4530f87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "===============================================\n",
            "length of predicted classes:  296\n",
            "length of targets:  296\n",
            "length of data:  296\n",
            "===============================================\n",
            "length of predicted classes:  105\n",
            "length of targets:  105\n",
            "length of data:  105\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  79\n",
            "length of targets:  79\n",
            "length of data:  79\n",
            "===============================================\n",
            "length of predicted classes:  199\n",
            "length of targets:  199\n",
            "length of data:  199\n",
            "===============================================\n",
            "length of predicted classes:  96\n",
            "length of targets:  96\n",
            "length of data:  96\n",
            "===============================================\n",
            "length of predicted classes:  166\n",
            "length of targets:  166\n",
            "length of data:  166\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  208\n",
            "length of targets:  208\n",
            "length of data:  208\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  80\n",
            "length of targets:  80\n",
            "length of data:  80\n",
            "===============================================\n",
            "length of predicted classes:  20\n",
            "length of targets:  20\n",
            "length of data:  20\n",
            "===============================================\n",
            "length of predicted classes:  33\n",
            "length of targets:  33\n",
            "length of data:  33\n",
            "===============================================\n",
            "length of predicted classes:  81\n",
            "length of targets:  81\n",
            "length of data:  81\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  71\n",
            "length of targets:  71\n",
            "length of data:  71\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  34\n",
            "length of targets:  34\n",
            "length of data:  34\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  55\n",
            "length of targets:  55\n",
            "length of data:  55\n",
            "===============================================\n",
            "length of predicted classes:  140\n",
            "length of targets:  140\n",
            "length of data:  140\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  126\n",
            "length of targets:  126\n",
            "length of data:  126\n",
            "===============================================\n",
            "length of predicted classes:  32\n",
            "length of targets:  32\n",
            "length of data:  32\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  295\n",
            "length of targets:  295\n",
            "length of data:  295\n",
            "===============================================\n",
            "length of predicted classes:  41\n",
            "length of targets:  41\n",
            "length of data:  41\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  91\n",
            "length of targets:  91\n",
            "length of data:  91\n",
            "===============================================\n",
            "length of predicted classes:  115\n",
            "length of targets:  115\n",
            "length of data:  115\n",
            "===============================================\n",
            "length of predicted classes:  282\n",
            "length of targets:  282\n",
            "length of data:  282\n",
            "===============================================\n",
            "length of predicted classes:  221\n",
            "length of targets:  221\n",
            "length of data:  221\n",
            "===============================================\n",
            "length of predicted classes:  23\n",
            "length of targets:  23\n",
            "length of data:  23\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  109\n",
            "length of targets:  109\n",
            "length of data:  109\n",
            "===============================================\n",
            "length of predicted classes:  63\n",
            "length of targets:  63\n",
            "length of data:  63\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  89\n",
            "length of targets:  89\n",
            "length of data:  89\n",
            "===============================================\n",
            "length of predicted classes:  187\n",
            "length of targets:  187\n",
            "length of data:  187\n",
            "===============================================\n",
            "length of predicted classes:  169\n",
            "length of targets:  169\n",
            "length of data:  169\n",
            "===============================================\n",
            "length of predicted classes:  86\n",
            "length of targets:  86\n",
            "length of data:  86\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  260\n",
            "length of targets:  260\n",
            "length of data:  260\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  35\n",
            "length of targets:  35\n",
            "length of data:  35\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  103\n",
            "length of targets:  103\n",
            "length of data:  103\n",
            "===============================================\n",
            "length of predicted classes:  275\n",
            "length of targets:  275\n",
            "length of data:  275\n",
            "===============================================\n",
            "length of predicted classes:  44\n",
            "length of targets:  44\n",
            "length of data:  44\n",
            "===============================================\n",
            "length of predicted classes:  45\n",
            "length of targets:  45\n",
            "length of data:  45\n",
            "===============================================\n",
            "length of predicted classes:  177\n",
            "length of targets:  177\n",
            "length of data:  177\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  249\n",
            "length of targets:  249\n",
            "length of data:  249\n",
            "===============================================\n",
            "length of predicted classes:  228\n",
            "length of targets:  228\n",
            "length of data:  228\n",
            "===============================================\n",
            "length of predicted classes:  46\n",
            "length of targets:  46\n",
            "length of data:  46\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  75\n",
            "length of targets:  75\n",
            "length of data:  75\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  57\n",
            "length of targets:  57\n",
            "length of data:  57\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  14\n",
            "length of targets:  14\n",
            "length of data:  14\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  86\n",
            "length of targets:  86\n",
            "length of data:  86\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  147\n",
            "length of targets:  147\n",
            "length of data:  147\n",
            "===============================================\n",
            "length of predicted classes:  27\n",
            "length of targets:  27\n",
            "length of data:  27\n",
            "===============================================\n",
            "length of predicted classes:  19\n",
            "length of targets:  19\n",
            "length of data:  19\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  10\n",
            "length of targets:  10\n",
            "length of data:  10\n",
            "===============================================\n",
            "length of predicted classes:  273\n",
            "length of targets:  273\n",
            "length of data:  273\n",
            "===============================================\n",
            "length of predicted classes:  278\n",
            "length of targets:  278\n",
            "length of data:  278\n",
            "===============================================\n",
            "length of predicted classes:  46\n",
            "length of targets:  46\n",
            "length of data:  46\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  153\n",
            "length of targets:  153\n",
            "length of data:  153\n",
            "===============================================\n",
            "length of predicted classes:  46\n",
            "length of targets:  46\n",
            "length of data:  46\n",
            "===============================================\n",
            "length of predicted classes:  87\n",
            "length of targets:  87\n",
            "length of data:  87\n",
            "===============================================\n",
            "length of predicted classes:  66\n",
            "length of targets:  66\n",
            "length of data:  66\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  184\n",
            "length of targets:  184\n",
            "length of data:  184\n",
            "===============================================\n",
            "length of predicted classes:  59\n",
            "length of targets:  59\n",
            "length of data:  59\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  13\n",
            "length of targets:  13\n",
            "length of data:  13\n",
            "===============================================\n",
            "length of predicted classes:  61\n",
            "length of targets:  61\n",
            "length of data:  61\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  168\n",
            "length of targets:  168\n",
            "length of data:  168\n",
            "===============================================\n",
            "length of predicted classes:  34\n",
            "length of targets:  34\n",
            "length of data:  34\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  152\n",
            "length of targets:  152\n",
            "length of data:  152\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  98\n",
            "length of targets:  98\n",
            "length of data:  98\n",
            "===============================================\n",
            "length of predicted classes:  168\n",
            "length of targets:  168\n",
            "length of data:  168\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  278\n",
            "length of targets:  278\n",
            "length of data:  278\n",
            "===============================================\n",
            "length of predicted classes:  250\n",
            "length of targets:  250\n",
            "length of data:  250\n",
            "===============================================\n",
            "length of predicted classes:  86\n",
            "length of targets:  86\n",
            "length of data:  86\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  43\n",
            "length of targets:  43\n",
            "length of data:  43\n",
            "===============================================\n",
            "length of predicted classes:  13\n",
            "length of targets:  13\n",
            "length of data:  13\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  71\n",
            "length of targets:  71\n",
            "length of data:  71\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  222\n",
            "length of targets:  222\n",
            "length of data:  222\n",
            "===============================================\n",
            "length of predicted classes:  250\n",
            "length of targets:  250\n",
            "length of data:  250\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  43\n",
            "length of targets:  43\n",
            "length of data:  43\n",
            "===============================================\n",
            "length of predicted classes:  83\n",
            "length of targets:  83\n",
            "length of data:  83\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  71\n",
            "length of targets:  71\n",
            "length of data:  71\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  72\n",
            "length of targets:  72\n",
            "length of data:  72\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  148\n",
            "length of targets:  148\n",
            "length of data:  148\n",
            "===============================================\n",
            "length of predicted classes:  0\n",
            "length of targets:  0\n",
            "length of data:  0\n",
            "===============================================\n",
            "length of predicted classes:  102\n",
            "length of targets:  102\n",
            "length of data:  102\n",
            "===============================================\n",
            "length of predicted classes:  27\n",
            "length of targets:  27\n",
            "length of data:  27\n",
            "===============================================\n",
            "length of predicted classes:  242\n",
            "length of targets:  242\n",
            "length of data:  242\n",
            "===============================================\n",
            "length of predicted classes:  46\n",
            "length of targets:  46\n",
            "length of data:  46\n",
            "===============================================\n",
            "length of predicted classes:  96\n",
            "length of targets:  96\n",
            "length of data:  96\n",
            "===============================================\n",
            "length of predicted classes:  22\n",
            "length of targets:  22\n",
            "length of data:  22\n",
            "===============================================\n",
            "length of predicted classes:  278\n",
            "length of targets:  278\n",
            "length of data:  278\n",
            "===============================================\n",
            "length of predicted classes:  33\n",
            "length of targets:  33\n",
            "length of data:  33\n",
            "===============================================\n",
            "length of predicted classes:  181\n",
            "length of targets:  181\n",
            "length of data:  181\n",
            "===============================================\n",
            "length of predicted classes:  131\n",
            "length of targets:  131\n",
            "length of data:  131\n",
            "===============================================\n",
            "length of predicted classes:  210\n",
            "length of targets:  210\n",
            "length of data:  210\n",
            "===============================================\n",
            "length of predicted classes:  63\n",
            "length of targets:  63\n",
            "length of data:  63\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  159\n",
            "length of targets:  159\n",
            "length of data:  159\n",
            "===============================================\n",
            "length of predicted classes:  17\n",
            "length of targets:  17\n",
            "length of data:  17\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  43\n",
            "length of targets:  43\n",
            "length of data:  43\n",
            "===============================================\n",
            "length of predicted classes:  40\n",
            "length of targets:  40\n",
            "length of data:  40\n",
            "===============================================\n",
            "length of predicted classes:  133\n",
            "length of targets:  133\n",
            "length of data:  133\n",
            "===============================================\n",
            "length of predicted classes:  51\n",
            "length of targets:  51\n",
            "length of data:  51\n",
            "===============================================\n",
            "length of predicted classes:  115\n",
            "length of targets:  115\n",
            "length of data:  115\n",
            "===============================================\n",
            "length of predicted classes:  176\n",
            "length of targets:  176\n",
            "length of data:  176\n",
            "===============================================\n",
            "length of predicted classes:  144\n",
            "length of targets:  144\n",
            "length of data:  144\n",
            "===============================================\n",
            "length of predicted classes:  106\n",
            "length of targets:  106\n",
            "length of data:  106\n",
            "===============================================\n",
            "length of predicted classes:  159\n",
            "length of targets:  159\n",
            "length of data:  159\n",
            "===============================================\n",
            "length of predicted classes:  59\n",
            "length of targets:  59\n",
            "length of data:  59\n",
            "===============================================\n",
            "length of predicted classes:  218\n",
            "length of targets:  218\n",
            "length of data:  218\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  9\n",
            "length of targets:  9\n",
            "length of data:  9\n",
            "===============================================\n",
            "length of predicted classes:  55\n",
            "length of targets:  55\n",
            "length of data:  55\n",
            "===============================================\n",
            "length of predicted classes:  45\n",
            "length of targets:  45\n",
            "length of data:  45\n",
            "===============================================\n",
            "length of predicted classes:  133\n",
            "length of targets:  133\n",
            "length of data:  133\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  135\n",
            "length of targets:  135\n",
            "length of data:  135\n",
            "===============================================\n",
            "length of predicted classes:  45\n",
            "length of targets:  45\n",
            "length of data:  45\n",
            "===============================================\n",
            "length of predicted classes:  72\n",
            "length of targets:  72\n",
            "length of data:  72\n",
            "===============================================\n",
            "length of predicted classes:  86\n",
            "length of targets:  86\n",
            "length of data:  86\n",
            "===============================================\n",
            "length of predicted classes:  87\n",
            "length of targets:  87\n",
            "length of data:  87\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  200\n",
            "length of targets:  200\n",
            "length of data:  200\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  114\n",
            "length of targets:  114\n",
            "length of data:  114\n",
            "===============================================\n",
            "length of predicted classes:  78\n",
            "length of targets:  78\n",
            "length of data:  78\n",
            "===============================================\n",
            "length of predicted classes:  29\n",
            "length of targets:  29\n",
            "length of data:  29\n",
            "===============================================\n",
            "length of predicted classes:  43\n",
            "length of targets:  43\n",
            "length of data:  43\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  49\n",
            "length of targets:  49\n",
            "length of data:  49\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  232\n",
            "length of targets:  232\n",
            "length of data:  232\n",
            "===============================================\n",
            "length of predicted classes:  39\n",
            "length of targets:  39\n",
            "length of data:  39\n",
            "===============================================\n",
            "length of predicted classes:  73\n",
            "length of targets:  73\n",
            "length of data:  73\n",
            "===============================================\n",
            "length of predicted classes:  127\n",
            "length of targets:  127\n",
            "length of data:  127\n",
            "===============================================\n",
            "length of predicted classes:  208\n",
            "length of targets:  208\n",
            "length of data:  208\n",
            "===============================================\n",
            "length of predicted classes:  59\n",
            "length of targets:  59\n",
            "length of data:  59\n",
            "===============================================\n",
            "length of predicted classes:  166\n",
            "length of targets:  166\n",
            "length of data:  166\n",
            "===============================================\n",
            "length of predicted classes:  92\n",
            "length of targets:  92\n",
            "length of data:  92\n",
            "===============================================\n",
            "length of predicted classes:  23\n",
            "length of targets:  23\n",
            "length of data:  23\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  227\n",
            "length of targets:  227\n",
            "length of data:  227\n",
            "===============================================\n",
            "length of predicted classes:  90\n",
            "length of targets:  90\n",
            "length of data:  90\n",
            "===============================================\n",
            "length of predicted classes:  37\n",
            "length of targets:  37\n",
            "length of data:  37\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  75\n",
            "length of targets:  75\n",
            "length of data:  75\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  246\n",
            "length of targets:  246\n",
            "length of data:  246\n",
            "===============================================\n",
            "length of predicted classes:  283\n",
            "length of targets:  283\n",
            "length of data:  283\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  29\n",
            "length of targets:  29\n",
            "length of data:  29\n",
            "===============================================\n",
            "length of predicted classes:  8\n",
            "length of targets:  8\n",
            "length of data:  8\n",
            "===============================================\n",
            "length of predicted classes:  63\n",
            "length of targets:  63\n",
            "length of data:  63\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  152\n",
            "length of targets:  152\n",
            "length of data:  152\n",
            "===============================================\n",
            "length of predicted classes:  132\n",
            "length of targets:  132\n",
            "length of data:  132\n",
            "===============================================\n",
            "length of predicted classes:  69\n",
            "length of targets:  69\n",
            "length of data:  69\n",
            "===============================================\n",
            "length of predicted classes:  20\n",
            "length of targets:  20\n",
            "length of data:  20\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  76\n",
            "length of targets:  76\n",
            "length of data:  76\n",
            "===============================================\n",
            "length of predicted classes:  59\n",
            "length of targets:  59\n",
            "length of data:  59\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  185\n",
            "length of targets:  185\n",
            "length of data:  185\n",
            "===============================================\n",
            "length of predicted classes:  239\n",
            "length of targets:  239\n",
            "length of data:  239\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  131\n",
            "length of targets:  131\n",
            "length of data:  131\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  38\n",
            "length of targets:  38\n",
            "length of data:  38\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  22\n",
            "length of targets:  22\n",
            "length of data:  22\n",
            "===============================================\n",
            "length of predicted classes:  79\n",
            "length of targets:  79\n",
            "length of data:  79\n",
            "===============================================\n",
            "length of predicted classes:  55\n",
            "length of targets:  55\n",
            "length of data:  55\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  0\n",
            "length of targets:  0\n",
            "length of data:  0\n",
            "===============================================\n",
            "length of predicted classes:  40\n",
            "length of targets:  40\n",
            "length of data:  40\n",
            "===============================================\n",
            "length of predicted classes:  110\n",
            "length of targets:  110\n",
            "length of data:  110\n",
            "===============================================\n",
            "length of predicted classes:  177\n",
            "length of targets:  177\n",
            "length of data:  177\n",
            "===============================================\n",
            "length of predicted classes:  142\n",
            "length of targets:  142\n",
            "length of data:  142\n",
            "===============================================\n",
            "length of predicted classes:  230\n",
            "length of targets:  230\n",
            "length of data:  230\n",
            "===============================================\n",
            "length of predicted classes:  217\n",
            "length of targets:  217\n",
            "length of data:  217\n",
            "===============================================\n",
            "length of predicted classes:  219\n",
            "length of targets:  219\n",
            "length of data:  219\n",
            "===============================================\n",
            "length of predicted classes:  123\n",
            "length of targets:  123\n",
            "length of data:  123\n",
            "===============================================\n",
            "length of predicted classes:  55\n",
            "length of targets:  55\n",
            "length of data:  55\n",
            "===============================================\n",
            "length of predicted classes:  264\n",
            "length of targets:  264\n",
            "length of data:  264\n",
            "===============================================\n",
            "length of predicted classes:  61\n",
            "length of targets:  61\n",
            "length of data:  61\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  166\n",
            "length of targets:  166\n",
            "length of data:  166\n",
            "===============================================\n",
            "length of predicted classes:  160\n",
            "length of targets:  160\n",
            "length of data:  160\n",
            "===============================================\n",
            "length of predicted classes:  282\n",
            "length of targets:  282\n",
            "length of data:  282\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  155\n",
            "length of targets:  155\n",
            "length of data:  155\n",
            "===============================================\n",
            "length of predicted classes:  48\n",
            "length of targets:  48\n",
            "length of data:  48\n",
            "===============================================\n",
            "length of predicted classes:  167\n",
            "length of targets:  167\n",
            "length of data:  167\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  265\n",
            "length of targets:  265\n",
            "length of data:  265\n",
            "===============================================\n",
            "length of predicted classes:  238\n",
            "length of targets:  238\n",
            "length of data:  238\n",
            "===============================================\n",
            "length of predicted classes:  226\n",
            "length of targets:  226\n",
            "length of data:  226\n",
            "===============================================\n",
            "length of predicted classes:  84\n",
            "length of targets:  84\n",
            "length of data:  84\n",
            "===============================================\n",
            "length of predicted classes:  95\n",
            "length of targets:  95\n",
            "length of data:  95\n",
            "===============================================\n",
            "length of predicted classes:  111\n",
            "length of targets:  111\n",
            "length of data:  111\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  85\n",
            "length of targets:  85\n",
            "length of data:  85\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  40\n",
            "length of targets:  40\n",
            "length of data:  40\n",
            "===============================================\n",
            "length of predicted classes:  30\n",
            "length of targets:  30\n",
            "length of data:  30\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  70\n",
            "length of targets:  70\n",
            "length of data:  70\n",
            "===============================================\n",
            "length of predicted classes:  6\n",
            "length of targets:  6\n",
            "length of data:  6\n",
            "===============================================\n",
            "length of predicted classes:  17\n",
            "length of targets:  17\n",
            "length of data:  17\n",
            "===============================================\n",
            "length of predicted classes:  56\n",
            "length of targets:  56\n",
            "length of data:  56\n",
            "===============================================\n",
            "length of predicted classes:  84\n",
            "length of targets:  84\n",
            "length of data:  84\n",
            "===============================================\n",
            "length of predicted classes:  50\n",
            "length of targets:  50\n",
            "length of data:  50\n",
            "===============================================\n",
            "length of predicted classes:  116\n",
            "length of targets:  116\n",
            "length of data:  116\n",
            "===============================================\n",
            "length of predicted classes:  53\n",
            "length of targets:  53\n",
            "length of data:  53\n",
            "===============================================\n",
            "length of predicted classes:  45\n",
            "length of targets:  45\n",
            "length of data:  45\n",
            "===============================================\n",
            "length of predicted classes:  33\n",
            "length of targets:  33\n",
            "length of data:  33\n",
            "===============================================\n",
            "length of predicted classes:  34\n",
            "length of targets:  34\n",
            "length of data:  34\n",
            "===============================================\n",
            "length of predicted classes:  140\n",
            "length of targets:  140\n",
            "length of data:  140\n",
            "===============================================\n",
            "length of predicted classes:  175\n",
            "length of targets:  175\n",
            "length of data:  175\n",
            "===============================================\n",
            "length of predicted classes:  142\n",
            "length of targets:  142\n",
            "length of data:  142\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  94\n",
            "length of targets:  94\n",
            "length of data:  94\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  79\n",
            "length of targets:  79\n",
            "length of data:  79\n",
            "===============================================\n",
            "length of predicted classes:  48\n",
            "length of targets:  48\n",
            "length of data:  48\n",
            "===============================================\n",
            "length of predicted classes:  148\n",
            "length of targets:  148\n",
            "length of data:  148\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  20\n",
            "length of targets:  20\n",
            "length of data:  20\n",
            "===============================================\n",
            "length of predicted classes:  204\n",
            "length of targets:  204\n",
            "length of data:  204\n",
            "===============================================\n",
            "length of predicted classes:  170\n",
            "length of targets:  170\n",
            "length of data:  170\n",
            "===============================================\n",
            "length of predicted classes:  156\n",
            "length of targets:  156\n",
            "length of data:  156\n",
            "===============================================\n",
            "length of predicted classes:  26\n",
            "length of targets:  26\n",
            "length of data:  26\n",
            "===============================================\n",
            "length of predicted classes:  145\n",
            "length of targets:  145\n",
            "length of data:  145\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  109\n",
            "length of targets:  109\n",
            "length of data:  109\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  199\n",
            "length of targets:  199\n",
            "length of data:  199\n",
            "===============================================\n",
            "length of predicted classes:  127\n",
            "length of targets:  127\n",
            "length of data:  127\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  171\n",
            "length of targets:  171\n",
            "length of data:  171\n",
            "===============================================\n",
            "length of predicted classes:  56\n",
            "length of targets:  56\n",
            "length of data:  56\n",
            "===============================================\n",
            "length of predicted classes:  140\n",
            "length of targets:  140\n",
            "length of data:  140\n",
            "===============================================\n",
            "length of predicted classes:  117\n",
            "length of targets:  117\n",
            "length of data:  117\n",
            "===============================================\n",
            "length of predicted classes:  49\n",
            "length of targets:  49\n",
            "length of data:  49\n",
            "===============================================\n",
            "length of predicted classes:  142\n",
            "length of targets:  142\n",
            "length of data:  142\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  279\n",
            "length of targets:  279\n",
            "length of data:  279\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  83\n",
            "length of targets:  83\n",
            "length of data:  83\n",
            "===============================================\n",
            "length of predicted classes:  23\n",
            "length of targets:  23\n",
            "length of data:  23\n",
            "===============================================\n",
            "length of predicted classes:  38\n",
            "length of targets:  38\n",
            "length of data:  38\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  68\n",
            "length of targets:  68\n",
            "length of data:  68\n",
            "===============================================\n",
            "length of predicted classes:  88\n",
            "length of targets:  88\n",
            "length of data:  88\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  102\n",
            "length of targets:  102\n",
            "length of data:  102\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  182\n",
            "length of targets:  182\n",
            "length of data:  182\n",
            "===============================================\n",
            "length of predicted classes:  23\n",
            "length of targets:  23\n",
            "length of data:  23\n",
            "===============================================\n",
            "length of predicted classes:  149\n",
            "length of targets:  149\n",
            "length of data:  149\n",
            "===============================================\n",
            "length of predicted classes:  36\n",
            "length of targets:  36\n",
            "length of data:  36\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  198\n",
            "length of targets:  198\n",
            "length of data:  198\n",
            "===============================================\n",
            "length of predicted classes:  141\n",
            "length of targets:  141\n",
            "length of data:  141\n",
            "===============================================\n",
            "length of predicted classes:  94\n",
            "length of targets:  94\n",
            "length of data:  94\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  112\n",
            "length of targets:  112\n",
            "length of data:  112\n",
            "===============================================\n",
            "length of predicted classes:  220\n",
            "length of targets:  220\n",
            "length of data:  220\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  85\n",
            "length of targets:  85\n",
            "length of data:  85\n",
            "===============================================\n",
            "length of predicted classes:  106\n",
            "length of targets:  106\n",
            "length of data:  106\n",
            "===============================================\n",
            "length of predicted classes:  42\n",
            "length of targets:  42\n",
            "length of data:  42\n",
            "===============================================\n",
            "length of predicted classes:  20\n",
            "length of targets:  20\n",
            "length of data:  20\n",
            "===============================================\n",
            "length of predicted classes:  67\n",
            "length of targets:  67\n",
            "length of data:  67\n",
            "===============================================\n",
            "length of predicted classes:  150\n",
            "length of targets:  150\n",
            "length of data:  150\n",
            "===============================================\n",
            "length of predicted classes:  32\n",
            "length of targets:  32\n",
            "length of data:  32\n",
            "===============================================\n",
            "length of predicted classes:  107\n",
            "length of targets:  107\n",
            "length of data:  107\n",
            "===============================================\n",
            "length of predicted classes:  31\n",
            "length of targets:  31\n",
            "length of data:  31\n",
            "===============================================\n",
            "length of predicted classes:  79\n",
            "length of targets:  79\n",
            "length of data:  79\n",
            "===============================================\n",
            "length of predicted classes:  74\n",
            "length of targets:  74\n",
            "length of data:  74\n",
            "===============================================\n",
            "length of predicted classes:  92\n",
            "length of targets:  92\n",
            "length of data:  92\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  112\n",
            "length of targets:  112\n",
            "length of data:  112\n",
            "===============================================\n",
            "length of predicted classes:  203\n",
            "length of targets:  203\n",
            "length of data:  203\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  54\n",
            "length of targets:  54\n",
            "length of data:  54\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  178\n",
            "length of targets:  178\n",
            "length of data:  178\n",
            "===============================================\n",
            "length of predicted classes:  66\n",
            "length of targets:  66\n",
            "length of data:  66\n",
            "===============================================\n",
            "length of predicted classes:  90\n",
            "length of targets:  90\n",
            "length of data:  90\n",
            "===============================================\n",
            "length of predicted classes:  155\n",
            "length of targets:  155\n",
            "length of data:  155\n",
            "===============================================\n",
            "length of predicted classes:  120\n",
            "length of targets:  120\n",
            "length of data:  120\n",
            "===============================================\n",
            "length of predicted classes:  100\n",
            "length of targets:  100\n",
            "length of data:  100\n",
            "===============================================\n",
            "length of predicted classes:  238\n",
            "length of targets:  238\n",
            "length of data:  238\n",
            "===============================================\n",
            "length of predicted classes:  104\n",
            "length of targets:  104\n",
            "length of data:  104\n",
            "===============================================\n",
            "length of predicted classes:  106\n",
            "length of targets:  106\n",
            "length of data:  106\n",
            "===============================================\n",
            "length of predicted classes:  100\n",
            "length of targets:  100\n",
            "length of data:  100\n",
            "===============================================\n",
            "length of predicted classes:  64\n",
            "length of targets:  64\n",
            "length of data:  64\n",
            "===============================================\n",
            "length of predicted classes:  29\n",
            "length of targets:  29\n",
            "length of data:  29\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  293\n",
            "length of targets:  293\n",
            "length of data:  293\n",
            "===============================================\n",
            "length of predicted classes:  50\n",
            "length of targets:  50\n",
            "length of data:  50\n",
            "===============================================\n",
            "length of predicted classes:  44\n",
            "length of targets:  44\n",
            "length of data:  44\n",
            "===============================================\n",
            "length of predicted classes:  57\n",
            "length of targets:  57\n",
            "length of data:  57\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  295\n",
            "length of targets:  295\n",
            "length of data:  295\n",
            "===============================================\n",
            "length of predicted classes:  166\n",
            "length of targets:  166\n",
            "length of data:  166\n",
            "===============================================\n",
            "length of predicted classes:  72\n",
            "length of targets:  72\n",
            "length of data:  72\n",
            "===============================================\n",
            "length of predicted classes:  145\n",
            "length of targets:  145\n",
            "length of data:  145\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  42\n",
            "length of targets:  42\n",
            "length of data:  42\n",
            "===============================================\n",
            "length of predicted classes:  222\n",
            "length of targets:  222\n",
            "length of data:  222\n",
            "===============================================\n",
            "length of predicted classes:  22\n",
            "length of targets:  22\n",
            "length of data:  22\n",
            "===============================================\n",
            "length of predicted classes:  15\n",
            "length of targets:  15\n",
            "length of data:  15\n",
            "===============================================\n",
            "length of predicted classes:  17\n",
            "length of targets:  17\n",
            "length of data:  17\n",
            "===============================================\n",
            "length of predicted classes:  13\n",
            "length of targets:  13\n",
            "length of data:  13\n",
            "===============================================\n",
            "length of predicted classes:  14\n",
            "length of targets:  14\n",
            "length of data:  14\n",
            "===============================================\n",
            "length of predicted classes:  74\n",
            "length of targets:  74\n",
            "length of data:  74\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  24\n",
            "length of targets:  24\n",
            "length of data:  24\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  182\n",
            "length of targets:  182\n",
            "length of data:  182\n",
            "===============================================\n",
            "length of predicted classes:  84\n",
            "length of targets:  84\n",
            "length of data:  84\n",
            "===============================================\n",
            "length of predicted classes:  119\n",
            "length of targets:  119\n",
            "length of data:  119\n",
            "===============================================\n",
            "length of predicted classes:  230\n",
            "length of targets:  230\n",
            "length of data:  230\n",
            "===============================================\n",
            "length of predicted classes:  101\n",
            "length of targets:  101\n",
            "length of data:  101\n",
            "===============================================\n",
            "length of predicted classes:  181\n",
            "length of targets:  181\n",
            "length of data:  181\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  25\n",
            "length of targets:  25\n",
            "length of data:  25\n",
            "===============================================\n",
            "length of predicted classes:  243\n",
            "length of targets:  243\n",
            "length of data:  243\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  93\n",
            "length of targets:  93\n",
            "length of data:  93\n",
            "===============================================\n",
            "length of predicted classes:  72\n",
            "length of targets:  72\n",
            "length of data:  72\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  38\n",
            "length of targets:  38\n",
            "length of data:  38\n",
            "===============================================\n",
            "length of predicted classes:  208\n",
            "length of targets:  208\n",
            "length of data:  208\n",
            "===============================================\n",
            "length of predicted classes:  230\n",
            "length of targets:  230\n",
            "length of data:  230\n",
            "===============================================\n",
            "length of predicted classes:  33\n",
            "length of targets:  33\n",
            "length of data:  33\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  211\n",
            "length of targets:  211\n",
            "length of data:  211\n",
            "===============================================\n",
            "length of predicted classes:  190\n",
            "length of targets:  190\n",
            "length of data:  190\n",
            "===============================================\n",
            "length of predicted classes:  105\n",
            "length of targets:  105\n",
            "length of data:  105\n",
            "===============================================\n",
            "length of predicted classes:  109\n",
            "length of targets:  109\n",
            "length of data:  109\n",
            "===============================================\n",
            "length of predicted classes:  25\n",
            "length of targets:  25\n",
            "length of data:  25\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  134\n",
            "length of targets:  134\n",
            "length of data:  134\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  273\n",
            "length of targets:  273\n",
            "length of data:  273\n",
            "===============================================\n",
            "length of predicted classes:  127\n",
            "length of targets:  127\n",
            "length of data:  127\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  4\n",
            "length of targets:  4\n",
            "length of data:  4\n",
            "===============================================\n",
            "length of predicted classes:  92\n",
            "length of targets:  92\n",
            "length of data:  92\n",
            "===============================================\n",
            "length of predicted classes:  215\n",
            "length of targets:  215\n",
            "length of data:  215\n",
            "===============================================\n",
            "length of predicted classes:  210\n",
            "length of targets:  210\n",
            "length of data:  210\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  93\n",
            "length of targets:  93\n",
            "length of data:  93\n",
            "===============================================\n",
            "length of predicted classes:  96\n",
            "length of targets:  96\n",
            "length of data:  96\n",
            "===============================================\n",
            "length of predicted classes:  233\n",
            "length of targets:  233\n",
            "length of data:  233\n",
            "===============================================\n",
            "length of predicted classes:  116\n",
            "length of targets:  116\n",
            "length of data:  116\n",
            "===============================================\n",
            "length of predicted classes:  80\n",
            "length of targets:  80\n",
            "length of data:  80\n",
            "===============================================\n",
            "length of predicted classes:  86\n",
            "length of targets:  86\n",
            "length of data:  86\n",
            "===============================================\n",
            "length of predicted classes:  89\n",
            "length of targets:  89\n",
            "length of data:  89\n",
            "===============================================\n",
            "length of predicted classes:  75\n",
            "length of targets:  75\n",
            "length of data:  75\n",
            "===============================================\n",
            "length of predicted classes:  172\n",
            "length of targets:  172\n",
            "length of data:  172\n",
            "===============================================\n",
            "length of predicted classes:  146\n",
            "length of targets:  146\n",
            "length of data:  146\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  17\n",
            "length of targets:  17\n",
            "length of data:  17\n",
            "===============================================\n",
            "length of predicted classes:  7\n",
            "length of targets:  7\n",
            "length of data:  7\n",
            "===============================================\n",
            "length of predicted classes:  115\n",
            "length of targets:  115\n",
            "length of data:  115\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  259\n",
            "length of targets:  259\n",
            "length of data:  259\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  150\n",
            "length of targets:  150\n",
            "length of data:  150\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  18\n",
            "length of targets:  18\n",
            "length of data:  18\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  6\n",
            "length of targets:  6\n",
            "length of data:  6\n",
            "===============================================\n",
            "length of predicted classes:  81\n",
            "length of targets:  81\n",
            "length of data:  81\n",
            "===============================================\n",
            "length of predicted classes:  208\n",
            "length of targets:  208\n",
            "length of data:  208\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  24\n",
            "length of targets:  24\n",
            "length of data:  24\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  28\n",
            "length of targets:  28\n",
            "length of data:  28\n",
            "===============================================\n",
            "length of predicted classes:  96\n",
            "length of targets:  96\n",
            "length of data:  96\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  33\n",
            "length of targets:  33\n",
            "length of data:  33\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  18\n",
            "length of targets:  18\n",
            "length of data:  18\n",
            "===============================================\n",
            "length of predicted classes:  136\n",
            "length of targets:  136\n",
            "length of data:  136\n",
            "===============================================\n",
            "length of predicted classes:  78\n",
            "length of targets:  78\n",
            "length of data:  78\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  143\n",
            "length of targets:  143\n",
            "length of data:  143\n",
            "===============================================\n",
            "length of predicted classes:  43\n",
            "length of targets:  43\n",
            "length of data:  43\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  33\n",
            "length of targets:  33\n",
            "length of data:  33\n",
            "===============================================\n",
            "length of predicted classes:  12\n",
            "length of targets:  12\n",
            "length of data:  12\n",
            "===============================================\n",
            "length of predicted classes:  80\n",
            "length of targets:  80\n",
            "length of data:  80\n",
            "===============================================\n",
            "length of predicted classes:  64\n",
            "length of targets:  64\n",
            "length of data:  64\n",
            "===============================================\n",
            "length of predicted classes:  72\n",
            "length of targets:  72\n",
            "length of data:  72\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  91\n",
            "length of targets:  91\n",
            "length of data:  91\n",
            "===============================================\n",
            "length of predicted classes:  79\n",
            "length of targets:  79\n",
            "length of data:  79\n",
            "===============================================\n",
            "length of predicted classes:  194\n",
            "length of targets:  194\n",
            "length of data:  194\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  243\n",
            "length of targets:  243\n",
            "length of data:  243\n",
            "===============================================\n",
            "length of predicted classes:  24\n",
            "length of targets:  24\n",
            "length of data:  24\n",
            "===============================================\n",
            "length of predicted classes:  292\n",
            "length of targets:  292\n",
            "length of data:  292\n",
            "===============================================\n",
            "length of predicted classes:  73\n",
            "length of targets:  73\n",
            "length of data:  73\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  68\n",
            "length of targets:  68\n",
            "length of data:  68\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  70\n",
            "length of targets:  70\n",
            "length of data:  70\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  173\n",
            "length of targets:  173\n",
            "length of data:  173\n",
            "===============================================\n",
            "length of predicted classes:  22\n",
            "length of targets:  22\n",
            "length of data:  22\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  84\n",
            "length of targets:  84\n",
            "length of data:  84\n",
            "===============================================\n",
            "length of predicted classes:  73\n",
            "length of targets:  73\n",
            "length of data:  73\n",
            "===============================================\n",
            "length of predicted classes:  125\n",
            "length of targets:  125\n",
            "length of data:  125\n",
            "===============================================\n",
            "length of predicted classes:  55\n",
            "length of targets:  55\n",
            "length of data:  55\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  296\n",
            "length of targets:  296\n",
            "length of data:  296\n",
            "===============================================\n",
            "length of predicted classes:  78\n",
            "length of targets:  78\n",
            "length of data:  78\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  8\n",
            "length of targets:  8\n",
            "length of data:  8\n",
            "===============================================\n",
            "length of predicted classes:  98\n",
            "length of targets:  98\n",
            "length of data:  98\n",
            "===============================================\n",
            "length of predicted classes:  178\n",
            "length of targets:  178\n",
            "length of data:  178\n",
            "===============================================\n",
            "length of predicted classes:  145\n",
            "length of targets:  145\n",
            "length of data:  145\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  193\n",
            "length of targets:  193\n",
            "length of data:  193\n",
            "===============================================\n",
            "length of predicted classes:  43\n",
            "length of targets:  43\n",
            "length of data:  43\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  66\n",
            "length of targets:  66\n",
            "length of data:  66\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  220\n",
            "length of targets:  220\n",
            "length of data:  220\n",
            "===============================================\n",
            "length of predicted classes:  251\n",
            "length of targets:  251\n",
            "length of data:  251\n",
            "===============================================\n",
            "length of predicted classes:  12\n",
            "length of targets:  12\n",
            "length of data:  12\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  133\n",
            "length of targets:  133\n",
            "length of data:  133\n",
            "===============================================\n",
            "length of predicted classes:  11\n",
            "length of targets:  11\n",
            "length of data:  11\n",
            "===============================================\n",
            "length of predicted classes:  64\n",
            "length of targets:  64\n",
            "length of data:  64\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  131\n",
            "length of targets:  131\n",
            "length of data:  131\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  204\n",
            "length of targets:  204\n",
            "length of data:  204\n",
            "===============================================\n",
            "length of predicted classes:  100\n",
            "length of targets:  100\n",
            "length of data:  100\n",
            "===============================================\n",
            "length of predicted classes:  48\n",
            "length of targets:  48\n",
            "length of data:  48\n",
            "===============================================\n",
            "length of predicted classes:  237\n",
            "length of targets:  237\n",
            "length of data:  237\n",
            "===============================================\n",
            "length of predicted classes:  58\n",
            "length of targets:  58\n",
            "length of data:  58\n",
            "===============================================\n",
            "length of predicted classes:  64\n",
            "length of targets:  64\n",
            "length of data:  64\n",
            "===============================================\n",
            "length of predicted classes:  252\n",
            "length of targets:  252\n",
            "length of data:  252\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  72\n",
            "length of targets:  72\n",
            "length of data:  72\n",
            "===============================================\n",
            "length of predicted classes:  63\n",
            "length of targets:  63\n",
            "length of data:  63\n",
            "===============================================\n",
            "length of predicted classes:  216\n",
            "length of targets:  216\n",
            "length of data:  216\n",
            "===============================================\n",
            "length of predicted classes:  67\n",
            "length of targets:  67\n",
            "length of data:  67\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  132\n",
            "length of targets:  132\n",
            "length of data:  132\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  250\n",
            "length of targets:  250\n",
            "length of data:  250\n",
            "===============================================\n",
            "length of predicted classes:  54\n",
            "length of targets:  54\n",
            "length of data:  54\n",
            "===============================================\n",
            "length of predicted classes:  145\n",
            "length of targets:  145\n",
            "length of data:  145\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  227\n",
            "length of targets:  227\n",
            "length of data:  227\n",
            "===============================================\n",
            "length of predicted classes:  92\n",
            "length of targets:  92\n",
            "length of data:  92\n",
            "===============================================\n",
            "length of predicted classes:  157\n",
            "length of targets:  157\n",
            "length of data:  157\n",
            "===============================================\n",
            "length of predicted classes:  34\n",
            "length of targets:  34\n",
            "length of data:  34\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  179\n",
            "length of targets:  179\n",
            "length of data:  179\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  228\n",
            "length of targets:  228\n",
            "length of data:  228\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  147\n",
            "length of targets:  147\n",
            "length of data:  147\n",
            "===============================================\n",
            "length of predicted classes:  53\n",
            "length of targets:  53\n",
            "length of data:  53\n",
            "===============================================\n",
            "length of predicted classes:  99\n",
            "length of targets:  99\n",
            "length of data:  99\n",
            "===============================================\n",
            "length of predicted classes:  129\n",
            "length of targets:  129\n",
            "length of data:  129\n",
            "===============================================\n",
            "length of predicted classes:  184\n",
            "length of targets:  184\n",
            "length of data:  184\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  108\n",
            "length of targets:  108\n",
            "length of data:  108\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  24\n",
            "length of targets:  24\n",
            "length of data:  24\n",
            "===============================================\n",
            "length of predicted classes:  153\n",
            "length of targets:  153\n",
            "length of data:  153\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  112\n",
            "length of targets:  112\n",
            "length of data:  112\n",
            "===============================================\n",
            "length of predicted classes:  25\n",
            "length of targets:  25\n",
            "length of data:  25\n",
            "===============================================\n",
            "length of predicted classes:  42\n",
            "length of targets:  42\n",
            "length of data:  42\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  98\n",
            "length of targets:  98\n",
            "length of data:  98\n",
            "===============================================\n",
            "length of predicted classes:  204\n",
            "length of targets:  204\n",
            "length of data:  204\n",
            "===============================================\n",
            "length of predicted classes:  187\n",
            "length of targets:  187\n",
            "length of data:  187\n",
            "===============================================\n",
            "length of predicted classes:  260\n",
            "length of targets:  260\n",
            "length of data:  260\n",
            "===============================================\n",
            "length of predicted classes:  96\n",
            "length of targets:  96\n",
            "length of data:  96\n",
            "===============================================\n",
            "length of predicted classes:  38\n",
            "length of targets:  38\n",
            "length of data:  38\n",
            "===============================================\n",
            "length of predicted classes:  219\n",
            "length of targets:  219\n",
            "length of data:  219\n",
            "===============================================\n",
            "length of predicted classes:  106\n",
            "length of targets:  106\n",
            "length of data:  106\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  70\n",
            "length of targets:  70\n",
            "length of data:  70\n",
            "===============================================\n",
            "length of predicted classes:  152\n",
            "length of targets:  152\n",
            "length of data:  152\n",
            "===============================================\n",
            "length of predicted classes:  177\n",
            "length of targets:  177\n",
            "length of data:  177\n",
            "===============================================\n",
            "length of predicted classes:  55\n",
            "length of targets:  55\n",
            "length of data:  55\n",
            "===============================================\n",
            "length of predicted classes:  27\n",
            "length of targets:  27\n",
            "length of data:  27\n",
            "===============================================\n",
            "length of predicted classes:  86\n",
            "length of targets:  86\n",
            "length of data:  86\n",
            "===============================================\n",
            "length of predicted classes:  207\n",
            "length of targets:  207\n",
            "length of data:  207\n",
            "===============================================\n",
            "length of predicted classes:  32\n",
            "length of targets:  32\n",
            "length of data:  32\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  151\n",
            "length of targets:  151\n",
            "length of data:  151\n",
            "===============================================\n",
            "length of predicted classes:  122\n",
            "length of targets:  122\n",
            "length of data:  122\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  178\n",
            "length of targets:  178\n",
            "length of data:  178\n",
            "===============================================\n",
            "length of predicted classes:  42\n",
            "length of targets:  42\n",
            "length of data:  42\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  123\n",
            "length of targets:  123\n",
            "length of data:  123\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  235\n",
            "length of targets:  235\n",
            "length of data:  235\n",
            "===============================================\n",
            "length of predicted classes:  179\n",
            "length of targets:  179\n",
            "length of data:  179\n",
            "===============================================\n",
            "length of predicted classes:  130\n",
            "length of targets:  130\n",
            "length of data:  130\n",
            "===============================================\n",
            "length of predicted classes:  153\n",
            "length of targets:  153\n",
            "length of data:  153\n",
            "===============================================\n",
            "length of predicted classes:  126\n",
            "length of targets:  126\n",
            "length of data:  126\n",
            "===============================================\n",
            "length of predicted classes:  9\n",
            "length of targets:  9\n",
            "length of data:  9\n",
            "===============================================\n",
            "length of predicted classes:  217\n",
            "length of targets:  217\n",
            "length of data:  217\n",
            "===============================================\n",
            "length of predicted classes:  47\n",
            "length of targets:  47\n",
            "length of data:  47\n",
            "===============================================\n",
            "length of predicted classes:  72\n",
            "length of targets:  72\n",
            "length of data:  72\n",
            "===============================================\n",
            "length of predicted classes:  265\n",
            "length of targets:  265\n",
            "length of data:  265\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  209\n",
            "length of targets:  209\n",
            "length of data:  209\n",
            "===============================================\n",
            "length of predicted classes:  140\n",
            "length of targets:  140\n",
            "length of data:  140\n",
            "===============================================\n",
            "length of predicted classes:  257\n",
            "length of targets:  257\n",
            "length of data:  257\n",
            "===============================================\n",
            "length of predicted classes:  114\n",
            "length of targets:  114\n",
            "length of data:  114\n",
            "===============================================\n",
            "length of predicted classes:  146\n",
            "length of targets:  146\n",
            "length of data:  146\n",
            "===============================================\n",
            "length of predicted classes:  179\n",
            "length of targets:  179\n",
            "length of data:  179\n",
            "===============================================\n",
            "length of predicted classes:  290\n",
            "length of targets:  290\n",
            "length of data:  290\n",
            "===============================================\n",
            "length of predicted classes:  86\n",
            "length of targets:  86\n",
            "length of data:  86\n",
            "===============================================\n",
            "length of predicted classes:  247\n",
            "length of targets:  247\n",
            "length of data:  247\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  189\n",
            "length of targets:  189\n",
            "length of data:  189\n",
            "===============================================\n",
            "length of predicted classes:  185\n",
            "length of targets:  185\n",
            "length of data:  185\n",
            "===============================================\n",
            "length of predicted classes:  215\n",
            "length of targets:  215\n",
            "length of data:  215\n",
            "===============================================\n",
            "length of predicted classes:  139\n",
            "length of targets:  139\n",
            "length of data:  139\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  32\n",
            "length of targets:  32\n",
            "length of data:  32\n",
            "===============================================\n",
            "length of predicted classes:  256\n",
            "length of targets:  256\n",
            "length of data:  256\n",
            "===============================================\n",
            "length of predicted classes:  6\n",
            "length of targets:  6\n",
            "length of data:  6\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  45\n",
            "length of targets:  45\n",
            "length of data:  45\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  161\n",
            "length of targets:  161\n",
            "length of data:  161\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  162\n",
            "length of targets:  162\n",
            "length of data:  162\n",
            "===============================================\n",
            "length of predicted classes:  160\n",
            "length of targets:  160\n",
            "length of data:  160\n",
            "===============================================\n",
            "length of predicted classes:  145\n",
            "length of targets:  145\n",
            "length of data:  145\n",
            "===============================================\n",
            "length of predicted classes:  159\n",
            "length of targets:  159\n",
            "length of data:  159\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  87\n",
            "length of targets:  87\n",
            "length of data:  87\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  34\n",
            "length of targets:  34\n",
            "length of data:  34\n",
            "===============================================\n",
            "length of predicted classes:  238\n",
            "length of targets:  238\n",
            "length of data:  238\n",
            "===============================================\n",
            "length of predicted classes:  247\n",
            "length of targets:  247\n",
            "length of data:  247\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  257\n",
            "length of targets:  257\n",
            "length of data:  257\n",
            "===============================================\n",
            "length of predicted classes:  154\n",
            "length of targets:  154\n",
            "length of data:  154\n",
            "===============================================\n",
            "length of predicted classes:  24\n",
            "length of targets:  24\n",
            "length of data:  24\n",
            "===============================================\n",
            "length of predicted classes:  154\n",
            "length of targets:  154\n",
            "length of data:  154\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  132\n",
            "length of targets:  132\n",
            "length of data:  132\n",
            "===============================================\n",
            "length of predicted classes:  96\n",
            "length of targets:  96\n",
            "length of data:  96\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  53\n",
            "length of targets:  53\n",
            "length of data:  53\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  98\n",
            "length of targets:  98\n",
            "length of data:  98\n",
            "===============================================\n",
            "length of predicted classes:  219\n",
            "length of targets:  219\n",
            "length of data:  219\n",
            "===============================================\n",
            "length of predicted classes:  234\n",
            "length of targets:  234\n",
            "length of data:  234\n",
            "===============================================\n",
            "length of predicted classes:  54\n",
            "length of targets:  54\n",
            "length of data:  54\n",
            "===============================================\n",
            "length of predicted classes:  2\n",
            "length of targets:  2\n",
            "length of data:  2\n",
            "===============================================\n",
            "length of predicted classes:  29\n",
            "length of targets:  29\n",
            "length of data:  29\n",
            "===============================================\n",
            "length of predicted classes:  282\n",
            "length of targets:  282\n",
            "length of data:  282\n",
            "===============================================\n",
            "length of predicted classes:  183\n",
            "length of targets:  183\n",
            "length of data:  183\n",
            "===============================================\n",
            "length of predicted classes:  24\n",
            "length of targets:  24\n",
            "length of data:  24\n",
            "===============================================\n",
            "length of predicted classes:  146\n",
            "length of targets:  146\n",
            "length of data:  146\n",
            "===============================================\n",
            "length of predicted classes:  138\n",
            "length of targets:  138\n",
            "length of data:  138\n",
            "===============================================\n",
            "length of predicted classes:  93\n",
            "length of targets:  93\n",
            "length of data:  93\n",
            "===============================================\n",
            "length of predicted classes:  195\n",
            "length of targets:  195\n",
            "length of data:  195\n",
            "===============================================\n",
            "length of predicted classes:  235\n",
            "length of targets:  235\n",
            "length of data:  235\n",
            "===============================================\n",
            "length of predicted classes:  268\n",
            "length of targets:  268\n",
            "length of data:  268\n",
            "===============================================\n",
            "length of predicted classes:  44\n",
            "length of targets:  44\n",
            "length of data:  44\n",
            "===============================================\n",
            "length of predicted classes:  69\n",
            "length of targets:  69\n",
            "length of data:  69\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  60\n",
            "length of targets:  60\n",
            "length of data:  60\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  31\n",
            "length of targets:  31\n",
            "length of data:  31\n",
            "===============================================\n",
            "length of predicted classes:  201\n",
            "length of targets:  201\n",
            "length of data:  201\n",
            "===============================================\n",
            "length of predicted classes:  31\n",
            "length of targets:  31\n",
            "length of data:  31\n",
            "===============================================\n",
            "length of predicted classes:  82\n",
            "length of targets:  82\n",
            "length of data:  82\n",
            "===============================================\n",
            "length of predicted classes:  40\n",
            "length of targets:  40\n",
            "length of data:  40\n",
            "===============================================\n",
            "length of predicted classes:  211\n",
            "length of targets:  211\n",
            "length of data:  211\n",
            "===============================================\n",
            "length of predicted classes:  291\n",
            "length of targets:  291\n",
            "length of data:  291\n",
            "===============================================\n",
            "length of predicted classes:  35\n",
            "length of targets:  35\n",
            "length of data:  35\n",
            "===============================================\n",
            "length of predicted classes:  88\n",
            "length of targets:  88\n",
            "length of data:  88\n",
            "===============================================\n",
            "length of predicted classes:  293\n",
            "length of targets:  293\n",
            "length of data:  293\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  122\n",
            "length of targets:  122\n",
            "length of data:  122\n",
            "===============================================\n",
            "length of predicted classes:  43\n",
            "length of targets:  43\n",
            "length of data:  43\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  43\n",
            "length of targets:  43\n",
            "length of data:  43\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  224\n",
            "length of targets:  224\n",
            "length of data:  224\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  109\n",
            "length of targets:  109\n",
            "length of data:  109\n",
            "===============================================\n",
            "length of predicted classes:  155\n",
            "length of targets:  155\n",
            "length of data:  155\n",
            "===============================================\n",
            "length of predicted classes:  41\n",
            "length of targets:  41\n",
            "length of data:  41\n",
            "===============================================\n",
            "length of predicted classes:  5\n",
            "length of targets:  5\n",
            "length of data:  5\n",
            "===============================================\n",
            "length of predicted classes:  206\n",
            "length of targets:  206\n",
            "length of data:  206\n",
            "===============================================\n",
            "length of predicted classes:  70\n",
            "length of targets:  70\n",
            "length of data:  70\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  81\n",
            "length of targets:  81\n",
            "length of data:  81\n",
            "===============================================\n",
            "length of predicted classes:  39\n",
            "length of targets:  39\n",
            "length of data:  39\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  120\n",
            "length of targets:  120\n",
            "length of data:  120\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  161\n",
            "length of targets:  161\n",
            "length of data:  161\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  71\n",
            "length of targets:  71\n",
            "length of data:  71\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  80\n",
            "length of targets:  80\n",
            "length of data:  80\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  115\n",
            "length of targets:  115\n",
            "length of data:  115\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  51\n",
            "length of targets:  51\n",
            "length of data:  51\n",
            "===============================================\n",
            "length of predicted classes:  37\n",
            "length of targets:  37\n",
            "length of data:  37\n",
            "===============================================\n",
            "length of predicted classes:  82\n",
            "length of targets:  82\n",
            "length of data:  82\n",
            "===============================================\n",
            "length of predicted classes:  133\n",
            "length of targets:  133\n",
            "length of data:  133\n",
            "===============================================\n",
            "length of predicted classes:  119\n",
            "length of targets:  119\n",
            "length of data:  119\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  76\n",
            "length of targets:  76\n",
            "length of data:  76\n",
            "===============================================\n",
            "length of predicted classes:  142\n",
            "length of targets:  142\n",
            "length of data:  142\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  33\n",
            "length of targets:  33\n",
            "length of data:  33\n",
            "===============================================\n",
            "length of predicted classes:  274\n",
            "length of targets:  274\n",
            "length of data:  274\n",
            "===============================================\n",
            "length of predicted classes:  21\n",
            "length of targets:  21\n",
            "length of data:  21\n",
            "===============================================\n",
            "length of predicted classes:  59\n",
            "length of targets:  59\n",
            "length of data:  59\n",
            "===============================================\n",
            "length of predicted classes:  89\n",
            "length of targets:  89\n",
            "length of data:  89\n",
            "===============================================\n",
            "length of predicted classes:  38\n",
            "length of targets:  38\n",
            "length of data:  38\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  95\n",
            "length of targets:  95\n",
            "length of data:  95\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  159\n",
            "length of targets:  159\n",
            "length of data:  159\n",
            "===============================================\n",
            "length of predicted classes:  151\n",
            "length of targets:  151\n",
            "length of data:  151\n",
            "===============================================\n",
            "length of predicted classes:  180\n",
            "length of targets:  180\n",
            "length of data:  180\n",
            "===============================================\n",
            "length of predicted classes:  27\n",
            "length of targets:  27\n",
            "length of data:  27\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  110\n",
            "length of targets:  110\n",
            "length of data:  110\n",
            "===============================================\n",
            "length of predicted classes:  155\n",
            "length of targets:  155\n",
            "length of data:  155\n",
            "===============================================\n",
            "length of predicted classes:  152\n",
            "length of targets:  152\n",
            "length of data:  152\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  238\n",
            "length of targets:  238\n",
            "length of data:  238\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  115\n",
            "length of targets:  115\n",
            "length of data:  115\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  258\n",
            "length of targets:  258\n",
            "length of data:  258\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  180\n",
            "length of targets:  180\n",
            "length of data:  180\n",
            "===============================================\n",
            "length of predicted classes:  281\n",
            "length of targets:  281\n",
            "length of data:  281\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  93\n",
            "length of targets:  93\n",
            "length of data:  93\n",
            "===============================================\n",
            "length of predicted classes:  101\n",
            "length of targets:  101\n",
            "length of data:  101\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  27\n",
            "length of targets:  27\n",
            "length of data:  27\n",
            "===============================================\n",
            "length of predicted classes:  29\n",
            "length of targets:  29\n",
            "length of data:  29\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  93\n",
            "length of targets:  93\n",
            "length of data:  93\n",
            "===============================================\n",
            "length of predicted classes:  80\n",
            "length of targets:  80\n",
            "length of data:  80\n",
            "===============================================\n",
            "length of predicted classes:  27\n",
            "length of targets:  27\n",
            "length of data:  27\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  278\n",
            "length of targets:  278\n",
            "length of data:  278\n",
            "===============================================\n",
            "length of predicted classes:  25\n",
            "length of targets:  25\n",
            "length of data:  25\n",
            "===============================================\n",
            "length of predicted classes:  232\n",
            "length of targets:  232\n",
            "length of data:  232\n",
            "===============================================\n",
            "length of predicted classes:  67\n",
            "length of targets:  67\n",
            "length of data:  67\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  266\n",
            "length of targets:  266\n",
            "length of data:  266\n",
            "===============================================\n",
            "length of predicted classes:  30\n",
            "length of targets:  30\n",
            "length of data:  30\n",
            "===============================================\n",
            "length of predicted classes:  96\n",
            "length of targets:  96\n",
            "length of data:  96\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  128\n",
            "length of targets:  128\n",
            "length of data:  128\n",
            "===============================================\n",
            "length of predicted classes:  32\n",
            "length of targets:  32\n",
            "length of data:  32\n",
            "===============================================\n",
            "length of predicted classes:  58\n",
            "length of targets:  58\n",
            "length of data:  58\n",
            "===============================================\n",
            "length of predicted classes:  18\n",
            "length of targets:  18\n",
            "length of data:  18\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  107\n",
            "length of targets:  107\n",
            "length of data:  107\n",
            "===============================================\n",
            "length of predicted classes:  39\n",
            "length of targets:  39\n",
            "length of data:  39\n",
            "===============================================\n",
            "length of predicted classes:  28\n",
            "length of targets:  28\n",
            "length of data:  28\n",
            "===============================================\n",
            "length of predicted classes:  264\n",
            "length of targets:  264\n",
            "length of data:  264\n",
            "===============================================\n",
            "length of predicted classes:  121\n",
            "length of targets:  121\n",
            "length of data:  121\n",
            "===============================================\n",
            "length of predicted classes:  73\n",
            "length of targets:  73\n",
            "length of data:  73\n",
            "===============================================\n",
            "length of predicted classes:  208\n",
            "length of targets:  208\n",
            "length of data:  208\n",
            "===============================================\n",
            "length of predicted classes:  188\n",
            "length of targets:  188\n",
            "length of data:  188\n",
            "===============================================\n",
            "length of predicted classes:  87\n",
            "length of targets:  87\n",
            "length of data:  87\n",
            "===============================================\n",
            "length of predicted classes:  261\n",
            "length of targets:  261\n",
            "length of data:  261\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  125\n",
            "length of targets:  125\n",
            "length of data:  125\n",
            "===============================================\n",
            "length of predicted classes:  131\n",
            "length of targets:  131\n",
            "length of data:  131\n",
            "===============================================\n",
            "length of predicted classes:  168\n",
            "length of targets:  168\n",
            "length of data:  168\n",
            "===============================================\n",
            "length of predicted classes:  164\n",
            "length of targets:  164\n",
            "length of data:  164\n",
            "===============================================\n",
            "length of predicted classes:  82\n",
            "length of targets:  82\n",
            "length of data:  82\n",
            "===============================================\n",
            "length of predicted classes:  22\n",
            "length of targets:  22\n",
            "length of data:  22\n",
            "===============================================\n",
            "length of predicted classes:  45\n",
            "length of targets:  45\n",
            "length of data:  45\n",
            "===============================================\n",
            "length of predicted classes:  10\n",
            "length of targets:  10\n",
            "length of data:  10\n",
            "===============================================\n",
            "length of predicted classes:  51\n",
            "length of targets:  51\n",
            "length of data:  51\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  150\n",
            "length of targets:  150\n",
            "length of data:  150\n",
            "===============================================\n",
            "length of predicted classes:  38\n",
            "length of targets:  38\n",
            "length of data:  38\n",
            "===============================================\n",
            "length of predicted classes:  64\n",
            "length of targets:  64\n",
            "length of data:  64\n",
            "===============================================\n",
            "length of predicted classes:  30\n",
            "length of targets:  30\n",
            "length of data:  30\n",
            "===============================================\n",
            "length of predicted classes:  39\n",
            "length of targets:  39\n",
            "length of data:  39\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  235\n",
            "length of targets:  235\n",
            "length of data:  235\n",
            "===============================================\n",
            "length of predicted classes:  77\n",
            "length of targets:  77\n",
            "length of data:  77\n",
            "===============================================\n",
            "length of predicted classes:  158\n",
            "length of targets:  158\n",
            "length of data:  158\n",
            "===============================================\n",
            "length of predicted classes:  155\n",
            "length of targets:  155\n",
            "length of data:  155\n",
            "===============================================\n",
            "length of predicted classes:  107\n",
            "length of targets:  107\n",
            "length of data:  107\n",
            "===============================================\n",
            "length of predicted classes:  43\n",
            "length of targets:  43\n",
            "length of data:  43\n",
            "===============================================\n",
            "length of predicted classes:  123\n",
            "length of targets:  123\n",
            "length of data:  123\n",
            "===============================================\n",
            "length of predicted classes:  291\n",
            "length of targets:  291\n",
            "length of data:  291\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  215\n",
            "length of targets:  215\n",
            "length of data:  215\n",
            "===============================================\n",
            "length of predicted classes:  78\n",
            "length of targets:  78\n",
            "length of data:  78\n",
            "===============================================\n",
            "length of predicted classes:  36\n",
            "length of targets:  36\n",
            "length of data:  36\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  21\n",
            "length of targets:  21\n",
            "length of data:  21\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  96\n",
            "length of targets:  96\n",
            "length of data:  96\n",
            "===============================================\n",
            "length of predicted classes:  281\n",
            "length of targets:  281\n",
            "length of data:  281\n",
            "===============================================\n",
            "length of predicted classes:  169\n",
            "length of targets:  169\n",
            "length of data:  169\n",
            "===============================================\n",
            "length of predicted classes:  142\n",
            "length of targets:  142\n",
            "length of data:  142\n",
            "===============================================\n",
            "length of predicted classes:  83\n",
            "length of targets:  83\n",
            "length of data:  83\n",
            "===============================================\n",
            "length of predicted classes:  264\n",
            "length of targets:  264\n",
            "length of data:  264\n",
            "===============================================\n",
            "length of predicted classes:  235\n",
            "length of targets:  235\n",
            "length of data:  235\n",
            "===============================================\n",
            "length of predicted classes:  33\n",
            "length of targets:  33\n",
            "length of data:  33\n",
            "===============================================\n",
            "length of predicted classes:  13\n",
            "length of targets:  13\n",
            "length of data:  13\n",
            "===============================================\n",
            "length of predicted classes:  52\n",
            "length of targets:  52\n",
            "length of data:  52\n",
            "===============================================\n",
            "length of predicted classes:  181\n",
            "length of targets:  181\n",
            "length of data:  181\n",
            "===============================================\n",
            "length of predicted classes:  185\n",
            "length of targets:  185\n",
            "length of data:  185\n",
            "===============================================\n",
            "length of predicted classes:  35\n",
            "length of targets:  35\n",
            "length of data:  35\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  288\n",
            "length of targets:  288\n",
            "length of data:  288\n",
            "===============================================\n",
            "length of predicted classes:  123\n",
            "length of targets:  123\n",
            "length of data:  123\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  185\n",
            "length of targets:  185\n",
            "length of data:  185\n",
            "===============================================\n",
            "length of predicted classes:  19\n",
            "length of targets:  19\n",
            "length of data:  19\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  18\n",
            "length of targets:  18\n",
            "length of data:  18\n",
            "===============================================\n",
            "length of predicted classes:  283\n",
            "length of targets:  283\n",
            "length of data:  283\n",
            "===============================================\n",
            "length of predicted classes:  37\n",
            "length of targets:  37\n",
            "length of data:  37\n",
            "===============================================\n",
            "length of predicted classes:  39\n",
            "length of targets:  39\n",
            "length of data:  39\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  274\n",
            "length of targets:  274\n",
            "length of data:  274\n",
            "===============================================\n",
            "length of predicted classes:  6\n",
            "length of targets:  6\n",
            "length of data:  6\n",
            "===============================================\n",
            "length of predicted classes:  25\n",
            "length of targets:  25\n",
            "length of data:  25\n",
            "===============================================\n",
            "length of predicted classes:  116\n",
            "length of targets:  116\n",
            "length of data:  116\n",
            "===============================================\n",
            "length of predicted classes:  16\n",
            "length of targets:  16\n",
            "length of data:  16\n",
            "===============================================\n",
            "length of predicted classes:  51\n",
            "length of targets:  51\n",
            "length of data:  51\n",
            "===============================================\n",
            "length of predicted classes:  220\n",
            "length of targets:  220\n",
            "length of data:  220\n",
            "===============================================\n",
            "length of predicted classes:  239\n",
            "length of targets:  239\n",
            "length of data:  239\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  95\n",
            "length of targets:  95\n",
            "length of data:  95\n",
            "===============================================\n",
            "length of predicted classes:  38\n",
            "length of targets:  38\n",
            "length of data:  38\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  0\n",
            "length of targets:  0\n",
            "length of data:  0\n",
            "===============================================\n",
            "length of predicted classes:  113\n",
            "length of targets:  113\n",
            "length of data:  113\n",
            "===============================================\n",
            "length of predicted classes:  98\n",
            "length of targets:  98\n",
            "length of data:  98\n",
            "===============================================\n",
            "length of predicted classes:  112\n",
            "length of targets:  112\n",
            "length of data:  112\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  35\n",
            "length of targets:  35\n",
            "length of data:  35\n",
            "===============================================\n",
            "length of predicted classes:  288\n",
            "length of targets:  288\n",
            "length of data:  288\n",
            "===============================================\n",
            "length of predicted classes:  100\n",
            "length of targets:  100\n",
            "length of data:  100\n",
            "===============================================\n",
            "length of predicted classes:  100\n",
            "length of targets:  100\n",
            "length of data:  100\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  180\n",
            "length of targets:  180\n",
            "length of data:  180\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  78\n",
            "length of targets:  78\n",
            "length of data:  78\n",
            "===============================================\n",
            "length of predicted classes:  64\n",
            "length of targets:  64\n",
            "length of data:  64\n",
            "===============================================\n",
            "length of predicted classes:  180\n",
            "length of targets:  180\n",
            "length of data:  180\n",
            "===============================================\n",
            "length of predicted classes:  285\n",
            "length of targets:  285\n",
            "length of data:  285\n",
            "===============================================\n",
            "length of predicted classes:  170\n",
            "length of targets:  170\n",
            "length of data:  170\n",
            "===============================================\n",
            "length of predicted classes:  17\n",
            "length of targets:  17\n",
            "length of data:  17\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  272\n",
            "length of targets:  272\n",
            "length of data:  272\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  245\n",
            "length of targets:  245\n",
            "length of data:  245\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  42\n",
            "length of targets:  42\n",
            "length of data:  42\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  288\n",
            "length of targets:  288\n",
            "length of data:  288\n",
            "===============================================\n",
            "length of predicted classes:  54\n",
            "length of targets:  54\n",
            "length of data:  54\n",
            "===============================================\n",
            "length of predicted classes:  101\n",
            "length of targets:  101\n",
            "length of data:  101\n",
            "===============================================\n",
            "length of predicted classes:  251\n",
            "length of targets:  251\n",
            "length of data:  251\n",
            "===============================================\n",
            "length of predicted classes:  65\n",
            "length of targets:  65\n",
            "length of data:  65\n",
            "===============================================\n",
            "length of predicted classes:  145\n",
            "length of targets:  145\n",
            "length of data:  145\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  152\n",
            "length of targets:  152\n",
            "length of data:  152\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  157\n",
            "length of targets:  157\n",
            "length of data:  157\n",
            "===============================================\n",
            "length of predicted classes:  25\n",
            "length of targets:  25\n",
            "length of data:  25\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  86\n",
            "length of targets:  86\n",
            "length of data:  86\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  8\n",
            "length of targets:  8\n",
            "length of data:  8\n",
            "===============================================\n",
            "length of predicted classes:  165\n",
            "length of targets:  165\n",
            "length of data:  165\n",
            "===============================================\n",
            "length of predicted classes:  17\n",
            "length of targets:  17\n",
            "length of data:  17\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  169\n",
            "length of targets:  169\n",
            "length of data:  169\n",
            "===============================================\n",
            "length of predicted classes:  161\n",
            "length of targets:  161\n",
            "length of data:  161\n",
            "===============================================\n",
            "length of predicted classes:  208\n",
            "length of targets:  208\n",
            "length of data:  208\n",
            "===============================================\n",
            "length of predicted classes:  8\n",
            "length of targets:  8\n",
            "length of data:  8\n",
            "===============================================\n",
            "length of predicted classes:  19\n",
            "length of targets:  19\n",
            "length of data:  19\n",
            "===============================================\n",
            "length of predicted classes:  121\n",
            "length of targets:  121\n",
            "length of data:  121\n",
            "===============================================\n",
            "length of predicted classes:  138\n",
            "length of targets:  138\n",
            "length of data:  138\n",
            "===============================================\n",
            "length of predicted classes:  185\n",
            "length of targets:  185\n",
            "length of data:  185\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  32\n",
            "length of targets:  32\n",
            "length of data:  32\n",
            "===============================================\n",
            "length of predicted classes:  86\n",
            "length of targets:  86\n",
            "length of data:  86\n",
            "===============================================\n",
            "length of predicted classes:  17\n",
            "length of targets:  17\n",
            "length of data:  17\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  91\n",
            "length of targets:  91\n",
            "length of data:  91\n",
            "===============================================\n",
            "length of predicted classes:  37\n",
            "length of targets:  37\n",
            "length of data:  37\n",
            "===============================================\n",
            "length of predicted classes:  24\n",
            "length of targets:  24\n",
            "length of data:  24\n",
            "===============================================\n",
            "length of predicted classes:  262\n",
            "length of targets:  262\n",
            "length of data:  262\n",
            "===============================================\n",
            "length of predicted classes:  31\n",
            "length of targets:  31\n",
            "length of data:  31\n",
            "===============================================\n",
            "length of predicted classes:  173\n",
            "length of targets:  173\n",
            "length of data:  173\n",
            "===============================================\n",
            "length of predicted classes:  93\n",
            "length of targets:  93\n",
            "length of data:  93\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  59\n",
            "length of targets:  59\n",
            "length of data:  59\n",
            "===============================================\n",
            "length of predicted classes:  87\n",
            "length of targets:  87\n",
            "length of data:  87\n",
            "===============================================\n",
            "length of predicted classes:  0\n",
            "length of targets:  0\n",
            "length of data:  0\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  183\n",
            "length of targets:  183\n",
            "length of data:  183\n",
            "===============================================\n",
            "length of predicted classes:  38\n",
            "length of targets:  38\n",
            "length of data:  38\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  140\n",
            "length of targets:  140\n",
            "length of data:  140\n",
            "===============================================\n",
            "length of predicted classes:  25\n",
            "length of targets:  25\n",
            "length of data:  25\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  250\n",
            "length of targets:  250\n",
            "length of data:  250\n",
            "===============================================\n",
            "length of predicted classes:  57\n",
            "length of targets:  57\n",
            "length of data:  57\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  112\n",
            "length of targets:  112\n",
            "length of data:  112\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  63\n",
            "length of targets:  63\n",
            "length of data:  63\n",
            "===============================================\n",
            "length of predicted classes:  184\n",
            "length of targets:  184\n",
            "length of data:  184\n",
            "===============================================\n",
            "length of predicted classes:  174\n",
            "length of targets:  174\n",
            "length of data:  174\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  194\n",
            "length of targets:  194\n",
            "length of data:  194\n",
            "===============================================\n",
            "length of predicted classes:  150\n",
            "length of targets:  150\n",
            "length of data:  150\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  8\n",
            "length of targets:  8\n",
            "length of data:  8\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  57\n",
            "length of targets:  57\n",
            "length of data:  57\n",
            "===============================================\n",
            "length of predicted classes:  269\n",
            "length of targets:  269\n",
            "length of data:  269\n",
            "===============================================\n",
            "length of predicted classes:  120\n",
            "length of targets:  120\n",
            "length of data:  120\n",
            "===============================================\n",
            "length of predicted classes:  103\n",
            "length of targets:  103\n",
            "length of data:  103\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  226\n",
            "length of targets:  226\n",
            "length of data:  226\n",
            "===============================================\n",
            "length of predicted classes:  68\n",
            "length of targets:  68\n",
            "length of data:  68\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  55\n",
            "length of targets:  55\n",
            "length of data:  55\n",
            "===============================================\n",
            "length of predicted classes:  112\n",
            "length of targets:  112\n",
            "length of data:  112\n",
            "===============================================\n",
            "length of predicted classes:  22\n",
            "length of targets:  22\n",
            "length of data:  22\n",
            "===============================================\n",
            "length of predicted classes:  192\n",
            "length of targets:  192\n",
            "length of data:  192\n",
            "===============================================\n",
            "length of predicted classes:  109\n",
            "length of targets:  109\n",
            "length of data:  109\n",
            "===============================================\n",
            "length of predicted classes:  295\n",
            "length of targets:  295\n",
            "length of data:  295\n",
            "===============================================\n",
            "length of predicted classes:  176\n",
            "length of targets:  176\n",
            "length of data:  176\n",
            "===============================================\n",
            "length of predicted classes:  77\n",
            "length of targets:  77\n",
            "length of data:  77\n",
            "===============================================\n",
            "length of predicted classes:  58\n",
            "length of targets:  58\n",
            "length of data:  58\n",
            "===============================================\n",
            "length of predicted classes:  21\n",
            "length of targets:  21\n",
            "length of data:  21\n",
            "===============================================\n",
            "length of predicted classes:  145\n",
            "length of targets:  145\n",
            "length of data:  145\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  65\n",
            "length of targets:  65\n",
            "length of data:  65\n",
            "===============================================\n",
            "length of predicted classes:  78\n",
            "length of targets:  78\n",
            "length of data:  78\n",
            "===============================================\n",
            "length of predicted classes:  40\n",
            "length of targets:  40\n",
            "length of data:  40\n",
            "===============================================\n",
            "length of predicted classes:  179\n",
            "length of targets:  179\n",
            "length of data:  179\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  82\n",
            "length of targets:  82\n",
            "length of data:  82\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  164\n",
            "length of targets:  164\n",
            "length of data:  164\n",
            "===============================================\n",
            "length of predicted classes:  191\n",
            "length of targets:  191\n",
            "length of data:  191\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  273\n",
            "length of targets:  273\n",
            "length of data:  273\n",
            "===============================================\n",
            "length of predicted classes:  52\n",
            "length of targets:  52\n",
            "length of data:  52\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  279\n",
            "length of targets:  279\n",
            "length of data:  279\n",
            "===============================================\n",
            "length of predicted classes:  186\n",
            "length of targets:  186\n",
            "length of data:  186\n",
            "===============================================\n",
            "length of predicted classes:  210\n",
            "length of targets:  210\n",
            "length of data:  210\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  42\n",
            "length of targets:  42\n",
            "length of data:  42\n",
            "===============================================\n",
            "length of predicted classes:  65\n",
            "length of targets:  65\n",
            "length of data:  65\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  191\n",
            "length of targets:  191\n",
            "length of data:  191\n",
            "===============================================\n",
            "length of predicted classes:  4\n",
            "length of targets:  4\n",
            "length of data:  4\n",
            "===============================================\n",
            "length of predicted classes:  200\n",
            "length of targets:  200\n",
            "length of data:  200\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  124\n",
            "length of targets:  124\n",
            "length of data:  124\n",
            "===============================================\n",
            "length of predicted classes:  71\n",
            "length of targets:  71\n",
            "length of data:  71\n",
            "===============================================\n",
            "length of predicted classes:  90\n",
            "length of targets:  90\n",
            "length of data:  90\n",
            "===============================================\n",
            "length of predicted classes:  234\n",
            "length of targets:  234\n",
            "length of data:  234\n",
            "===============================================\n",
            "length of predicted classes:  80\n",
            "length of targets:  80\n",
            "length of data:  80\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  145\n",
            "length of targets:  145\n",
            "length of data:  145\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  286\n",
            "length of targets:  286\n",
            "length of data:  286\n",
            "===============================================\n",
            "length of predicted classes:  179\n",
            "length of targets:  179\n",
            "length of data:  179\n",
            "===============================================\n",
            "length of predicted classes:  283\n",
            "length of targets:  283\n",
            "length of data:  283\n",
            "===============================================\n",
            "length of predicted classes:  291\n",
            "length of targets:  291\n",
            "length of data:  291\n",
            "===============================================\n",
            "length of predicted classes:  89\n",
            "length of targets:  89\n",
            "length of data:  89\n",
            "===============================================\n",
            "length of predicted classes:  19\n",
            "length of targets:  19\n",
            "length of data:  19\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  290\n",
            "length of targets:  290\n",
            "length of data:  290\n",
            "===============================================\n",
            "length of predicted classes:  249\n",
            "length of targets:  249\n",
            "length of data:  249\n",
            "===============================================\n",
            "length of predicted classes:  63\n",
            "length of targets:  63\n",
            "length of data:  63\n",
            "===============================================\n",
            "length of predicted classes:  241\n",
            "length of targets:  241\n",
            "length of data:  241\n",
            "===============================================\n",
            "length of predicted classes:  42\n",
            "length of targets:  42\n",
            "length of data:  42\n",
            "===============================================\n",
            "length of predicted classes:  15\n",
            "length of targets:  15\n",
            "length of data:  15\n",
            "===============================================\n",
            "length of predicted classes:  260\n",
            "length of targets:  260\n",
            "length of data:  260\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  58\n",
            "length of targets:  58\n",
            "length of data:  58\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  62\n",
            "length of targets:  62\n",
            "length of data:  62\n",
            "===============================================\n",
            "length of predicted classes:  72\n",
            "length of targets:  72\n",
            "length of data:  72\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  49\n",
            "length of targets:  49\n",
            "length of data:  49\n",
            "===============================================\n",
            "length of predicted classes:  53\n",
            "length of targets:  53\n",
            "length of data:  53\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  97\n",
            "length of targets:  97\n",
            "length of data:  97\n",
            "===============================================\n",
            "length of predicted classes:  72\n",
            "length of targets:  72\n",
            "length of data:  72\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  79\n",
            "length of targets:  79\n",
            "length of data:  79\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  276\n",
            "length of targets:  276\n",
            "length of data:  276\n",
            "===============================================\n",
            "length of predicted classes:  92\n",
            "length of targets:  92\n",
            "length of data:  92\n",
            "===============================================\n",
            "length of predicted classes:  12\n",
            "length of targets:  12\n",
            "length of data:  12\n",
            "===============================================\n",
            "length of predicted classes:  8\n",
            "length of targets:  8\n",
            "length of data:  8\n",
            "===============================================\n",
            "length of predicted classes:  95\n",
            "length of targets:  95\n",
            "length of data:  95\n",
            "===============================================\n",
            "length of predicted classes:  182\n",
            "length of targets:  182\n",
            "length of data:  182\n",
            "===============================================\n",
            "length of predicted classes:  90\n",
            "length of targets:  90\n",
            "length of data:  90\n",
            "===============================================\n",
            "length of predicted classes:  172\n",
            "length of targets:  172\n",
            "length of data:  172\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  39\n",
            "length of targets:  39\n",
            "length of data:  39\n",
            "===============================================\n",
            "length of predicted classes:  229\n",
            "length of targets:  229\n",
            "length of data:  229\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  58\n",
            "length of targets:  58\n",
            "length of data:  58\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  127\n",
            "length of targets:  127\n",
            "length of data:  127\n",
            "===============================================\n",
            "length of predicted classes:  191\n",
            "length of targets:  191\n",
            "length of data:  191\n",
            "===============================================\n",
            "length of predicted classes:  19\n",
            "length of targets:  19\n",
            "length of data:  19\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  297\n",
            "length of targets:  297\n",
            "length of data:  297\n",
            "===============================================\n",
            "length of predicted classes:  122\n",
            "length of targets:  122\n",
            "length of data:  122\n",
            "===============================================\n",
            "length of predicted classes:  99\n",
            "length of targets:  99\n",
            "length of data:  99\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  60\n",
            "length of targets:  60\n",
            "length of data:  60\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  86\n",
            "length of targets:  86\n",
            "length of data:  86\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  189\n",
            "length of targets:  189\n",
            "length of data:  189\n",
            "===============================================\n",
            "length of predicted classes:  75\n",
            "length of targets:  75\n",
            "length of data:  75\n",
            "===============================================\n",
            "length of predicted classes:  113\n",
            "length of targets:  113\n",
            "length of data:  113\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  211\n",
            "length of targets:  211\n",
            "length of data:  211\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  10\n",
            "length of targets:  10\n",
            "length of data:  10\n",
            "===============================================\n",
            "length of predicted classes:  161\n",
            "length of targets:  161\n",
            "length of data:  161\n",
            "===============================================\n",
            "length of predicted classes:  218\n",
            "length of targets:  218\n",
            "length of data:  218\n",
            "===============================================\n",
            "length of predicted classes:  108\n",
            "length of targets:  108\n",
            "length of data:  108\n",
            "===============================================\n",
            "length of predicted classes:  107\n",
            "length of targets:  107\n",
            "length of data:  107\n",
            "===============================================\n",
            "length of predicted classes:  102\n",
            "length of targets:  102\n",
            "length of data:  102\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  161\n",
            "length of targets:  161\n",
            "length of data:  161\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  68\n",
            "length of targets:  68\n",
            "length of data:  68\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  34\n",
            "length of targets:  34\n",
            "length of data:  34\n",
            "===============================================\n",
            "length of predicted classes:  285\n",
            "length of targets:  285\n",
            "length of data:  285\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  151\n",
            "length of targets:  151\n",
            "length of data:  151\n",
            "===============================================\n",
            "length of predicted classes:  25\n",
            "length of targets:  25\n",
            "length of data:  25\n",
            "===============================================\n",
            "length of predicted classes:  87\n",
            "length of targets:  87\n",
            "length of data:  87\n",
            "===============================================\n",
            "length of predicted classes:  292\n",
            "length of targets:  292\n",
            "length of data:  292\n",
            "===============================================\n",
            "length of predicted classes:  149\n",
            "length of targets:  149\n",
            "length of data:  149\n",
            "===============================================\n",
            "length of predicted classes:  126\n",
            "length of targets:  126\n",
            "length of data:  126\n",
            "===============================================\n",
            "length of predicted classes:  271\n",
            "length of targets:  271\n",
            "length of data:  271\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  43\n",
            "length of targets:  43\n",
            "length of data:  43\n",
            "===============================================\n",
            "length of predicted classes:  100\n",
            "length of targets:  100\n",
            "length of data:  100\n",
            "===============================================\n",
            "length of predicted classes:  74\n",
            "length of targets:  74\n",
            "length of data:  74\n",
            "===============================================\n",
            "length of predicted classes:  231\n",
            "length of targets:  231\n",
            "length of data:  231\n",
            "===============================================\n",
            "length of predicted classes:  116\n",
            "length of targets:  116\n",
            "length of data:  116\n",
            "===============================================\n",
            "length of predicted classes:  140\n",
            "length of targets:  140\n",
            "length of data:  140\n",
            "===============================================\n",
            "length of predicted classes:  103\n",
            "length of targets:  103\n",
            "length of data:  103\n",
            "===============================================\n",
            "length of predicted classes:  30\n",
            "length of targets:  30\n",
            "length of data:  30\n",
            "===============================================\n",
            "length of predicted classes:  295\n",
            "length of targets:  295\n",
            "length of data:  295\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  212\n",
            "length of targets:  212\n",
            "length of data:  212\n",
            "===============================================\n",
            "length of predicted classes:  246\n",
            "length of targets:  246\n",
            "length of data:  246\n",
            "===============================================\n",
            "length of predicted classes:  236\n",
            "length of targets:  236\n",
            "length of data:  236\n",
            "===============================================\n",
            "length of predicted classes:  56\n",
            "length of targets:  56\n",
            "length of data:  56\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  289\n",
            "length of targets:  289\n",
            "length of data:  289\n",
            "===============================================\n",
            "length of predicted classes:  72\n",
            "length of targets:  72\n",
            "length of data:  72\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  22\n",
            "length of targets:  22\n",
            "length of data:  22\n",
            "===============================================\n",
            "length of predicted classes:  62\n",
            "length of targets:  62\n",
            "length of data:  62\n",
            "===============================================\n",
            "length of predicted classes:  71\n",
            "length of targets:  71\n",
            "length of data:  71\n",
            "===============================================\n",
            "length of predicted classes:  149\n",
            "length of targets:  149\n",
            "length of data:  149\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  25\n",
            "length of targets:  25\n",
            "length of data:  25\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  102\n",
            "length of targets:  102\n",
            "length of data:  102\n",
            "===============================================\n",
            "length of predicted classes:  44\n",
            "length of targets:  44\n",
            "length of data:  44\n",
            "===============================================\n",
            "length of predicted classes:  220\n",
            "length of targets:  220\n",
            "length of data:  220\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  65\n",
            "length of targets:  65\n",
            "length of data:  65\n",
            "===============================================\n",
            "length of predicted classes:  105\n",
            "length of targets:  105\n",
            "length of data:  105\n",
            "===============================================\n",
            "length of predicted classes:  180\n",
            "length of targets:  180\n",
            "length of data:  180\n",
            "===============================================\n",
            "length of predicted classes:  132\n",
            "length of targets:  132\n",
            "length of data:  132\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  154\n",
            "length of targets:  154\n",
            "length of data:  154\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  29\n",
            "length of targets:  29\n",
            "length of data:  29\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  6\n",
            "length of targets:  6\n",
            "length of data:  6\n",
            "===============================================\n",
            "length of predicted classes:  244\n",
            "length of targets:  244\n",
            "length of data:  244\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  194\n",
            "length of targets:  194\n",
            "length of data:  194\n",
            "===============================================\n",
            "length of predicted classes:  64\n",
            "length of targets:  64\n",
            "length of data:  64\n",
            "===============================================\n",
            "length of predicted classes:  169\n",
            "length of targets:  169\n",
            "length of data:  169\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  54\n",
            "length of targets:  54\n",
            "length of data:  54\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  32\n",
            "length of targets:  32\n",
            "length of data:  32\n",
            "===============================================\n",
            "length of predicted classes:  160\n",
            "length of targets:  160\n",
            "length of data:  160\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  92\n",
            "length of targets:  92\n",
            "length of data:  92\n",
            "===============================================\n",
            "length of predicted classes:  127\n",
            "length of targets:  127\n",
            "length of data:  127\n",
            "===============================================\n",
            "length of predicted classes:  29\n",
            "length of targets:  29\n",
            "length of data:  29\n",
            "===============================================\n",
            "length of predicted classes:  78\n",
            "length of targets:  78\n",
            "length of data:  78\n",
            "===============================================\n",
            "length of predicted classes:  12\n",
            "length of targets:  12\n",
            "length of data:  12\n",
            "===============================================\n",
            "length of predicted classes:  283\n",
            "length of targets:  283\n",
            "length of data:  283\n",
            "===============================================\n",
            "length of predicted classes:  2\n",
            "length of targets:  2\n",
            "length of data:  2\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  83\n",
            "length of targets:  83\n",
            "length of data:  83\n",
            "===============================================\n",
            "length of predicted classes:  251\n",
            "length of targets:  251\n",
            "length of data:  251\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  33\n",
            "length of targets:  33\n",
            "length of data:  33\n",
            "===============================================\n",
            "length of predicted classes:  108\n",
            "length of targets:  108\n",
            "length of data:  108\n",
            "===============================================\n",
            "length of predicted classes:  197\n",
            "length of targets:  197\n",
            "length of data:  197\n",
            "===============================================\n",
            "length of predicted classes:  5\n",
            "length of targets:  5\n",
            "length of data:  5\n",
            "===============================================\n",
            "length of predicted classes:  100\n",
            "length of targets:  100\n",
            "length of data:  100\n",
            "===============================================\n",
            "length of predicted classes:  27\n",
            "length of targets:  27\n",
            "length of data:  27\n",
            "===============================================\n",
            "length of predicted classes:  51\n",
            "length of targets:  51\n",
            "length of data:  51\n",
            "===============================================\n",
            "length of predicted classes:  31\n",
            "length of targets:  31\n",
            "length of data:  31\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  72\n",
            "length of targets:  72\n",
            "length of data:  72\n",
            "===============================================\n",
            "length of predicted classes:  257\n",
            "length of targets:  257\n",
            "length of data:  257\n",
            "===============================================\n",
            "length of predicted classes:  121\n",
            "length of targets:  121\n",
            "length of data:  121\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  25\n",
            "length of targets:  25\n",
            "length of data:  25\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  299\n",
            "length of targets:  299\n",
            "length of data:  299\n",
            "===============================================\n",
            "length of predicted classes:  62\n",
            "length of targets:  62\n",
            "length of data:  62\n",
            "===============================================\n",
            "length of predicted classes:  95\n",
            "length of targets:  95\n",
            "length of data:  95\n",
            "===============================================\n",
            "length of predicted classes:  24\n",
            "length of targets:  24\n",
            "length of data:  24\n",
            "===============================================\n",
            "length of predicted classes:  223\n",
            "length of targets:  223\n",
            "length of data:  223\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  5\n",
            "length of targets:  5\n",
            "length of data:  5\n",
            "===============================================\n",
            "length of predicted classes:  167\n",
            "length of targets:  167\n",
            "length of data:  167\n",
            "===============================================\n",
            "length of predicted classes:  168\n",
            "length of targets:  168\n",
            "length of data:  168\n",
            "===============================================\n",
            "length of predicted classes:  16\n",
            "length of targets:  16\n",
            "length of data:  16\n",
            "===============================================\n",
            "length of predicted classes:  33\n",
            "length of targets:  33\n",
            "length of data:  33\n",
            "===============================================\n",
            "length of predicted classes:  171\n",
            "length of targets:  171\n",
            "length of data:  171\n",
            "===============================================\n",
            "length of predicted classes:  23\n",
            "length of targets:  23\n",
            "length of data:  23\n",
            "===============================================\n",
            "length of predicted classes:  290\n",
            "length of targets:  290\n",
            "length of data:  290\n",
            "===============================================\n",
            "length of predicted classes:  221\n",
            "length of targets:  221\n",
            "length of data:  221\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  132\n",
            "length of targets:  132\n",
            "length of data:  132\n",
            "===============================================\n",
            "length of predicted classes:  270\n",
            "length of targets:  270\n",
            "length of data:  270\n",
            "===============================================\n",
            "length of predicted classes:  39\n",
            "length of targets:  39\n",
            "length of data:  39\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  85\n",
            "length of targets:  85\n",
            "length of data:  85\n",
            "===============================================\n",
            "length of predicted classes:  187\n",
            "length of targets:  187\n",
            "length of data:  187\n",
            "===============================================\n",
            "length of predicted classes:  100\n",
            "length of targets:  100\n",
            "length of data:  100\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  59\n",
            "length of targets:  59\n",
            "length of data:  59\n",
            "===============================================\n",
            "length of predicted classes:  300\n",
            "length of targets:  300\n",
            "length of data:  300\n",
            "===============================================\n",
            "length of predicted classes:  163\n",
            "length of targets:  163\n",
            "length of data:  163\n",
            "===============================================\n",
            "length of predicted classes:  172\n",
            "length of targets:  172\n",
            "length of data:  172\n",
            "ليّسِ  لَلَوُّكي لَ َب اِلٌقَبض  ّأَنَ  يَبٌرَأَ ِا لّمّدَيّن  َأٌوَ َيّه بَ ٌاَلِد يَنً  لَهّ َأِو \n",
            "Length of data:  2974\n",
            "Length of predictions:  520842\n",
            "Accuracy:  26.542980194091797\n"
          ]
        }
      ]
    }
  ]
}