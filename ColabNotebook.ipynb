{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DV7fmwYwVX_",
        "outputId": "c34c5b37-99a7-4315-80c3-e2cee747bffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "u2nZg8-K24xf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import re\n",
        "import torch\n",
        "import numpy as np\n",
        "# from gensim.models import KeyedVectors\n",
        "# from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import os\n",
        "\n",
        "# import stanfordnlp\n",
        "\n",
        "# Download the Arabic models for the neural pipeline\n",
        "# stanfordnlp.download('ar', force=True)\n",
        "# Build a neural pipeline using the Arabic models\n",
        "# nlp = stanfordnlp.Pipeline(lang='ar')\n",
        "\n",
        "# def split_arabic_sentences_with_stanfordnlp(corpus_text):\n",
        "#     # Process the text\n",
        "#     doc = nlp(corpus_text)\n",
        "\n",
        "#     # Extract sentences from the doc\n",
        "#     sentences = [sentence.text for sentence in doc.sentences]\n",
        "\n",
        "#     return sentences\n",
        "# file_path = './SG_300_3_400/w2v_SG_300_3_400_10.model'\n",
        "# word_embed_model = Word2Vec.load(file_path)\n",
        "\n",
        "max_len=300\n",
        "\n",
        "with open('/content/drive/MyDrive/NLP project/Arabic-text-to-diacritics/files/arabic_letters.pickle', 'rb') as file:\n",
        "            ARABIC_LETTERS_LIST = pkl.load(file)\n",
        "with open('/content/drive/MyDrive/NLP project/Arabic-text-to-diacritics/files/diacritics.pickle', 'rb') as file:\n",
        "            DIACRITICS_LIST = pkl.load(file)\n",
        "\n",
        "arabic_letters=[]\n",
        "for letter in ARABIC_LETTERS_LIST:\n",
        "    arabic_letters.append(letter[0])\n",
        "arabic_letters.append(\" \")\n",
        "\n",
        "\n",
        "dicritics=[]\n",
        "for letter in DIACRITICS_LIST:\n",
        "    dicritics.append(letter[0])\n",
        "\n",
        "classes = {\n",
        "    'َ': 0,\n",
        "    'ُ': 1,\n",
        "    'ِ': 2,\n",
        "    'ْ': 3,\n",
        "    'ّ': 4,\n",
        "    'ً': 5,\n",
        "    'ٌ': 6,\n",
        "    'ٍ': 7,\n",
        "    'َّ': 8,\n",
        "    'ُّ': 9,\n",
        "    'ِّ': 10,\n",
        "    'ًّ': 11,\n",
        "    'ٌّ': 12,\n",
        "    'ٍّ': 13,\n",
        "    \"\":14,\n",
        "}\n",
        "\n",
        "inverted_classes = {v: k for k, v in classes.items()}\n",
        "\n",
        "##################### to delete #####################\n",
        "def read_text(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        return file.read()\n",
        "\n",
        "def write_to_file_second(dirctory,file_path, text):\n",
        "    if not os.path.exists(dirctory):\n",
        "        os.makedirs(dirctory)\n",
        "\n",
        "    with open(dirctory+\"/\"+file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        lines=text\n",
        "        for l in lines:\n",
        "            file.write(l)\n",
        "        file.write('\\n')\n",
        "def write_to_file_labels(dirctory,file_path, text):\n",
        "    if not os.path.exists(dirctory):\n",
        "        os.makedirs(dirctory)\n",
        "\n",
        "    with open(dirctory+\"/\"+file_path, \"a\", encoding=\"utf-8\") as file:\n",
        "        lines=text\n",
        "        for l in lines:\n",
        "            file.write(inverted_classes[l])\n",
        "        file.write('\\n')\n",
        "\n",
        "\n",
        "def write_to_file_string(dirctory,file_path, text):\n",
        "    if not os.path.exists(dirctory):\n",
        "        os.makedirs(dirctory)\n",
        "\n",
        "    with open(dirctory+\"/\"+file_path, \"a\", encoding=\"utf-8\") as file:\n",
        "        lines=text\n",
        "        file.write(lines)\n",
        "        file.write('\\n')\n",
        "        file.write('\\n')\n",
        "#####################################################\n",
        "\n",
        "def preprocess(text):\n",
        "        # Remove URLs\n",
        "        text = re.sub(r\"http[s|S]\\S+\", \"\", text,flags=re.MULTILINE)\n",
        "        text = re.sub(r\"www\\S+\", \"\", text,flags=re.MULTILINE)\n",
        "        # Remove English letters\n",
        "        text = re.sub(r\"[A-Za-z]+\", \"\", text,flags=re.MULTILINE)\n",
        "        # Remove Kashida Arabic character\n",
        "        text = re.sub(r\"\\u0640\", \"\", text,flags=re.MULTILINE)\n",
        "        # Add space before and after the numbers\n",
        "        text = re.sub(r\"(\\d+)\", r\" \\1 \", text,flags=re.MULTILINE)\n",
        "        # removes SHIFT+J Arabic character\n",
        "        text = re.sub(r\"\\u0691\", \"\", text,flags=re.MULTILINE)\n",
        "        # remove english numbers\n",
        "        text = re.sub(r\"[0-9]+\", \"\", text,flags=re.MULTILINE)\n",
        "        # remove arabic numbers\n",
        "        text = re.sub(r\"[٠-٩]+\", \"\", text,flags=re.MULTILINE)\n",
        "         # remove brackets\n",
        "        # text = re.sub(r\"\\[.*?\\]\", \"\", text)\n",
        "        # text = re.sub(r\"\\(.*?\\)\", \"\", text)\n",
        "        return text\n",
        "\n",
        "\n",
        "# text=read_text('Dataset/train.txt')\n",
        "# text=preprocess(text)\n",
        "# print(len(text))\n",
        "# # delete except arabic letters and dicritics and punctuation\n",
        "# # remove multiple spaces\n",
        "\n",
        "# write_to_file_string(\"test\",\"data.txt\",text)\n",
        "\n",
        "def split_text(text):\n",
        "    text=text.split('.')\n",
        "    # split text to sentences on all arabic sparatators\n",
        "\n",
        "    data=[]\n",
        "    for t in text:\n",
        "        if(len(t)==0): continue\n",
        "        if(len(t)<max_len):\n",
        "            while(len(t)<max_len):\n",
        "                 t+=\" \"\n",
        "            data.append(t)\n",
        "        if(len(t)>max_len):\n",
        "            data.append(t[:max_len])\n",
        "            supdata=t[max_len:]\n",
        "            while(len(supdata)>max_len):\n",
        "                data.append(supdata[:max_len])\n",
        "                supdata=supdata[max_len:]\n",
        "            if(len(supdata)<max_len):\n",
        "                while(len(supdata)<max_len):\n",
        "                    supdata+=\" \"\n",
        "                data.append(supdata)\n",
        "    return data\n",
        "\n",
        "\n",
        "HARAQAT = [\"ْ\", \"ّ\", \"ٌ\", \"ٍ\", \"ِ\", \"ً\", \"َ\", \"ُ\"]\n",
        "ARAB_CHARS = \"ىعظحرسيشضق ثلصطكآماإهزءأفؤغجئدةخوبذتن\"\n",
        "# [\".\", \"،\", \":\", \"؛\", \"-\", \"؟\"]\n",
        "VALID_ARABIC = HARAQAT + list(ARAB_CHARS) + ['.']\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "_whitespace_re = re.compile(r\"\\s+\")\n",
        "\n",
        "def remove_spaces(text):\n",
        "    text = re.sub(_whitespace_re, \" \", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def preprocessing(text):\n",
        "    text = filter(lambda char: char in VALID_ARABIC, text)\n",
        "    text = remove_spaces(''.join(list(text)))\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def get_data_labels(text):\n",
        "    data=\"\"\n",
        "    labels=[]\n",
        "    for i in range(len(text)):\n",
        "        if(text[i] in arabic_letters):\n",
        "            data+=text[i]\n",
        "            if(i+1<len(text) and text[i+1] in dicritics):\n",
        "                if(i+2<len(text) and classes[text[i+1]]==4 and text[i+2] in dicritics):\n",
        "                    labels.append(classes[text[i+1]+text[i+2]])\n",
        "                    i+=2\n",
        "                else:\n",
        "                    labels.append(classes[text[i+1]])\n",
        "                    i+=1\n",
        "            else:\n",
        "                labels.append(14)\n",
        "    return data,labels\n",
        "\n",
        "# def one_hot_encoding(text):\n",
        "#     onehot_encoded=[]\n",
        "#     for i in range(len(text)):\n",
        "#          if text[i] in arabic_letters:\n",
        "#             idx=arabic_letters.index(text[i])\n",
        "#             encode=np.zeros(len(arabic_letters))\n",
        "#             encode[idx]=1\n",
        "#             onehot_encoded.append(encode)\n",
        "#     onehot_encoded=torch.tensor(onehot_encoded)\n",
        "#     return onehot_encoded\n",
        "\n",
        "def encoding(text):\n",
        "    idx=arabic_letters.index(text)\n",
        "    encode=np.zeros(len(arabic_letters))\n",
        "    encode[idx]=1\n",
        "    return torch.tensor(encode,dtype=torch.float32).to(device)\n",
        "\n",
        "\n",
        "def get_dataloader(encoded_data, encoding_labels,batch_size=1):\n",
        "    # Create TensorDataset\n",
        "    dataset = TensorDataset(encoded_data.to(device), encoding_labels.to(device))\n",
        "    # Create DataLoader\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "def get_validation():\n",
        "    text=read_text('/content/drive/MyDrive/NLP project/Arabic-text-to-diacritics/Dataset/val.txt')\n",
        "    text=preprocess(text)\n",
        "    # size=int(0.02*len(text))\n",
        "    # text=text[:size]\n",
        "    # text=split_text(text)\n",
        "    # write_to_file_second(\"test\",\"data.txt\",text)\n",
        "    # text=split_text(text)\n",
        "    data=[]\n",
        "    labels=[]\n",
        "    for i in range (len(text)):\n",
        "        if(text[i] in arabic_letters):\n",
        "                data+=(text[i])\n",
        "                if(i+1<len(text) and text[i+1] in dicritics):\n",
        "                    if(i+2<len(text) and classes[text[i+1]]==4 and text[i+2] in dicritics):\n",
        "                        labels.append(classes[text[i+1]+text[i+2]])\n",
        "                        i+=2\n",
        "                    else:\n",
        "                        labels.append(classes[text[i+1]])\n",
        "                        i+=1\n",
        "                else:\n",
        "                    labels.append(14)\n",
        "        # else:\n",
        "        #     data.append(text[i])\n",
        "        #     labels.append(15)\n",
        "    encoded_data = torch.empty(0, len(arabic_letters),dtype=torch.float32).to(device)\n",
        "\n",
        "    for letter in data:\n",
        "        if letter in arabic_letters:\n",
        "            x = encoding(letter).unsqueeze(0).to(device)\n",
        "        else:\n",
        "            x=np.zeros((1,len(arabic_letters)))\n",
        "            x=torch.tensor(x,dtype=torch.float32).to(device)\n",
        "        encoded_data = torch.cat((encoded_data, x), 0)\n",
        "    labels=torch.tensor(labels,dtype=torch.long).to(device)\n",
        "    # print(encoded_data.shape)\n",
        "    # print(labels.shape)\n",
        "    dataloader=get_dataloader(encoded_data,labels)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "def get_data(path):\n",
        "    text=read_text(path)\n",
        "    text=preprocess(text)\n",
        "    # size=int(0.02*len(text))\n",
        "    # text=text[:size]\n",
        "    text = preprocessing(text)\n",
        "    text=\"\".join(text)\n",
        "    # write_to_file_string(\"test\",\"data.txt\",text)\n",
        "    # text=split_text(text)\n",
        "    text=text.split('.')\n",
        "    print(len(text))\n",
        "\n",
        "    # data=[]\n",
        "    # labels=[]\n",
        "    # for t in text:\n",
        "    #     d=\"\"\n",
        "    #     l=[]\n",
        "    #     if len(t)>300:\n",
        "    #         continue\n",
        "    #     else:\n",
        "    #         d,l=get_data_labels(t)\n",
        "    #         if(len(d)==0): continue\n",
        "    #         if(len(d)<max_len):\n",
        "    #             while(len(d)<max_len):\n",
        "    #                 d+=\" \"\n",
        "    #                 l.append(14)\n",
        "    #             data.append(d)\n",
        "    #             labels.append(l)\n",
        "    #             continue\n",
        "\n",
        "    # write_to_file_second(\"test\",\"data.txt\",text)\n",
        "    # # get max length of sentence in text\n",
        "    # maxdata=text.split('\\n')\n",
        "\n",
        "    # max_len=0\n",
        "    # for t in maxdata:\n",
        "    #     if(len(t)>max_len):\n",
        "    #         max_len=len(t)\n",
        "    # print(max_len)\n",
        "\n",
        "    # split text to sentences on all arabic sparatators\n",
        "    # text = split_arabic_sentences_with_stanfordnlp(text)\n",
        "\n",
        "    # Filter out empty strings or whitespace-only sentences\n",
        "    # text = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "    data=[]\n",
        "    labels=[]\n",
        "    for t in text:\n",
        "        d=\"\"\n",
        "        l=[]\n",
        "        d,l=get_data_labels(t)\n",
        "        if(len(d)==0): continue\n",
        "        if(len(d)<max_len):\n",
        "            while(len(d)<max_len):\n",
        "                 d+=\" \"\n",
        "                 l.append(14)\n",
        "            data.append(d)\n",
        "            labels.append(l)\n",
        "            continue\n",
        "        if(len(d)>max_len):\n",
        "            data.append(d[:max_len])\n",
        "            labels.append(l[:max_len])\n",
        "            supdata=d[max_len:]\n",
        "            suplabels=l[max_len:]\n",
        "            while(len(supdata)>max_len):\n",
        "                data.append(supdata[:max_len])\n",
        "                labels.append(suplabels[:max_len])\n",
        "                supdata=supdata[max_len:]\n",
        "                suplabels=suplabels[max_len:]\n",
        "            if(len(supdata)<max_len):\n",
        "                while(len(supdata)<max_len):\n",
        "                    supdata+=\" \"\n",
        "                    suplabels.append(14)\n",
        "                data.append(supdata)\n",
        "                labels.append(suplabels)\n",
        "        # data.append(d)\n",
        "        # labels.append(l)\n",
        "\n",
        "    # for d in data:\n",
        "    #     write_to_file_string(\"test\",\"data.txt\",d)\n",
        "    return data,labels\n",
        "\n",
        "def get_features(data,labels):\n",
        "    encoded_data = torch.empty(0, max_len, len(arabic_letters),dtype=torch.float32).to(device)\n",
        "    for d in data:\n",
        "        enc = torch.empty(0, len(arabic_letters),dtype=torch.float32).to(device)\n",
        "        for letter in d:\n",
        "            x = encoding(letter).unsqueeze(0).to(device)\n",
        "            enc = torch.cat((enc, x), 0)\n",
        "        encoded_data = torch.cat((encoded_data, enc.unsqueeze(0)), 0)\n",
        "    # print(encoded_data.shape)\n",
        "    encoding_labels=torch.tensor(labels,dtype=torch.long).to(device)\n",
        "    # print(encoding_labels.shape)\n",
        "    return encoded_data,encoding_labels\n",
        "\n",
        "def get_word2vec_features(data, labels, model):\n",
        "    max_seq_length = 300\n",
        "    encoded_data = torch.empty(0, max_len, max_seq_length, dtype=torch.float32)\n",
        "\n",
        "    for sentence in data:\n",
        "        encoded_sentence = torch.empty(0, max_seq_length, dtype=torch.float32)\n",
        "        for letter in sentence:\n",
        "            if letter in model.wv:\n",
        "                # Convert NumPy array to PyTorch tensor\n",
        "                 embedding  = torch.tensor(model.wv[letter], dtype=torch.float32).unsqueeze(0)\n",
        "            else:\n",
        "                # If the word is not in the model's vocabulary, fill with zeros\n",
        "                embedding  = torch.zeros((1, max_seq_length), dtype=torch.float32)\n",
        "\n",
        "            encoded_sentence = torch.cat((encoded_sentence, embedding), 0)\n",
        "\n",
        "        encoded_data = torch.cat((encoded_data, encoded_sentence.unsqueeze(0)), 0)\n",
        "\n",
        "    encoding_labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return encoded_data, encoding_labels\n",
        "\n",
        "\n",
        "def get_tf_idf_features(data, labels):\n",
        "    # import tfidf using vectorizer\n",
        "    # create the transform\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    # tokenize and build vocab\n",
        "    vectorizer.fit(data)\n",
        "    # summarize\n",
        "    # print(vectorizer.vocabulary_)\n",
        "    # print(vectorizer.idf_)\n",
        "    # encode document\n",
        "    encoded_data = vectorizer.transform(data)\n",
        "    # summarize encoded vector\n",
        "    # print(encoded_data.shape)\n",
        "    # print(encoded_data.toarray())\n",
        "    # print(labels)\n",
        "    encoding_labels = torch.tensor(labels, dtype=torch.long)\n",
        "    return encoded_data, encoding_labels\n",
        "\n",
        "class DataSet():\n",
        "\n",
        "    def __init__(self,path,batch_size=1) :\n",
        "        print(\"Loading data...\")\n",
        "        data1,labels1=get_data(path)\n",
        "        # now labels is list of list [[1,2,3,4,5,15,15,0],[1,2,3,4,5,15,15,0]]\n",
        "        # data is list of string ['احمد','محمد']\n",
        "        print(\"Extracting features...\")\n",
        "        data,labels=get_features(data1,labels1)\n",
        "        # now the data and labels are tensor\n",
        "        # data is tensor of shape (number of sentences,max_len,37)\n",
        "        # labels is tensor of shape (number of sentences,max_len)\n",
        "        print(\"Creating dataloader...\")\n",
        "\n",
        "        dataloader=get_dataloader(data,labels,batch_size)\n",
        "        self.x=data1\n",
        "        self.y=labels1\n",
        "        self.dataloader=dataloader\n",
        "        print(\"Done data creation !\")\n",
        "\n",
        "    def __len__(self):\n",
        "         return len(self.y)\n",
        "    def item(self,idx):\n",
        "         return self.x[idx],self.y[idx]\n",
        "    def getdata(self):\n",
        "        return self.dataloader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "C5LvCIPR2eVx"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch import optim\n",
        "from numpy import vstack\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "def write_to_file_string(dirctory,file_path, text):\n",
        "    if not os.path.exists(dirctory):\n",
        "        os.makedirs(dirctory)\n",
        "\n",
        "    with open(dirctory+\"/\"+file_path, \"a\", encoding=\"utf-8\") as file:\n",
        "        lines=text\n",
        "        file.write(lines)\n",
        "        file.write('\\n')\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, inp_vocab_size: int, hidden_dim: int = 256, seq_len: int = 600, num_classes: int = 16):\n",
        "        super().__init__()\n",
        "        # self.lstm = nn.LSTM(inp_vocab_size, hidden_dim,num_layers=3, batch_first=True, bidirectional=True)\n",
        "        self.lstm = nn.LSTM(inp_vocab_size, hidden_dim, batch_first=True, bidirectional=True).to(device)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes).to(device)  # Output layer for 0 to 16 integers\n",
        "\n",
        "    def forward(self, input_sequence: torch.Tensor):\n",
        "        output, _ = self.lstm(input_sequence)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "def train(train_dl, model):\n",
        "    # define the optimization\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "    model.to(device)\n",
        "    # enumerate epochs\n",
        "    for epoch in range(50):\n",
        "        for i, (inputs, targets) in enumerate(train_dl):\n",
        "            # convert the input and target to tensor\n",
        "            # clear the gradients\n",
        "            optimizer.zero_grad()\n",
        "            # compute the model output\n",
        "            yhat = model(inputs.to(device))\n",
        "            yhat = yhat.view(-1, yhat.size(2))  # Reshape model output to [batch_size * sequence_length, num_classes]\n",
        "            targets = targets.view(-1)  # Reshape targets to [batch_size * sequence_length]\n",
        "            # print(yhat.shape)\n",
        "            # print(targets.shape)\n",
        "\n",
        "            # calculate loss\n",
        "            loss = criterion(yhat, targets.to(device))\n",
        "            loss.backward()\n",
        "            # update model weights\n",
        "            optimizer.step()\n",
        "            print(f'epoch {epoch} batch {i} loss {loss.item()}')\n",
        "            # break\n",
        "    # save model to file after training\n",
        "    torch.save(model.state_dict(), 'model.pth')\n",
        "\n",
        "\n",
        "def calculate_DER(actual_labels, predicted_labels):\n",
        "    # Convert lists to PyTorch tensors if they are not already\n",
        "    if not isinstance(actual_labels, torch.Tensor):\n",
        "        actual_labels = torch.tensor(actual_labels)\n",
        "    if not isinstance(predicted_labels, torch.Tensor):\n",
        "        predicted_labels = torch.tensor(predicted_labels)\n",
        "\n",
        "    # Check if the lengths of both label sequences match\n",
        "    if len(actual_labels) != len(predicted_labels):\n",
        "        raise ValueError(\"Lengths of actual and predicted labels should match.\")\n",
        "\n",
        "    total_errors = torch.sum(actual_labels != predicted_labels)\n",
        "    total_frames = len(actual_labels)\n",
        "\n",
        "    # DER calculation\n",
        "    DER = (1-(total_errors / total_frames)) * 100.0\n",
        "    return DER.item()  # Convert PyTorch scalar to Python float\n",
        "\n",
        "\n",
        "def evaluate_model(test_dl, model):\n",
        "    predictions, actuals = [], []\n",
        "    model.to(device)\n",
        "    for i, (inputs, targets) in enumerate(test_dl):\n",
        "        # evaluate the model on the test set\n",
        "        yhat = model(inputs.to(device))\n",
        "        yhat = yhat.detach().cpu().numpy()\n",
        "        # reshape the outputs to [batch_size * sequence_length, num_classes]\n",
        "        yhat = yhat.reshape(-1, yhat.shape[-1])\n",
        "        # get predicted classes\n",
        "        predicted_classes = np.argmax(yhat, axis=1)\n",
        "        # convert targets to numpy array and reshape\n",
        "        targets = targets.cpu().numpy().reshape(-1)\n",
        "        # store predictions and actuals\n",
        "        predictions.extend(predicted_classes.tolist())\n",
        "        actuals.extend(targets.tolist())\n",
        "        # break\n",
        "    # calculate accuracy\n",
        "    acc = calculate_DER(np.array(actuals), np.array(predictions))\n",
        "\n",
        "    return acc\n"
      ],
      "metadata": {
        "id": "qhy617nk2VPI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Traindata = DataSet( \"/content/drive/MyDrive/NLP project/Arabic-text-to-diacritics/Dataset/train.txt\", batch_size = 256 )\n",
        "Traindataloader = Traindata.getdata()\n",
        "print(\"----------------- Train data Loaded----------------\")\n",
        "valdata = DataSet( \"/content/drive/MyDrive/NLP project/Arabic-text-to-diacritics/Dataset/val.txt\", batch_size = 1 )\n",
        "Validationdataloader = valdata.getdata()\n",
        "print(\"----------------- Validation data Loaded----------------\")\n",
        "\n",
        "# Validationdataloader=get_validation()\n",
        "\n",
        "inp_vocab_size = 37\n",
        "hidden_dim = 128\n",
        "seq_len = 300\n",
        "num_classes = 15\n",
        "\n",
        "model = LSTM(inp_vocab_size, hidden_dim, seq_len, num_classes)\n",
        "\n",
        "print(\"-------------------start training-------------------\")\n",
        "train(Traindataloader, model)\n",
        "\n",
        "print(\"-------------------start evaluating-------------------\")\n",
        "\n",
        "acc = evaluate_model(Validationdataloader, model)\n",
        "print(\"Accuracy: \", acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqgXGly72RLu",
        "outputId": "b6645f15-8123-4e8d-f20e-522c3aaeac62"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "epoch 28 batch 84 loss 0.06921812146902084\n",
            "epoch 28 batch 85 loss 0.06572571396827698\n",
            "epoch 28 batch 86 loss 0.06371429562568665\n",
            "epoch 28 batch 87 loss 0.06425383687019348\n",
            "epoch 28 batch 88 loss 0.06524693220853806\n",
            "epoch 28 batch 89 loss 0.06228915601968765\n",
            "epoch 28 batch 90 loss 0.06271766871213913\n",
            "epoch 28 batch 91 loss 0.06621242314577103\n",
            "epoch 28 batch 92 loss 0.06276986002922058\n",
            "epoch 28 batch 93 loss 0.0647362470626831\n",
            "epoch 28 batch 94 loss 0.06455430388450623\n",
            "epoch 28 batch 95 loss 0.06710965186357498\n",
            "epoch 28 batch 96 loss 0.06679923087358475\n",
            "epoch 28 batch 97 loss 0.06933280825614929\n",
            "epoch 28 batch 98 loss 0.07150765508413315\n",
            "epoch 28 batch 99 loss 0.06315736472606659\n",
            "epoch 28 batch 100 loss 0.06722758710384369\n",
            "epoch 28 batch 101 loss 0.06410891562700272\n",
            "epoch 28 batch 102 loss 0.06612001359462738\n",
            "epoch 28 batch 103 loss 0.06609189510345459\n",
            "epoch 28 batch 104 loss 0.06576810032129288\n",
            "epoch 28 batch 105 loss 0.057711608707904816\n",
            "epoch 28 batch 106 loss 0.06519404798746109\n",
            "epoch 28 batch 107 loss 0.06495482474565506\n",
            "epoch 28 batch 108 loss 0.062424030154943466\n",
            "epoch 28 batch 109 loss 0.07366254180669785\n",
            "epoch 28 batch 110 loss 0.06701413542032242\n",
            "epoch 28 batch 111 loss 0.06519876420497894\n",
            "epoch 28 batch 112 loss 0.06483292579650879\n",
            "epoch 28 batch 113 loss 0.06510486453771591\n",
            "epoch 28 batch 114 loss 0.06665923446416855\n",
            "epoch 28 batch 115 loss 0.0633162409067154\n",
            "epoch 28 batch 116 loss 0.06371091306209564\n",
            "epoch 28 batch 117 loss 0.06761405616998672\n",
            "epoch 28 batch 118 loss 0.06538059562444687\n",
            "epoch 28 batch 119 loss 0.06713329255580902\n",
            "epoch 28 batch 120 loss 0.062242843210697174\n",
            "epoch 28 batch 121 loss 0.06752567738294601\n",
            "epoch 28 batch 122 loss 0.05876467376947403\n",
            "epoch 28 batch 123 loss 0.06119583174586296\n",
            "epoch 28 batch 124 loss 0.0650506243109703\n",
            "epoch 28 batch 125 loss 0.06473483890295029\n",
            "epoch 28 batch 126 loss 0.06785481423139572\n",
            "epoch 28 batch 127 loss 0.06128639727830887\n",
            "epoch 28 batch 128 loss 0.06816235929727554\n",
            "epoch 28 batch 129 loss 0.06001070514321327\n",
            "epoch 28 batch 130 loss 0.06541848927736282\n",
            "epoch 28 batch 131 loss 0.06380656361579895\n",
            "epoch 28 batch 132 loss 0.06582627445459366\n",
            "epoch 28 batch 133 loss 0.06520187109708786\n",
            "epoch 28 batch 134 loss 0.061879321932792664\n",
            "epoch 28 batch 135 loss 0.06611590832471848\n",
            "epoch 28 batch 136 loss 0.06288544088602066\n",
            "epoch 28 batch 137 loss 0.070377878844738\n",
            "epoch 28 batch 138 loss 0.07037930190563202\n",
            "epoch 28 batch 139 loss 0.06268933415412903\n",
            "epoch 28 batch 140 loss 0.06749215722084045\n",
            "epoch 28 batch 141 loss 0.06237326189875603\n",
            "epoch 28 batch 142 loss 0.06680356711149216\n",
            "epoch 28 batch 143 loss 0.06467455625534058\n",
            "epoch 28 batch 144 loss 0.06990974396467209\n",
            "epoch 28 batch 145 loss 0.06825833767652512\n",
            "epoch 28 batch 146 loss 0.05829165130853653\n",
            "epoch 28 batch 147 loss 0.06270051002502441\n",
            "epoch 28 batch 148 loss 0.0679101049900055\n",
            "epoch 28 batch 149 loss 0.06896346807479858\n",
            "epoch 28 batch 150 loss 0.06409678608179092\n",
            "epoch 28 batch 151 loss 0.06208191439509392\n",
            "epoch 28 batch 152 loss 0.06626426428556442\n",
            "epoch 28 batch 153 loss 0.06659333407878876\n",
            "epoch 28 batch 154 loss 0.06440207362174988\n",
            "epoch 28 batch 155 loss 0.06365981698036194\n",
            "epoch 28 batch 156 loss 0.060294486582279205\n",
            "epoch 28 batch 157 loss 0.06060679629445076\n",
            "epoch 28 batch 158 loss 0.06813639402389526\n",
            "epoch 28 batch 159 loss 0.05960885435342789\n",
            "epoch 28 batch 160 loss 0.0685613825917244\n",
            "epoch 28 batch 161 loss 0.0638904720544815\n",
            "epoch 28 batch 162 loss 0.06833351403474808\n",
            "epoch 28 batch 163 loss 0.06523321568965912\n",
            "epoch 28 batch 164 loss 0.06251514703035355\n",
            "epoch 28 batch 165 loss 0.06306736171245575\n",
            "epoch 28 batch 166 loss 0.0680670514702797\n",
            "epoch 28 batch 167 loss 0.0625721737742424\n",
            "epoch 28 batch 168 loss 0.0689372643828392\n",
            "epoch 28 batch 169 loss 0.06778288632631302\n",
            "epoch 28 batch 170 loss 0.07072428613901138\n",
            "epoch 28 batch 171 loss 0.07039466500282288\n",
            "epoch 28 batch 172 loss 0.06221325695514679\n",
            "epoch 28 batch 173 loss 0.06367235630750656\n",
            "epoch 28 batch 174 loss 0.06474262475967407\n",
            "epoch 28 batch 175 loss 0.06523396819829941\n",
            "epoch 28 batch 176 loss 0.0647071972489357\n",
            "epoch 28 batch 177 loss 0.05856217071413994\n",
            "epoch 28 batch 178 loss 0.06406114995479584\n",
            "epoch 28 batch 179 loss 0.06788324564695358\n",
            "epoch 28 batch 180 loss 0.06603498756885529\n",
            "epoch 28 batch 181 loss 0.06423323601484299\n",
            "epoch 28 batch 182 loss 0.06518793851137161\n",
            "epoch 28 batch 183 loss 0.06369641423225403\n",
            "epoch 28 batch 184 loss 0.0644453838467598\n",
            "epoch 28 batch 185 loss 0.06435602158308029\n",
            "epoch 28 batch 186 loss 0.06537238508462906\n",
            "epoch 28 batch 187 loss 0.06535234302282333\n",
            "epoch 28 batch 188 loss 0.07294025272130966\n",
            "epoch 28 batch 189 loss 0.06710494309663773\n",
            "epoch 28 batch 190 loss 0.063885398209095\n",
            "epoch 28 batch 191 loss 0.06795386970043182\n",
            "epoch 28 batch 192 loss 0.06440017372369766\n",
            "epoch 28 batch 193 loss 0.06922847777605057\n",
            "epoch 28 batch 194 loss 0.05933321639895439\n",
            "epoch 28 batch 195 loss 0.0666486993432045\n",
            "epoch 28 batch 196 loss 0.06419479846954346\n",
            "epoch 28 batch 197 loss 0.06417006254196167\n",
            "epoch 28 batch 198 loss 0.0644872635602951\n",
            "epoch 28 batch 199 loss 0.06828386336565018\n",
            "epoch 28 batch 200 loss 0.06263279169797897\n",
            "epoch 28 batch 201 loss 0.05856451764702797\n",
            "epoch 28 batch 202 loss 0.060452405363321304\n",
            "epoch 28 batch 203 loss 0.06652401387691498\n",
            "epoch 28 batch 204 loss 0.06642889231443405\n",
            "epoch 28 batch 205 loss 0.06867203116416931\n",
            "epoch 28 batch 206 loss 0.06813246756792068\n",
            "epoch 28 batch 207 loss 0.07145937532186508\n",
            "epoch 28 batch 208 loss 0.06471243500709534\n",
            "epoch 28 batch 209 loss 0.07077422738075256\n",
            "epoch 28 batch 210 loss 0.06350361555814743\n",
            "epoch 28 batch 211 loss 0.06632915884256363\n",
            "epoch 28 batch 212 loss 0.06675176322460175\n",
            "epoch 28 batch 213 loss 0.06800638139247894\n",
            "epoch 28 batch 214 loss 0.06701871007680893\n",
            "epoch 28 batch 215 loss 0.06563650816679001\n",
            "epoch 28 batch 216 loss 0.06214708462357521\n",
            "epoch 28 batch 217 loss 0.06482738256454468\n",
            "epoch 28 batch 218 loss 0.06736740469932556\n",
            "epoch 28 batch 219 loss 0.06542783975601196\n",
            "epoch 28 batch 220 loss 0.06049535050988197\n",
            "epoch 28 batch 221 loss 0.06488121300935745\n",
            "epoch 28 batch 222 loss 0.06867318600416183\n",
            "epoch 28 batch 223 loss 0.06457832455635071\n",
            "epoch 28 batch 224 loss 0.0636860728263855\n",
            "epoch 28 batch 225 loss 0.061201032251119614\n",
            "epoch 28 batch 226 loss 0.06664231419563293\n",
            "epoch 28 batch 227 loss 0.06761962920427322\n",
            "epoch 28 batch 228 loss 0.06452523916959763\n",
            "epoch 28 batch 229 loss 0.06491575390100479\n",
            "epoch 28 batch 230 loss 0.06450726836919785\n",
            "epoch 29 batch 0 loss 0.06392466276884079\n",
            "epoch 29 batch 1 loss 0.06423182785511017\n",
            "epoch 29 batch 2 loss 0.06173199787735939\n",
            "epoch 29 batch 3 loss 0.06683626025915146\n",
            "epoch 29 batch 4 loss 0.0667559877038002\n",
            "epoch 29 batch 5 loss 0.060422345995903015\n",
            "epoch 29 batch 6 loss 0.05951434001326561\n",
            "epoch 29 batch 7 loss 0.05720466002821922\n",
            "epoch 29 batch 8 loss 0.06634371727705002\n",
            "epoch 29 batch 9 loss 0.060736168175935745\n",
            "epoch 29 batch 10 loss 0.06584922969341278\n",
            "epoch 29 batch 11 loss 0.058726418763399124\n",
            "epoch 29 batch 12 loss 0.06851089745759964\n",
            "epoch 29 batch 13 loss 0.060740139335393906\n",
            "epoch 29 batch 14 loss 0.06256243586540222\n",
            "epoch 29 batch 15 loss 0.059464193880558014\n",
            "epoch 29 batch 16 loss 0.06353665143251419\n",
            "epoch 29 batch 17 loss 0.0661134272813797\n",
            "epoch 29 batch 18 loss 0.05523865297436714\n",
            "epoch 29 batch 19 loss 0.06340886652469635\n",
            "epoch 29 batch 20 loss 0.0618724450469017\n",
            "epoch 29 batch 21 loss 0.06435322016477585\n",
            "epoch 29 batch 22 loss 0.05672341212630272\n",
            "epoch 29 batch 23 loss 0.06371138244867325\n",
            "epoch 29 batch 24 loss 0.0579981692135334\n",
            "epoch 29 batch 25 loss 0.06429120153188705\n",
            "epoch 29 batch 26 loss 0.06701939553022385\n",
            "epoch 29 batch 27 loss 0.06302788108587265\n",
            "epoch 29 batch 28 loss 0.061389949172735214\n",
            "epoch 29 batch 29 loss 0.06148909404873848\n",
            "epoch 29 batch 30 loss 0.06083974987268448\n",
            "epoch 29 batch 31 loss 0.06305089592933655\n",
            "epoch 29 batch 32 loss 0.06627999246120453\n",
            "epoch 29 batch 33 loss 0.0640200823545456\n",
            "epoch 29 batch 34 loss 0.05934235826134682\n",
            "epoch 29 batch 35 loss 0.0613606832921505\n",
            "epoch 29 batch 36 loss 0.060740720480680466\n",
            "epoch 29 batch 37 loss 0.06163904070854187\n",
            "epoch 29 batch 38 loss 0.07425934821367264\n",
            "epoch 29 batch 39 loss 0.0649518370628357\n",
            "epoch 29 batch 40 loss 0.06087923049926758\n",
            "epoch 29 batch 41 loss 0.06757743656635284\n",
            "epoch 29 batch 42 loss 0.0571516789495945\n",
            "epoch 29 batch 43 loss 0.06588868051767349\n",
            "epoch 29 batch 44 loss 0.0666155144572258\n",
            "epoch 29 batch 45 loss 0.07155036926269531\n",
            "epoch 29 batch 46 loss 0.06169041991233826\n",
            "epoch 29 batch 47 loss 0.0606672540307045\n",
            "epoch 29 batch 48 loss 0.06995194405317307\n",
            "epoch 29 batch 49 loss 0.06328553706407547\n",
            "epoch 29 batch 50 loss 0.06608378887176514\n",
            "epoch 29 batch 51 loss 0.0633341372013092\n",
            "epoch 29 batch 52 loss 0.06198808178305626\n",
            "epoch 29 batch 53 loss 0.06001649051904678\n",
            "epoch 29 batch 54 loss 0.06491237878799438\n",
            "epoch 29 batch 55 loss 0.0645584911108017\n",
            "epoch 29 batch 56 loss 0.0691661611199379\n",
            "epoch 29 batch 57 loss 0.059136804193258286\n",
            "epoch 29 batch 58 loss 0.0614333413541317\n",
            "epoch 29 batch 59 loss 0.06117828190326691\n",
            "epoch 29 batch 60 loss 0.06468535214662552\n",
            "epoch 29 batch 61 loss 0.06678474694490433\n",
            "epoch 29 batch 62 loss 0.059318892657756805\n",
            "epoch 29 batch 63 loss 0.06110985577106476\n",
            "epoch 29 batch 64 loss 0.059898409992456436\n",
            "epoch 29 batch 65 loss 0.06454413384199142\n",
            "epoch 29 batch 66 loss 0.061260808259248734\n",
            "epoch 29 batch 67 loss 0.06717181950807571\n",
            "epoch 29 batch 68 loss 0.06242646649479866\n",
            "epoch 29 batch 69 loss 0.057327449321746826\n",
            "epoch 29 batch 70 loss 0.06524534523487091\n",
            "epoch 29 batch 71 loss 0.06350323557853699\n",
            "epoch 29 batch 72 loss 0.05797119066119194\n",
            "epoch 29 batch 73 loss 0.06434682756662369\n",
            "epoch 29 batch 74 loss 0.06536460667848587\n",
            "epoch 29 batch 75 loss 0.06002555042505264\n",
            "epoch 29 batch 76 loss 0.05829516053199768\n",
            "epoch 29 batch 77 loss 0.06207725405693054\n",
            "epoch 29 batch 78 loss 0.06595734506845474\n",
            "epoch 29 batch 79 loss 0.06818241626024246\n",
            "epoch 29 batch 80 loss 0.06652643531560898\n",
            "epoch 29 batch 81 loss 0.065545953810215\n",
            "epoch 29 batch 82 loss 0.06788789480924606\n",
            "epoch 29 batch 83 loss 0.06777205318212509\n",
            "epoch 29 batch 84 loss 0.06698824465274811\n",
            "epoch 29 batch 85 loss 0.06609857827425003\n",
            "epoch 29 batch 86 loss 0.06887084245681763\n",
            "epoch 29 batch 87 loss 0.06169145554304123\n",
            "epoch 29 batch 88 loss 0.06339646130800247\n",
            "epoch 29 batch 89 loss 0.058537472039461136\n",
            "epoch 29 batch 90 loss 0.0699038878083229\n",
            "epoch 29 batch 91 loss 0.06487679481506348\n",
            "epoch 29 batch 92 loss 0.065207839012146\n",
            "epoch 29 batch 93 loss 0.06178554520010948\n",
            "epoch 29 batch 94 loss 0.06612934917211533\n",
            "epoch 29 batch 95 loss 0.06511447578668594\n",
            "epoch 29 batch 96 loss 0.06808687001466751\n",
            "epoch 29 batch 97 loss 0.06715107709169388\n",
            "epoch 29 batch 98 loss 0.06792694330215454\n",
            "epoch 29 batch 99 loss 0.06848931312561035\n",
            "epoch 29 batch 100 loss 0.06533044576644897\n",
            "epoch 29 batch 101 loss 0.06742933392524719\n",
            "epoch 29 batch 102 loss 0.06257890909910202\n",
            "epoch 29 batch 103 loss 0.067806176841259\n",
            "epoch 29 batch 104 loss 0.06179347261786461\n",
            "epoch 29 batch 105 loss 0.0673193410038948\n",
            "epoch 29 batch 106 loss 0.06357549130916595\n",
            "epoch 29 batch 107 loss 0.0618574284017086\n",
            "epoch 29 batch 108 loss 0.06970585882663727\n",
            "epoch 29 batch 109 loss 0.06874778121709824\n",
            "epoch 29 batch 110 loss 0.0650341659784317\n",
            "epoch 29 batch 111 loss 0.06405597925186157\n",
            "epoch 29 batch 112 loss 0.06868274509906769\n",
            "epoch 29 batch 113 loss 0.0681820660829544\n",
            "epoch 29 batch 114 loss 0.06588359922170639\n",
            "epoch 29 batch 115 loss 0.06764032691717148\n",
            "epoch 29 batch 116 loss 0.06624850630760193\n",
            "epoch 29 batch 117 loss 0.06586002558469772\n",
            "epoch 29 batch 118 loss 0.06599408388137817\n",
            "epoch 29 batch 119 loss 0.0602257214486599\n",
            "epoch 29 batch 120 loss 0.06863832473754883\n",
            "epoch 29 batch 121 loss 0.0656842440366745\n",
            "epoch 29 batch 122 loss 0.06566230207681656\n",
            "epoch 29 batch 123 loss 0.06357406079769135\n",
            "epoch 29 batch 124 loss 0.0659264400601387\n",
            "epoch 29 batch 125 loss 0.06790506094694138\n",
            "epoch 29 batch 126 loss 0.06822551786899567\n",
            "epoch 29 batch 127 loss 0.0629076287150383\n",
            "epoch 29 batch 128 loss 0.06352929025888443\n",
            "epoch 29 batch 129 loss 0.06766914576292038\n",
            "epoch 29 batch 130 loss 0.06529740244150162\n",
            "epoch 29 batch 131 loss 0.06506898999214172\n",
            "epoch 29 batch 132 loss 0.06651385128498077\n",
            "epoch 29 batch 133 loss 0.06639981269836426\n",
            "epoch 29 batch 134 loss 0.067124143242836\n",
            "epoch 29 batch 135 loss 0.06719359755516052\n",
            "epoch 29 batch 136 loss 0.0688103660941124\n",
            "epoch 29 batch 137 loss 0.06787523627281189\n",
            "epoch 29 batch 138 loss 0.06543786078691483\n",
            "epoch 29 batch 139 loss 0.06023027375340462\n",
            "epoch 29 batch 140 loss 0.06719598174095154\n",
            "epoch 29 batch 141 loss 0.06390469521284103\n",
            "epoch 29 batch 142 loss 0.06513892859220505\n",
            "epoch 29 batch 143 loss 0.06599437445402145\n",
            "epoch 29 batch 144 loss 0.062458399683237076\n",
            "epoch 29 batch 145 loss 0.061519380658864975\n",
            "epoch 29 batch 146 loss 0.07075897604227066\n",
            "epoch 29 batch 147 loss 0.06489922106266022\n",
            "epoch 29 batch 148 loss 0.06634841859340668\n",
            "epoch 29 batch 149 loss 0.06708662211894989\n",
            "epoch 29 batch 150 loss 0.06982318311929703\n",
            "epoch 29 batch 151 loss 0.06663299351930618\n",
            "epoch 29 batch 152 loss 0.059655241668224335\n",
            "epoch 29 batch 153 loss 0.06912533193826675\n",
            "epoch 29 batch 154 loss 0.06762858480215073\n",
            "epoch 29 batch 155 loss 0.06560414284467697\n",
            "epoch 29 batch 156 loss 0.058787353336811066\n",
            "epoch 29 batch 157 loss 0.06569354981184006\n",
            "epoch 29 batch 158 loss 0.06586260348558426\n",
            "epoch 29 batch 159 loss 0.06666868180036545\n",
            "epoch 29 batch 160 loss 0.064937524497509\n",
            "epoch 29 batch 161 loss 0.07009249925613403\n",
            "epoch 29 batch 162 loss 0.06531988084316254\n",
            "epoch 29 batch 163 loss 0.06926976144313812\n",
            "epoch 29 batch 164 loss 0.0635191947221756\n",
            "epoch 29 batch 165 loss 0.07586769759654999\n",
            "epoch 29 batch 166 loss 0.05856503173708916\n",
            "epoch 29 batch 167 loss 0.06827060878276825\n",
            "epoch 29 batch 168 loss 0.06989288330078125\n",
            "epoch 29 batch 169 loss 0.055222950875759125\n",
            "epoch 29 batch 170 loss 0.06253765523433685\n",
            "epoch 29 batch 171 loss 0.060804449021816254\n",
            "epoch 29 batch 172 loss 0.06516221165657043\n",
            "epoch 29 batch 173 loss 0.06719367951154709\n",
            "epoch 29 batch 174 loss 0.06024075299501419\n",
            "epoch 29 batch 175 loss 0.06526408344507217\n",
            "epoch 29 batch 176 loss 0.06694801896810532\n",
            "epoch 29 batch 177 loss 0.06346114724874496\n",
            "epoch 29 batch 178 loss 0.06293367594480515\n",
            "epoch 29 batch 179 loss 0.06287484616041183\n",
            "epoch 29 batch 180 loss 0.06464476883411407\n",
            "epoch 29 batch 181 loss 0.06154823303222656\n",
            "epoch 29 batch 182 loss 0.063310407102108\n",
            "epoch 29 batch 183 loss 0.06264689564704895\n",
            "epoch 29 batch 184 loss 0.06898948550224304\n",
            "epoch 29 batch 185 loss 0.06994841992855072\n",
            "epoch 29 batch 186 loss 0.06308098137378693\n",
            "epoch 29 batch 187 loss 0.062198251485824585\n",
            "epoch 29 batch 188 loss 0.0706198662519455\n",
            "epoch 29 batch 189 loss 0.06870818883180618\n",
            "epoch 29 batch 190 loss 0.06019710376858711\n",
            "epoch 29 batch 191 loss 0.06527022272348404\n",
            "epoch 29 batch 192 loss 0.060242947190999985\n",
            "epoch 29 batch 193 loss 0.06547896564006805\n",
            "epoch 29 batch 194 loss 0.06263599544763565\n",
            "epoch 29 batch 195 loss 0.06465306133031845\n",
            "epoch 29 batch 196 loss 0.058467503637075424\n",
            "epoch 29 batch 197 loss 0.06874971091747284\n",
            "epoch 29 batch 198 loss 0.06743278354406357\n",
            "epoch 29 batch 199 loss 0.061982858926057816\n",
            "epoch 29 batch 200 loss 0.06745395064353943\n",
            "epoch 29 batch 201 loss 0.06503696739673615\n",
            "epoch 29 batch 202 loss 0.06264190375804901\n",
            "epoch 29 batch 203 loss 0.06714604049921036\n",
            "epoch 29 batch 204 loss 0.06338854134082794\n",
            "epoch 29 batch 205 loss 0.06470093131065369\n",
            "epoch 29 batch 206 loss 0.06592457741498947\n",
            "epoch 29 batch 207 loss 0.06144191697239876\n",
            "epoch 29 batch 208 loss 0.06428729742765427\n",
            "epoch 29 batch 209 loss 0.06611678749322891\n",
            "epoch 29 batch 210 loss 0.06652489304542542\n",
            "epoch 29 batch 211 loss 0.05785830318927765\n",
            "epoch 29 batch 212 loss 0.05887610837817192\n",
            "epoch 29 batch 213 loss 0.06280458718538284\n",
            "epoch 29 batch 214 loss 0.0651441216468811\n",
            "epoch 29 batch 215 loss 0.06983178108930588\n",
            "epoch 29 batch 216 loss 0.0685587003827095\n",
            "epoch 29 batch 217 loss 0.05994955077767372\n",
            "epoch 29 batch 218 loss 0.06618766486644745\n",
            "epoch 29 batch 219 loss 0.06813068687915802\n",
            "epoch 29 batch 220 loss 0.0652635470032692\n",
            "epoch 29 batch 221 loss 0.06020248308777809\n",
            "epoch 29 batch 222 loss 0.07106837630271912\n",
            "epoch 29 batch 223 loss 0.06523452699184418\n",
            "epoch 29 batch 224 loss 0.06217121705412865\n",
            "epoch 29 batch 225 loss 0.0654919221997261\n",
            "epoch 29 batch 226 loss 0.0659014880657196\n",
            "epoch 29 batch 227 loss 0.06051505357027054\n",
            "epoch 29 batch 228 loss 0.06965987384319305\n",
            "epoch 29 batch 229 loss 0.06849420070648193\n",
            "epoch 29 batch 230 loss 0.08215627074241638\n",
            "epoch 30 batch 0 loss 0.06476391851902008\n",
            "epoch 30 batch 1 loss 0.06563863903284073\n",
            "epoch 30 batch 2 loss 0.06104990467429161\n",
            "epoch 30 batch 3 loss 0.06459781527519226\n",
            "epoch 30 batch 4 loss 0.06297633796930313\n",
            "epoch 30 batch 5 loss 0.06250392645597458\n",
            "epoch 30 batch 6 loss 0.06310047954320908\n",
            "epoch 30 batch 7 loss 0.060447972267866135\n",
            "epoch 30 batch 8 loss 0.0634029358625412\n",
            "epoch 30 batch 9 loss 0.05867592990398407\n",
            "epoch 30 batch 10 loss 0.06316052377223969\n",
            "epoch 30 batch 11 loss 0.06112861633300781\n",
            "epoch 30 batch 12 loss 0.0684700533747673\n",
            "epoch 30 batch 13 loss 0.06713636964559555\n",
            "epoch 30 batch 14 loss 0.06707968562841415\n",
            "epoch 30 batch 15 loss 0.06447622925043106\n",
            "epoch 30 batch 16 loss 0.05975639447569847\n",
            "epoch 30 batch 17 loss 0.06688041985034943\n",
            "epoch 30 batch 18 loss 0.06157468631863594\n",
            "epoch 30 batch 19 loss 0.057421062141656876\n",
            "epoch 30 batch 20 loss 0.0671362653374672\n",
            "epoch 30 batch 21 loss 0.06261678040027618\n",
            "epoch 30 batch 22 loss 0.06622879952192307\n",
            "epoch 30 batch 23 loss 0.06094972789287567\n",
            "epoch 30 batch 24 loss 0.06733042746782303\n",
            "epoch 30 batch 25 loss 0.05513998121023178\n",
            "epoch 30 batch 26 loss 0.06677465885877609\n",
            "epoch 30 batch 27 loss 0.0680389329791069\n",
            "epoch 30 batch 28 loss 0.06172653287649155\n",
            "epoch 30 batch 29 loss 0.061322491616010666\n",
            "epoch 30 batch 30 loss 0.06806567311286926\n",
            "epoch 30 batch 31 loss 0.06056057661771774\n",
            "epoch 30 batch 32 loss 0.05928219482302666\n",
            "epoch 30 batch 33 loss 0.0632491260766983\n",
            "epoch 30 batch 34 loss 0.06344995647668839\n",
            "epoch 30 batch 35 loss 0.06559699028730392\n",
            "epoch 30 batch 36 loss 0.05972069501876831\n",
            "epoch 30 batch 37 loss 0.06605471670627594\n",
            "epoch 30 batch 38 loss 0.06329888850450516\n",
            "epoch 30 batch 39 loss 0.06068361923098564\n",
            "epoch 30 batch 40 loss 0.06083051860332489\n",
            "epoch 30 batch 41 loss 0.06320061534643173\n",
            "epoch 30 batch 42 loss 0.06429143995046616\n",
            "epoch 30 batch 43 loss 0.056553032249212265\n",
            "epoch 30 batch 44 loss 0.064893439412117\n",
            "epoch 30 batch 45 loss 0.05422811582684517\n",
            "epoch 30 batch 46 loss 0.06001727283000946\n",
            "epoch 30 batch 47 loss 0.06263844668865204\n",
            "epoch 30 batch 48 loss 0.06402377039194107\n",
            "epoch 30 batch 49 loss 0.06078025698661804\n",
            "epoch 30 batch 50 loss 0.06173077970743179\n",
            "epoch 30 batch 51 loss 0.06618168950080872\n",
            "epoch 30 batch 52 loss 0.06527017056941986\n",
            "epoch 30 batch 53 loss 0.06036854535341263\n",
            "epoch 30 batch 54 loss 0.06869404762983322\n",
            "epoch 30 batch 55 loss 0.06392557919025421\n",
            "epoch 30 batch 56 loss 0.06445804238319397\n",
            "epoch 30 batch 57 loss 0.06689321994781494\n",
            "epoch 30 batch 58 loss 0.0633106604218483\n",
            "epoch 30 batch 59 loss 0.059476952999830246\n",
            "epoch 30 batch 60 loss 0.06453074514865875\n",
            "epoch 30 batch 61 loss 0.06091216951608658\n",
            "epoch 30 batch 62 loss 0.0641825869679451\n",
            "epoch 30 batch 63 loss 0.0593666136264801\n",
            "epoch 30 batch 64 loss 0.0666060820221901\n",
            "epoch 30 batch 65 loss 0.06606317311525345\n",
            "epoch 30 batch 66 loss 0.06106799468398094\n",
            "epoch 30 batch 67 loss 0.06266719102859497\n",
            "epoch 30 batch 68 loss 0.06825889647006989\n",
            "epoch 30 batch 69 loss 0.05886541306972504\n",
            "epoch 30 batch 70 loss 0.06595871597528458\n",
            "epoch 30 batch 71 loss 0.05995817109942436\n",
            "epoch 30 batch 72 loss 0.06824837625026703\n",
            "epoch 30 batch 73 loss 0.06660667061805725\n",
            "epoch 30 batch 74 loss 0.06994981318712234\n",
            "epoch 30 batch 75 loss 0.06564139574766159\n",
            "epoch 30 batch 76 loss 0.0625385046005249\n",
            "epoch 30 batch 77 loss 0.06744850426912308\n",
            "epoch 30 batch 78 loss 0.06065485626459122\n",
            "epoch 30 batch 79 loss 0.061675332486629486\n",
            "epoch 30 batch 80 loss 0.07154791057109833\n",
            "epoch 30 batch 81 loss 0.06594051420688629\n",
            "epoch 30 batch 82 loss 0.07362140715122223\n",
            "epoch 30 batch 83 loss 0.06299227476119995\n",
            "epoch 30 batch 84 loss 0.06746897101402283\n",
            "epoch 30 batch 85 loss 0.06518270820379257\n",
            "epoch 30 batch 86 loss 0.06878655403852463\n",
            "epoch 30 batch 87 loss 0.06619717180728912\n",
            "epoch 30 batch 88 loss 0.06494450569152832\n",
            "epoch 30 batch 89 loss 0.05969739332795143\n",
            "epoch 30 batch 90 loss 0.06301622092723846\n",
            "epoch 30 batch 91 loss 0.06457751989364624\n",
            "epoch 30 batch 92 loss 0.06200043484568596\n",
            "epoch 30 batch 93 loss 0.059175312519073486\n",
            "epoch 30 batch 94 loss 0.06202077865600586\n",
            "epoch 30 batch 95 loss 0.06824512779712677\n",
            "epoch 30 batch 96 loss 0.06530170142650604\n",
            "epoch 30 batch 97 loss 0.06365007907152176\n",
            "epoch 30 batch 98 loss 0.06676303595304489\n",
            "epoch 30 batch 99 loss 0.06833707541227341\n",
            "epoch 30 batch 100 loss 0.06592756509780884\n",
            "epoch 30 batch 101 loss 0.06256812810897827\n",
            "epoch 30 batch 102 loss 0.06116027757525444\n",
            "epoch 30 batch 103 loss 0.061433251947164536\n",
            "epoch 30 batch 104 loss 0.06316113471984863\n",
            "epoch 30 batch 105 loss 0.06605306267738342\n",
            "epoch 30 batch 106 loss 0.062147606164216995\n",
            "epoch 30 batch 107 loss 0.061033301055431366\n",
            "epoch 30 batch 108 loss 0.06490163505077362\n",
            "epoch 30 batch 109 loss 0.06467018276453018\n",
            "epoch 30 batch 110 loss 0.06380034238100052\n",
            "epoch 30 batch 111 loss 0.059982381761074066\n",
            "epoch 30 batch 112 loss 0.060042236000299454\n",
            "epoch 30 batch 113 loss 0.06905348598957062\n",
            "epoch 30 batch 114 loss 0.06096601486206055\n",
            "epoch 30 batch 115 loss 0.06403429806232452\n",
            "epoch 30 batch 116 loss 0.06509155035018921\n",
            "epoch 30 batch 117 loss 0.06629613041877747\n",
            "epoch 30 batch 118 loss 0.07138899713754654\n",
            "epoch 30 batch 119 loss 0.06211482360959053\n",
            "epoch 30 batch 120 loss 0.0693221390247345\n",
            "epoch 30 batch 121 loss 0.06533804535865784\n",
            "epoch 30 batch 122 loss 0.06507981568574905\n",
            "epoch 30 batch 123 loss 0.06956863403320312\n",
            "epoch 30 batch 124 loss 0.06862111389636993\n",
            "epoch 30 batch 125 loss 0.07176963984966278\n",
            "epoch 30 batch 126 loss 0.06946977972984314\n",
            "epoch 30 batch 127 loss 0.06983819603919983\n",
            "epoch 30 batch 128 loss 0.06328706443309784\n",
            "epoch 30 batch 129 loss 0.06265787035226822\n",
            "epoch 30 batch 130 loss 0.06518816947937012\n",
            "epoch 30 batch 131 loss 0.06243295595049858\n",
            "epoch 30 batch 132 loss 0.06867869198322296\n",
            "epoch 30 batch 133 loss 0.06316544115543365\n",
            "epoch 30 batch 134 loss 0.06285899132490158\n",
            "epoch 30 batch 135 loss 0.0580935999751091\n",
            "epoch 30 batch 136 loss 0.06459084898233414\n",
            "epoch 30 batch 137 loss 0.06764261424541473\n",
            "epoch 30 batch 138 loss 0.07153207063674927\n",
            "epoch 30 batch 139 loss 0.06990599632263184\n",
            "epoch 30 batch 140 loss 0.06830453872680664\n",
            "epoch 30 batch 141 loss 0.06538787484169006\n",
            "epoch 30 batch 142 loss 0.07005928456783295\n",
            "epoch 30 batch 143 loss 0.06332945823669434\n",
            "epoch 30 batch 144 loss 0.06117149442434311\n",
            "epoch 30 batch 145 loss 0.061905696988105774\n",
            "epoch 30 batch 146 loss 0.05789661407470703\n",
            "epoch 30 batch 147 loss 0.06776905059814453\n",
            "epoch 30 batch 148 loss 0.0689711943268776\n",
            "epoch 30 batch 149 loss 0.06672731786966324\n",
            "epoch 30 batch 150 loss 0.061700452119112015\n",
            "epoch 30 batch 151 loss 0.06951714307069778\n",
            "epoch 30 batch 152 loss 0.06239774823188782\n",
            "epoch 30 batch 153 loss 0.06693939119577408\n",
            "epoch 30 batch 154 loss 0.06278721988201141\n",
            "epoch 30 batch 155 loss 0.06459362804889679\n",
            "epoch 30 batch 156 loss 0.06419648975133896\n",
            "epoch 30 batch 157 loss 0.06749077141284943\n",
            "epoch 30 batch 158 loss 0.060379646718502045\n",
            "epoch 30 batch 159 loss 0.06062433868646622\n",
            "epoch 30 batch 160 loss 0.06913793832063675\n",
            "epoch 30 batch 161 loss 0.0701981633901596\n",
            "epoch 30 batch 162 loss 0.06315696239471436\n",
            "epoch 30 batch 163 loss 0.06525074690580368\n",
            "epoch 30 batch 164 loss 0.06198091432452202\n",
            "epoch 30 batch 165 loss 0.06342948973178864\n",
            "epoch 30 batch 166 loss 0.06894950568675995\n",
            "epoch 30 batch 167 loss 0.06745917350053787\n",
            "epoch 30 batch 168 loss 0.0656668022274971\n",
            "epoch 30 batch 169 loss 0.06711560487747192\n",
            "epoch 30 batch 170 loss 0.06201466917991638\n",
            "epoch 30 batch 171 loss 0.06604578346014023\n",
            "epoch 30 batch 172 loss 0.06833023577928543\n",
            "epoch 30 batch 173 loss 0.06531503051519394\n",
            "epoch 30 batch 174 loss 0.06394895911216736\n",
            "epoch 30 batch 175 loss 0.0637749657034874\n",
            "epoch 30 batch 176 loss 0.06575770676136017\n",
            "epoch 30 batch 177 loss 0.06561807543039322\n",
            "epoch 30 batch 178 loss 0.06547277420759201\n",
            "epoch 30 batch 179 loss 0.06518881022930145\n",
            "epoch 30 batch 180 loss 0.06656382232904434\n",
            "epoch 30 batch 181 loss 0.07241743057966232\n",
            "epoch 30 batch 182 loss 0.06613326072692871\n",
            "epoch 30 batch 183 loss 0.059448909014463425\n",
            "epoch 30 batch 184 loss 0.06243219971656799\n",
            "epoch 30 batch 185 loss 0.059946589171886444\n",
            "epoch 30 batch 186 loss 0.061151351779699326\n",
            "epoch 30 batch 187 loss 0.06032318249344826\n",
            "epoch 30 batch 188 loss 0.06733003258705139\n",
            "epoch 30 batch 189 loss 0.06609075516462326\n",
            "epoch 30 batch 190 loss 0.060102418065071106\n",
            "epoch 30 batch 191 loss 0.06206206604838371\n",
            "epoch 30 batch 192 loss 0.062130287289619446\n",
            "epoch 30 batch 193 loss 0.07092584669589996\n",
            "epoch 30 batch 194 loss 0.07337526977062225\n",
            "epoch 30 batch 195 loss 0.0636451244354248\n",
            "epoch 30 batch 196 loss 0.057309359312057495\n",
            "epoch 30 batch 197 loss 0.0642256811261177\n",
            "epoch 30 batch 198 loss 0.06596288084983826\n",
            "epoch 30 batch 199 loss 0.06273705512285233\n",
            "epoch 30 batch 200 loss 0.06327083706855774\n",
            "epoch 30 batch 201 loss 0.07228580117225647\n",
            "epoch 30 batch 202 loss 0.06385783106088638\n",
            "epoch 30 batch 203 loss 0.07069675624370575\n",
            "epoch 30 batch 204 loss 0.06285425275564194\n",
            "epoch 30 batch 205 loss 0.06182345747947693\n",
            "epoch 30 batch 206 loss 0.06646519899368286\n",
            "epoch 30 batch 207 loss 0.0665079727768898\n",
            "epoch 30 batch 208 loss 0.06346919387578964\n",
            "epoch 30 batch 209 loss 0.06546244770288467\n",
            "epoch 30 batch 210 loss 0.056031692773103714\n",
            "epoch 30 batch 211 loss 0.060893695801496506\n",
            "epoch 30 batch 212 loss 0.06865547597408295\n",
            "epoch 30 batch 213 loss 0.0644586905837059\n",
            "epoch 30 batch 214 loss 0.06657812744379044\n",
            "epoch 30 batch 215 loss 0.063321053981781\n",
            "epoch 30 batch 216 loss 0.05846930295228958\n",
            "epoch 30 batch 217 loss 0.06826014071702957\n",
            "epoch 30 batch 218 loss 0.06370849907398224\n",
            "epoch 30 batch 219 loss 0.06121426820755005\n",
            "epoch 30 batch 220 loss 0.06236749514937401\n",
            "epoch 30 batch 221 loss 0.0660393163561821\n",
            "epoch 30 batch 222 loss 0.06275700777769089\n",
            "epoch 30 batch 223 loss 0.06490969657897949\n",
            "epoch 30 batch 224 loss 0.062331900000572205\n",
            "epoch 30 batch 225 loss 0.05763053894042969\n",
            "epoch 30 batch 226 loss 0.06104159727692604\n",
            "epoch 30 batch 227 loss 0.06483687460422516\n",
            "epoch 30 batch 228 loss 0.06522362679243088\n",
            "epoch 30 batch 229 loss 0.06186201050877571\n",
            "epoch 30 batch 230 loss 0.06434724479913712\n",
            "epoch 31 batch 0 loss 0.052719343453645706\n",
            "epoch 31 batch 1 loss 0.06757689267396927\n",
            "epoch 31 batch 2 loss 0.06042734161019325\n",
            "epoch 31 batch 3 loss 0.06545279175043106\n",
            "epoch 31 batch 4 loss 0.06386830657720566\n",
            "epoch 31 batch 5 loss 0.06498164683580399\n",
            "epoch 31 batch 6 loss 0.0667584165930748\n",
            "epoch 31 batch 7 loss 0.06222376972436905\n",
            "epoch 31 batch 8 loss 0.06125868484377861\n",
            "epoch 31 batch 9 loss 0.05940980836749077\n",
            "epoch 31 batch 10 loss 0.07115907967090607\n",
            "epoch 31 batch 11 loss 0.06564587354660034\n",
            "epoch 31 batch 12 loss 0.06525345891714096\n",
            "epoch 31 batch 13 loss 0.061660874634981155\n",
            "epoch 31 batch 14 loss 0.06681613624095917\n",
            "epoch 31 batch 15 loss 0.0645141676068306\n",
            "epoch 31 batch 16 loss 0.06109088286757469\n",
            "epoch 31 batch 17 loss 0.06702858209609985\n",
            "epoch 31 batch 18 loss 0.0656224936246872\n",
            "epoch 31 batch 19 loss 0.06191353499889374\n",
            "epoch 31 batch 20 loss 0.061725176870822906\n",
            "epoch 31 batch 21 loss 0.05957057327032089\n",
            "epoch 31 batch 22 loss 0.06421743333339691\n",
            "epoch 31 batch 23 loss 0.056708794087171555\n",
            "epoch 31 batch 24 loss 0.055577851831912994\n",
            "epoch 31 batch 25 loss 0.07172413170337677\n",
            "epoch 31 batch 26 loss 0.06269558519124985\n",
            "epoch 31 batch 27 loss 0.06046213209629059\n",
            "epoch 31 batch 28 loss 0.058476708829402924\n",
            "epoch 31 batch 29 loss 0.0578315369784832\n",
            "epoch 31 batch 30 loss 0.05710301175713539\n",
            "epoch 31 batch 31 loss 0.0664549469947815\n",
            "epoch 31 batch 32 loss 0.056478943675756454\n",
            "epoch 31 batch 33 loss 0.053917113691568375\n",
            "epoch 31 batch 34 loss 0.06166434660553932\n",
            "epoch 31 batch 35 loss 0.05959004536271095\n",
            "epoch 31 batch 36 loss 0.06332860141992569\n",
            "epoch 31 batch 37 loss 0.06253509968519211\n",
            "epoch 31 batch 38 loss 0.06350298225879669\n",
            "epoch 31 batch 39 loss 0.05984422564506531\n",
            "epoch 31 batch 40 loss 0.06130479648709297\n",
            "epoch 31 batch 41 loss 0.06263875216245651\n",
            "epoch 31 batch 42 loss 0.0580289326608181\n",
            "epoch 31 batch 43 loss 0.06677082926034927\n",
            "epoch 31 batch 44 loss 0.06415373086929321\n",
            "epoch 31 batch 45 loss 0.062419116497039795\n",
            "epoch 31 batch 46 loss 0.06803898513317108\n",
            "epoch 31 batch 47 loss 0.06602770835161209\n",
            "epoch 31 batch 48 loss 0.06525509804487228\n",
            "epoch 31 batch 49 loss 0.06070428341627121\n",
            "epoch 31 batch 50 loss 0.0628962367773056\n",
            "epoch 31 batch 51 loss 0.06297223269939423\n",
            "epoch 31 batch 52 loss 0.060049571096897125\n",
            "epoch 31 batch 53 loss 0.06075415387749672\n",
            "epoch 31 batch 54 loss 0.062171000987291336\n",
            "epoch 31 batch 55 loss 0.06180128827691078\n",
            "epoch 31 batch 56 loss 0.06320440024137497\n",
            "epoch 31 batch 57 loss 0.06633149832487106\n",
            "epoch 31 batch 58 loss 0.06542704999446869\n",
            "epoch 31 batch 59 loss 0.06609508395195007\n",
            "epoch 31 batch 60 loss 0.07103962451219559\n",
            "epoch 31 batch 61 loss 0.06035522371530533\n",
            "epoch 31 batch 62 loss 0.06532671302556992\n",
            "epoch 31 batch 63 loss 0.06521496176719666\n",
            "epoch 31 batch 64 loss 0.06160919740796089\n",
            "epoch 31 batch 65 loss 0.06750275194644928\n",
            "epoch 31 batch 66 loss 0.05912252888083458\n",
            "epoch 31 batch 67 loss 0.06789403408765793\n",
            "epoch 31 batch 68 loss 0.056362222880125046\n",
            "epoch 31 batch 69 loss 0.06656430661678314\n",
            "epoch 31 batch 70 loss 0.0648166760802269\n",
            "epoch 31 batch 71 loss 0.06326480954885483\n",
            "epoch 31 batch 72 loss 0.07078272104263306\n",
            "epoch 31 batch 73 loss 0.07105471193790436\n",
            "epoch 31 batch 74 loss 0.05962606519460678\n",
            "epoch 31 batch 75 loss 0.06734547764062881\n",
            "epoch 31 batch 76 loss 0.06603645533323288\n",
            "epoch 31 batch 77 loss 0.061597347259521484\n",
            "epoch 31 batch 78 loss 0.06173018738627434\n",
            "epoch 31 batch 79 loss 0.06189599260687828\n",
            "epoch 31 batch 80 loss 0.06343119591474533\n",
            "epoch 31 batch 81 loss 0.0684560015797615\n",
            "epoch 31 batch 82 loss 0.062172241508960724\n",
            "epoch 31 batch 83 loss 0.06162397935986519\n",
            "epoch 31 batch 84 loss 0.06758466362953186\n",
            "epoch 31 batch 85 loss 0.05668117478489876\n",
            "epoch 31 batch 86 loss 0.06303422152996063\n",
            "epoch 31 batch 87 loss 0.06745345145463943\n",
            "epoch 31 batch 88 loss 0.06516878306865692\n",
            "epoch 31 batch 89 loss 0.05989697203040123\n",
            "epoch 31 batch 90 loss 0.06814073771238327\n",
            "epoch 31 batch 91 loss 0.061157453805208206\n",
            "epoch 31 batch 92 loss 0.059360429644584656\n",
            "epoch 31 batch 93 loss 0.06647558510303497\n",
            "epoch 31 batch 94 loss 0.07285932451486588\n",
            "epoch 31 batch 95 loss 0.05774650722742081\n",
            "epoch 31 batch 96 loss 0.06704334914684296\n",
            "epoch 31 batch 97 loss 0.06308890134096146\n",
            "epoch 31 batch 98 loss 0.06265842914581299\n",
            "epoch 31 batch 99 loss 0.06353538483381271\n",
            "epoch 31 batch 100 loss 0.06406603008508682\n",
            "epoch 31 batch 101 loss 0.05737273022532463\n",
            "epoch 31 batch 102 loss 0.06175261363387108\n",
            "epoch 31 batch 103 loss 0.06378614902496338\n",
            "epoch 31 batch 104 loss 0.06566309183835983\n",
            "epoch 31 batch 105 loss 0.06464491039514542\n",
            "epoch 31 batch 106 loss 0.061877671629190445\n",
            "epoch 31 batch 107 loss 0.06438392400741577\n",
            "epoch 31 batch 108 loss 0.06957802176475525\n",
            "epoch 31 batch 109 loss 0.06392988562583923\n",
            "epoch 31 batch 110 loss 0.06357733905315399\n",
            "epoch 31 batch 111 loss 0.06237293407320976\n",
            "epoch 31 batch 112 loss 0.06327540427446365\n",
            "epoch 31 batch 113 loss 0.06387222558259964\n",
            "epoch 31 batch 114 loss 0.05954619124531746\n",
            "epoch 31 batch 115 loss 0.05751625820994377\n",
            "epoch 31 batch 116 loss 0.0625908374786377\n",
            "epoch 31 batch 117 loss 0.06824441254138947\n",
            "epoch 31 batch 118 loss 0.05813783034682274\n",
            "epoch 31 batch 119 loss 0.06556310504674911\n",
            "epoch 31 batch 120 loss 0.06543660163879395\n",
            "epoch 31 batch 121 loss 0.06331706047058105\n",
            "epoch 31 batch 122 loss 0.06307126581668854\n",
            "epoch 31 batch 123 loss 0.06824564933776855\n",
            "epoch 31 batch 124 loss 0.05899962782859802\n",
            "epoch 31 batch 125 loss 0.06632766872644424\n",
            "epoch 31 batch 126 loss 0.06373321264982224\n",
            "epoch 31 batch 127 loss 0.06437356024980545\n",
            "epoch 31 batch 128 loss 0.06817563623189926\n",
            "epoch 31 batch 129 loss 0.06554834544658661\n",
            "epoch 31 batch 130 loss 0.06207454204559326\n",
            "epoch 31 batch 131 loss 0.05673419311642647\n",
            "epoch 31 batch 132 loss 0.06723342835903168\n",
            "epoch 31 batch 133 loss 0.06762498617172241\n",
            "epoch 31 batch 134 loss 0.0637364462018013\n",
            "epoch 31 batch 135 loss 0.0682755559682846\n",
            "epoch 31 batch 136 loss 0.05658084899187088\n",
            "epoch 31 batch 137 loss 0.06600146740674973\n",
            "epoch 31 batch 138 loss 0.06301605701446533\n",
            "epoch 31 batch 139 loss 0.0584593191742897\n",
            "epoch 31 batch 140 loss 0.06558798253536224\n",
            "epoch 31 batch 141 loss 0.07178179174661636\n",
            "epoch 31 batch 142 loss 0.06495527923107147\n",
            "epoch 31 batch 143 loss 0.06561795622110367\n",
            "epoch 31 batch 144 loss 0.061531346291303635\n",
            "epoch 31 batch 145 loss 0.06099458783864975\n",
            "epoch 31 batch 146 loss 0.06782456487417221\n",
            "epoch 31 batch 147 loss 0.06478458642959595\n",
            "epoch 31 batch 148 loss 0.06254049390554428\n",
            "epoch 31 batch 149 loss 0.06210019439458847\n",
            "epoch 31 batch 150 loss 0.0592983104288578\n",
            "epoch 31 batch 151 loss 0.07171186059713364\n",
            "epoch 31 batch 152 loss 0.06342440843582153\n",
            "epoch 31 batch 153 loss 0.073245570063591\n",
            "epoch 31 batch 154 loss 0.07084599882364273\n",
            "epoch 31 batch 155 loss 0.062019240111112595\n",
            "epoch 31 batch 156 loss 0.06192003935575485\n",
            "epoch 31 batch 157 loss 0.06079570949077606\n",
            "epoch 31 batch 158 loss 0.06571919471025467\n",
            "epoch 31 batch 159 loss 0.06247072294354439\n",
            "epoch 31 batch 160 loss 0.06558533012866974\n",
            "epoch 31 batch 161 loss 0.06280349940061569\n",
            "epoch 31 batch 162 loss 0.07045365124940872\n",
            "epoch 31 batch 163 loss 0.07134764641523361\n",
            "epoch 31 batch 164 loss 0.06760101020336151\n",
            "epoch 31 batch 165 loss 0.06459067016839981\n",
            "epoch 31 batch 166 loss 0.05953063443303108\n",
            "epoch 31 batch 167 loss 0.06091015040874481\n",
            "epoch 31 batch 168 loss 0.06485163420438766\n",
            "epoch 31 batch 169 loss 0.06166021525859833\n",
            "epoch 31 batch 170 loss 0.061178676784038544\n",
            "epoch 31 batch 171 loss 0.062002018094062805\n",
            "epoch 31 batch 172 loss 0.05836313217878342\n",
            "epoch 31 batch 173 loss 0.06754996627569199\n",
            "epoch 31 batch 174 loss 0.060227151960134506\n",
            "epoch 31 batch 175 loss 0.06298686563968658\n",
            "epoch 31 batch 176 loss 0.06301312148571014\n",
            "epoch 31 batch 177 loss 0.06141529232263565\n",
            "epoch 31 batch 178 loss 0.06641589105129242\n",
            "epoch 31 batch 179 loss 0.06504013389348984\n",
            "epoch 31 batch 180 loss 0.06115216016769409\n",
            "epoch 31 batch 181 loss 0.06437889486551285\n",
            "epoch 31 batch 182 loss 0.06786179542541504\n",
            "epoch 31 batch 183 loss 0.06085069105029106\n",
            "epoch 31 batch 184 loss 0.06485069543123245\n",
            "epoch 31 batch 185 loss 0.0669577345252037\n",
            "epoch 31 batch 186 loss 0.06470241397619247\n",
            "epoch 31 batch 187 loss 0.06413381546735764\n",
            "epoch 31 batch 188 loss 0.06069615110754967\n",
            "epoch 31 batch 189 loss 0.065325066447258\n",
            "epoch 31 batch 190 loss 0.06679169833660126\n",
            "epoch 31 batch 191 loss 0.06761414557695389\n",
            "epoch 31 batch 192 loss 0.06404392421245575\n",
            "epoch 31 batch 193 loss 0.06584581732749939\n",
            "epoch 31 batch 194 loss 0.06607628613710403\n",
            "epoch 31 batch 195 loss 0.06976919621229172\n",
            "epoch 31 batch 196 loss 0.06485031545162201\n",
            "epoch 31 batch 197 loss 0.06576645374298096\n",
            "epoch 31 batch 198 loss 0.06549273431301117\n",
            "epoch 31 batch 199 loss 0.06670134514570236\n",
            "epoch 31 batch 200 loss 0.06590438634157181\n",
            "epoch 31 batch 201 loss 0.059609171003103256\n",
            "epoch 31 batch 202 loss 0.06166272982954979\n",
            "epoch 31 batch 203 loss 0.0739818811416626\n",
            "epoch 31 batch 204 loss 0.0666486993432045\n",
            "epoch 31 batch 205 loss 0.057638395577669144\n",
            "epoch 31 batch 206 loss 0.06631511449813843\n",
            "epoch 31 batch 207 loss 0.06302006542682648\n",
            "epoch 31 batch 208 loss 0.06592540442943573\n",
            "epoch 31 batch 209 loss 0.06586100906133652\n",
            "epoch 31 batch 210 loss 0.06359662860631943\n",
            "epoch 31 batch 211 loss 0.0633707121014595\n",
            "epoch 31 batch 212 loss 0.06452906131744385\n",
            "epoch 31 batch 213 loss 0.06632189452648163\n",
            "epoch 31 batch 214 loss 0.06745873391628265\n",
            "epoch 31 batch 215 loss 0.06480269879102707\n",
            "epoch 31 batch 216 loss 0.07193870097398758\n",
            "epoch 31 batch 217 loss 0.06733963638544083\n",
            "epoch 31 batch 218 loss 0.06794681400060654\n",
            "epoch 31 batch 219 loss 0.06538539379835129\n",
            "epoch 31 batch 220 loss 0.06921347975730896\n",
            "epoch 31 batch 221 loss 0.06741955876350403\n",
            "epoch 31 batch 222 loss 0.0711628720164299\n",
            "epoch 31 batch 223 loss 0.06106799468398094\n",
            "epoch 31 batch 224 loss 0.0660439282655716\n",
            "epoch 31 batch 225 loss 0.06692758202552795\n",
            "epoch 31 batch 226 loss 0.06529352068901062\n",
            "epoch 31 batch 227 loss 0.06928085535764694\n",
            "epoch 31 batch 228 loss 0.0650811418890953\n",
            "epoch 31 batch 229 loss 0.06097148358821869\n",
            "epoch 31 batch 230 loss 0.05374850332736969\n",
            "epoch 32 batch 0 loss 0.06745865195989609\n",
            "epoch 32 batch 1 loss 0.05794408917427063\n",
            "epoch 32 batch 2 loss 0.06155434250831604\n",
            "epoch 32 batch 3 loss 0.057671692222356796\n",
            "epoch 32 batch 4 loss 0.06389253586530685\n",
            "epoch 32 batch 5 loss 0.06586103141307831\n",
            "epoch 32 batch 6 loss 0.06211976334452629\n",
            "epoch 32 batch 7 loss 0.06869035959243774\n",
            "epoch 32 batch 8 loss 0.06766533106565475\n",
            "epoch 32 batch 9 loss 0.0697934702038765\n",
            "epoch 32 batch 10 loss 0.06680411100387573\n",
            "epoch 32 batch 11 loss 0.06308446079492569\n",
            "epoch 32 batch 12 loss 0.06000348925590515\n",
            "epoch 32 batch 13 loss 0.06384045630693436\n",
            "epoch 32 batch 14 loss 0.060115959495306015\n",
            "epoch 32 batch 15 loss 0.06616669148206711\n",
            "epoch 32 batch 16 loss 0.05898841843008995\n",
            "epoch 32 batch 17 loss 0.059480585157871246\n",
            "epoch 32 batch 18 loss 0.06361187249422073\n",
            "epoch 32 batch 19 loss 0.06451214104890823\n",
            "epoch 32 batch 20 loss 0.05815885588526726\n",
            "epoch 32 batch 21 loss 0.06564320623874664\n",
            "epoch 32 batch 22 loss 0.06025491654872894\n",
            "epoch 32 batch 23 loss 0.06493648141622543\n",
            "epoch 32 batch 24 loss 0.0627904161810875\n",
            "epoch 32 batch 25 loss 0.06552552431821823\n",
            "epoch 32 batch 26 loss 0.059460900723934174\n",
            "epoch 32 batch 27 loss 0.067231684923172\n",
            "epoch 32 batch 28 loss 0.0641513541340828\n",
            "epoch 32 batch 29 loss 0.06344505399465561\n",
            "epoch 32 batch 30 loss 0.06198212131857872\n",
            "epoch 32 batch 31 loss 0.06522034853696823\n",
            "epoch 32 batch 32 loss 0.059817124158144\n",
            "epoch 32 batch 33 loss 0.05962619185447693\n",
            "epoch 32 batch 34 loss 0.06015783175826073\n",
            "epoch 32 batch 35 loss 0.06984565407037735\n",
            "epoch 32 batch 36 loss 0.06354013830423355\n",
            "epoch 32 batch 37 loss 0.05962830409407616\n",
            "epoch 32 batch 38 loss 0.06365051865577698\n",
            "epoch 32 batch 39 loss 0.06336992979049683\n",
            "epoch 32 batch 40 loss 0.06786208599805832\n",
            "epoch 32 batch 41 loss 0.0663120299577713\n",
            "epoch 32 batch 42 loss 0.06332746148109436\n",
            "epoch 32 batch 43 loss 0.06672536581754684\n",
            "epoch 32 batch 44 loss 0.06328438222408295\n",
            "epoch 32 batch 45 loss 0.060909874737262726\n",
            "epoch 32 batch 46 loss 0.06152791157364845\n",
            "epoch 32 batch 47 loss 0.05963419750332832\n",
            "epoch 32 batch 48 loss 0.06232529878616333\n",
            "epoch 32 batch 49 loss 0.062230903655290604\n",
            "epoch 32 batch 50 loss 0.06277915090322495\n",
            "epoch 32 batch 51 loss 0.06473013013601303\n",
            "epoch 32 batch 52 loss 0.06475051492452621\n",
            "epoch 32 batch 53 loss 0.059010136872529984\n",
            "epoch 32 batch 54 loss 0.06749121844768524\n",
            "epoch 32 batch 55 loss 0.05839158222079277\n",
            "epoch 32 batch 56 loss 0.06428776681423187\n",
            "epoch 32 batch 57 loss 0.06273414939641953\n",
            "epoch 32 batch 58 loss 0.0563986711204052\n",
            "epoch 32 batch 59 loss 0.06238932907581329\n",
            "epoch 32 batch 60 loss 0.0657941922545433\n",
            "epoch 32 batch 61 loss 0.06250288337469101\n",
            "epoch 32 batch 62 loss 0.05847933515906334\n",
            "epoch 32 batch 63 loss 0.0633740946650505\n",
            "epoch 32 batch 64 loss 0.061893168836832047\n",
            "epoch 32 batch 65 loss 0.06319793313741684\n",
            "epoch 32 batch 66 loss 0.06418631970882416\n",
            "epoch 32 batch 67 loss 0.058713410049676895\n",
            "epoch 32 batch 68 loss 0.06291809678077698\n",
            "epoch 32 batch 69 loss 0.0695875808596611\n",
            "epoch 32 batch 70 loss 0.0640714168548584\n",
            "epoch 32 batch 71 loss 0.06169204041361809\n",
            "epoch 32 batch 72 loss 0.05940130725502968\n",
            "epoch 32 batch 73 loss 0.06621859967708588\n",
            "epoch 32 batch 74 loss 0.06715317815542221\n",
            "epoch 32 batch 75 loss 0.0630914717912674\n",
            "epoch 32 batch 76 loss 0.06407606601715088\n",
            "epoch 32 batch 77 loss 0.06425139307975769\n",
            "epoch 32 batch 78 loss 0.06640858948230743\n",
            "epoch 32 batch 79 loss 0.06179238483309746\n",
            "epoch 32 batch 80 loss 0.062372997403144836\n",
            "epoch 32 batch 81 loss 0.06608029454946518\n",
            "epoch 32 batch 82 loss 0.07040224969387054\n",
            "epoch 32 batch 83 loss 0.05442056059837341\n",
            "epoch 32 batch 84 loss 0.06074649095535278\n",
            "epoch 32 batch 85 loss 0.06472907215356827\n",
            "epoch 32 batch 86 loss 0.06533139199018478\n",
            "epoch 32 batch 87 loss 0.0573551245033741\n",
            "epoch 32 batch 88 loss 0.062453631311655045\n",
            "epoch 32 batch 89 loss 0.06552022695541382\n",
            "epoch 32 batch 90 loss 0.06726323068141937\n",
            "epoch 32 batch 91 loss 0.06326985359191895\n",
            "epoch 32 batch 92 loss 0.06287872046232224\n",
            "epoch 32 batch 93 loss 0.06416212767362595\n",
            "epoch 32 batch 94 loss 0.06166236102581024\n",
            "epoch 32 batch 95 loss 0.06018121540546417\n",
            "epoch 32 batch 96 loss 0.06708861887454987\n",
            "epoch 32 batch 97 loss 0.0654674619436264\n",
            "epoch 32 batch 98 loss 0.06680928170681\n",
            "epoch 32 batch 99 loss 0.061556197702884674\n",
            "epoch 32 batch 100 loss 0.06813259422779083\n",
            "epoch 32 batch 101 loss 0.06551291793584824\n",
            "epoch 32 batch 102 loss 0.0662313774228096\n",
            "epoch 32 batch 103 loss 0.06118664890527725\n",
            "epoch 32 batch 104 loss 0.055364057421684265\n",
            "epoch 32 batch 105 loss 0.061730410903692245\n",
            "epoch 32 batch 106 loss 0.06398559361696243\n",
            "epoch 32 batch 107 loss 0.06288604438304901\n",
            "epoch 32 batch 108 loss 0.06317451596260071\n",
            "epoch 32 batch 109 loss 0.06203455105423927\n",
            "epoch 32 batch 110 loss 0.058696199208498\n",
            "epoch 32 batch 111 loss 0.061694711446762085\n",
            "epoch 32 batch 112 loss 0.06686927378177643\n",
            "epoch 32 batch 113 loss 0.06627921760082245\n",
            "epoch 32 batch 114 loss 0.06149296462535858\n",
            "epoch 32 batch 115 loss 0.064231276512146\n",
            "epoch 32 batch 116 loss 0.06150849163532257\n",
            "epoch 32 batch 117 loss 0.06323378533124924\n",
            "epoch 32 batch 118 loss 0.06276421248912811\n",
            "epoch 32 batch 119 loss 0.06600233912467957\n",
            "epoch 32 batch 120 loss 0.06304106116294861\n",
            "epoch 32 batch 121 loss 0.06022218242287636\n",
            "epoch 32 batch 122 loss 0.06543999165296555\n",
            "epoch 32 batch 123 loss 0.06662766635417938\n",
            "epoch 32 batch 124 loss 0.05743904039263725\n",
            "epoch 32 batch 125 loss 0.06173454970121384\n",
            "epoch 32 batch 126 loss 0.06694626808166504\n",
            "epoch 32 batch 127 loss 0.06643904745578766\n",
            "epoch 32 batch 128 loss 0.06344066560268402\n",
            "epoch 32 batch 129 loss 0.061845485121011734\n",
            "epoch 32 batch 130 loss 0.06489857286214828\n",
            "epoch 32 batch 131 loss 0.06063088774681091\n",
            "epoch 32 batch 132 loss 0.0700065940618515\n",
            "epoch 32 batch 133 loss 0.06483872979879379\n",
            "epoch 32 batch 134 loss 0.05311274901032448\n",
            "epoch 32 batch 135 loss 0.06895564496517181\n",
            "epoch 32 batch 136 loss 0.06362530589103699\n",
            "epoch 32 batch 137 loss 0.06246747449040413\n",
            "epoch 32 batch 138 loss 0.05995824187994003\n",
            "epoch 32 batch 139 loss 0.0603802315890789\n",
            "epoch 32 batch 140 loss 0.06661020219326019\n",
            "epoch 32 batch 141 loss 0.07053913921117783\n",
            "epoch 32 batch 142 loss 0.0630372017621994\n",
            "epoch 32 batch 143 loss 0.06145484372973442\n",
            "epoch 32 batch 144 loss 0.06179989501833916\n",
            "epoch 32 batch 145 loss 0.0661747008562088\n",
            "epoch 32 batch 146 loss 0.06755383312702179\n",
            "epoch 32 batch 147 loss 0.06660294532775879\n",
            "epoch 32 batch 148 loss 0.0621693916618824\n",
            "epoch 32 batch 149 loss 0.06932025402784348\n",
            "epoch 32 batch 150 loss 0.05802205950021744\n",
            "epoch 32 batch 151 loss 0.06561074405908585\n",
            "epoch 32 batch 152 loss 0.05862997844815254\n",
            "epoch 32 batch 153 loss 0.06340125948190689\n",
            "epoch 32 batch 154 loss 0.05768287554383278\n",
            "epoch 32 batch 155 loss 0.0676400437951088\n",
            "epoch 32 batch 156 loss 0.06701111793518066\n",
            "epoch 32 batch 157 loss 0.06600923836231232\n",
            "epoch 32 batch 158 loss 0.06309133023023605\n",
            "epoch 32 batch 159 loss 0.06982926279306412\n",
            "epoch 32 batch 160 loss 0.06632157415151596\n",
            "epoch 32 batch 161 loss 0.06823728233575821\n",
            "epoch 32 batch 162 loss 0.05943482741713524\n",
            "epoch 32 batch 163 loss 0.06514919549226761\n",
            "epoch 32 batch 164 loss 0.06568560004234314\n",
            "epoch 32 batch 165 loss 0.06650881469249725\n",
            "epoch 32 batch 166 loss 0.06734348088502884\n",
            "epoch 32 batch 167 loss 0.06722794473171234\n",
            "epoch 32 batch 168 loss 0.061920590698719025\n",
            "epoch 32 batch 169 loss 0.06570252776145935\n",
            "epoch 32 batch 170 loss 0.06009179353713989\n",
            "epoch 32 batch 171 loss 0.06849567592144012\n",
            "epoch 32 batch 172 loss 0.0613495372235775\n",
            "epoch 32 batch 173 loss 0.06686490029096603\n",
            "epoch 32 batch 174 loss 0.06298089027404785\n",
            "epoch 32 batch 175 loss 0.06096312776207924\n",
            "epoch 32 batch 176 loss 0.06416936218738556\n",
            "epoch 32 batch 177 loss 0.05749626085162163\n",
            "epoch 32 batch 178 loss 0.056795552372932434\n",
            "epoch 32 batch 179 loss 0.07026845216751099\n",
            "epoch 32 batch 180 loss 0.07083530724048615\n",
            "epoch 32 batch 181 loss 0.06371798366308212\n",
            "epoch 32 batch 182 loss 0.06701350212097168\n",
            "epoch 32 batch 183 loss 0.0636746883392334\n",
            "epoch 32 batch 184 loss 0.0626811608672142\n",
            "epoch 32 batch 185 loss 0.06374981254339218\n",
            "epoch 32 batch 186 loss 0.06337903439998627\n",
            "epoch 32 batch 187 loss 0.0656241923570633\n",
            "epoch 32 batch 188 loss 0.06143078953027725\n",
            "epoch 32 batch 189 loss 0.05610400065779686\n",
            "epoch 32 batch 190 loss 0.06718283891677856\n",
            "epoch 32 batch 191 loss 0.06908342242240906\n",
            "epoch 32 batch 192 loss 0.0653991773724556\n",
            "epoch 32 batch 193 loss 0.06362173706293106\n",
            "epoch 32 batch 194 loss 0.059826985001564026\n",
            "epoch 32 batch 195 loss 0.06325637549161911\n",
            "epoch 32 batch 196 loss 0.06432104110717773\n",
            "epoch 32 batch 197 loss 0.06963745504617691\n",
            "epoch 32 batch 198 loss 0.06578708440065384\n",
            "epoch 32 batch 199 loss 0.06523477286100388\n",
            "epoch 32 batch 200 loss 0.0684698075056076\n",
            "epoch 32 batch 201 loss 0.06262456625699997\n",
            "epoch 32 batch 202 loss 0.06322826445102692\n",
            "epoch 32 batch 203 loss 0.06111225113272667\n",
            "epoch 32 batch 204 loss 0.07146899402141571\n",
            "epoch 32 batch 205 loss 0.06408845633268356\n",
            "epoch 32 batch 206 loss 0.06531248986721039\n",
            "epoch 32 batch 207 loss 0.0716940239071846\n",
            "epoch 32 batch 208 loss 0.06849166005849838\n",
            "epoch 32 batch 209 loss 0.059496499598026276\n",
            "epoch 32 batch 210 loss 0.05758241191506386\n",
            "epoch 32 batch 211 loss 0.06470701098442078\n",
            "epoch 32 batch 212 loss 0.061021994799375534\n",
            "epoch 32 batch 213 loss 0.060132987797260284\n",
            "epoch 32 batch 214 loss 0.0673665702342987\n",
            "epoch 32 batch 215 loss 0.07130802422761917\n",
            "epoch 32 batch 216 loss 0.06649544090032578\n",
            "epoch 32 batch 217 loss 0.07181832939386368\n",
            "epoch 32 batch 218 loss 0.062007784843444824\n",
            "epoch 32 batch 219 loss 0.06104831025004387\n",
            "epoch 32 batch 220 loss 0.06147560849785805\n",
            "epoch 32 batch 221 loss 0.06582436710596085\n",
            "epoch 32 batch 222 loss 0.062436543405056\n",
            "epoch 32 batch 223 loss 0.06480160355567932\n",
            "epoch 32 batch 224 loss 0.06218292936682701\n",
            "epoch 32 batch 225 loss 0.062345486134290695\n",
            "epoch 32 batch 226 loss 0.061787787824869156\n",
            "epoch 32 batch 227 loss 0.06249598041176796\n",
            "epoch 32 batch 228 loss 0.06701004505157471\n",
            "epoch 32 batch 229 loss 0.06375181674957275\n",
            "epoch 32 batch 230 loss 0.057734597474336624\n",
            "epoch 33 batch 0 loss 0.06620155274868011\n",
            "epoch 33 batch 1 loss 0.06555265933275223\n",
            "epoch 33 batch 2 loss 0.06309116631746292\n",
            "epoch 33 batch 3 loss 0.05958544462919235\n",
            "epoch 33 batch 4 loss 0.05872860550880432\n",
            "epoch 33 batch 5 loss 0.060706425458192825\n",
            "epoch 33 batch 6 loss 0.060631897300481796\n",
            "epoch 33 batch 7 loss 0.06565820425748825\n",
            "epoch 33 batch 8 loss 0.06175640597939491\n",
            "epoch 33 batch 9 loss 0.060321349650621414\n",
            "epoch 33 batch 10 loss 0.06437914073467255\n",
            "epoch 33 batch 11 loss 0.06111706420779228\n",
            "epoch 33 batch 12 loss 0.05972936749458313\n",
            "epoch 33 batch 13 loss 0.06732535362243652\n",
            "epoch 33 batch 14 loss 0.06600877642631531\n",
            "epoch 33 batch 15 loss 0.06418310105800629\n",
            "epoch 33 batch 16 loss 0.06298992037773132\n",
            "epoch 33 batch 17 loss 0.0684279203414917\n",
            "epoch 33 batch 18 loss 0.06602627784013748\n",
            "epoch 33 batch 19 loss 0.06234784424304962\n",
            "epoch 33 batch 20 loss 0.06620293110609055\n",
            "epoch 33 batch 21 loss 0.060172613710165024\n",
            "epoch 33 batch 22 loss 0.05825958400964737\n",
            "epoch 33 batch 23 loss 0.05860745161771774\n",
            "epoch 33 batch 24 loss 0.06659730523824692\n",
            "epoch 33 batch 25 loss 0.06334734708070755\n",
            "epoch 33 batch 26 loss 0.06997093558311462\n",
            "epoch 33 batch 27 loss 0.06110716983675957\n",
            "epoch 33 batch 28 loss 0.06393127143383026\n",
            "epoch 33 batch 29 loss 0.06519012153148651\n",
            "epoch 33 batch 30 loss 0.06616473942995071\n",
            "epoch 33 batch 31 loss 0.05637276917695999\n",
            "epoch 33 batch 32 loss 0.06141720339655876\n",
            "epoch 33 batch 33 loss 0.06804659217596054\n",
            "epoch 33 batch 34 loss 0.05834898725152016\n",
            "epoch 33 batch 35 loss 0.0650092214345932\n",
            "epoch 33 batch 36 loss 0.058921586722135544\n",
            "epoch 33 batch 37 loss 0.0643443763256073\n",
            "epoch 33 batch 38 loss 0.06562122702598572\n",
            "epoch 33 batch 39 loss 0.05595675855875015\n",
            "epoch 33 batch 40 loss 0.06632398068904877\n",
            "epoch 33 batch 41 loss 0.05793079361319542\n",
            "epoch 33 batch 42 loss 0.0591387040913105\n",
            "epoch 33 batch 43 loss 0.05515161529183388\n",
            "epoch 33 batch 44 loss 0.06683576852083206\n",
            "epoch 33 batch 45 loss 0.0639844462275505\n",
            "epoch 33 batch 46 loss 0.06657787412405014\n",
            "epoch 33 batch 47 loss 0.06219656020402908\n",
            "epoch 33 batch 48 loss 0.061680521816015244\n",
            "epoch 33 batch 49 loss 0.058108095079660416\n",
            "epoch 33 batch 50 loss 0.05674692615866661\n",
            "epoch 33 batch 51 loss 0.06309696286916733\n",
            "epoch 33 batch 52 loss 0.058364707976579666\n",
            "epoch 33 batch 53 loss 0.0620439387857914\n",
            "epoch 33 batch 54 loss 0.06627091765403748\n",
            "epoch 33 batch 55 loss 0.05872805789113045\n",
            "epoch 33 batch 56 loss 0.05755329877138138\n",
            "epoch 33 batch 57 loss 0.06278644502162933\n",
            "epoch 33 batch 58 loss 0.06355934590101242\n",
            "epoch 33 batch 59 loss 0.0604369156062603\n",
            "epoch 33 batch 60 loss 0.061864450573921204\n",
            "epoch 33 batch 61 loss 0.06328021734952927\n",
            "epoch 33 batch 62 loss 0.06389159709215164\n",
            "epoch 33 batch 63 loss 0.06126883253455162\n",
            "epoch 33 batch 64 loss 0.06539782136678696\n",
            "epoch 33 batch 65 loss 0.06296905875205994\n",
            "epoch 33 batch 66 loss 0.060145746916532516\n",
            "epoch 33 batch 67 loss 0.06194169819355011\n",
            "epoch 33 batch 68 loss 0.056435782462358475\n",
            "epoch 33 batch 69 loss 0.06295552849769592\n",
            "epoch 33 batch 70 loss 0.06933873146772385\n",
            "epoch 33 batch 71 loss 0.06342343986034393\n",
            "epoch 33 batch 72 loss 0.06669046729803085\n",
            "epoch 33 batch 73 loss 0.06587672233581543\n",
            "epoch 33 batch 74 loss 0.06571726500988007\n",
            "epoch 33 batch 75 loss 0.06470131874084473\n",
            "epoch 33 batch 76 loss 0.0639156624674797\n",
            "epoch 33 batch 77 loss 0.05922812595963478\n",
            "epoch 33 batch 78 loss 0.061513569205999374\n",
            "epoch 33 batch 79 loss 0.06942859292030334\n",
            "epoch 33 batch 80 loss 0.060248102992773056\n",
            "epoch 33 batch 81 loss 0.06066600978374481\n",
            "epoch 33 batch 82 loss 0.05570133030414581\n",
            "epoch 33 batch 83 loss 0.0671023353934288\n",
            "epoch 33 batch 84 loss 0.0639064759016037\n",
            "epoch 33 batch 85 loss 0.07141053676605225\n",
            "epoch 33 batch 86 loss 0.05928521603345871\n",
            "epoch 33 batch 87 loss 0.0599740669131279\n",
            "epoch 33 batch 88 loss 0.061770979315042496\n",
            "epoch 33 batch 89 loss 0.06341304630041122\n",
            "epoch 33 batch 90 loss 0.06701511889696121\n",
            "epoch 33 batch 91 loss 0.06643232703208923\n",
            "epoch 33 batch 92 loss 0.0653851330280304\n",
            "epoch 33 batch 93 loss 0.05904768779873848\n",
            "epoch 33 batch 94 loss 0.062116559594869614\n",
            "epoch 33 batch 95 loss 0.05887872725725174\n",
            "epoch 33 batch 96 loss 0.06196264177560806\n",
            "epoch 33 batch 97 loss 0.06715077906847\n",
            "epoch 33 batch 98 loss 0.06407405436038971\n",
            "epoch 33 batch 99 loss 0.061512693762779236\n",
            "epoch 33 batch 100 loss 0.07116644829511642\n",
            "epoch 33 batch 101 loss 0.05719643831253052\n",
            "epoch 33 batch 102 loss 0.05641799792647362\n",
            "epoch 33 batch 103 loss 0.05883253738284111\n",
            "epoch 33 batch 104 loss 0.06182122603058815\n",
            "epoch 33 batch 105 loss 0.06637339293956757\n",
            "epoch 33 batch 106 loss 0.06557398289442062\n",
            "epoch 33 batch 107 loss 0.06556608527898788\n",
            "epoch 33 batch 108 loss 0.06488820910453796\n",
            "epoch 33 batch 109 loss 0.0642809197306633\n",
            "epoch 33 batch 110 loss 0.06548059731721878\n",
            "epoch 33 batch 111 loss 0.058134958148002625\n",
            "epoch 33 batch 112 loss 0.06293994933366776\n",
            "epoch 33 batch 113 loss 0.06580149382352829\n",
            "epoch 33 batch 114 loss 0.058519698679447174\n",
            "epoch 33 batch 115 loss 0.0618373304605484\n",
            "epoch 33 batch 116 loss 0.06304015964269638\n",
            "epoch 33 batch 117 loss 0.06547541171312332\n",
            "epoch 33 batch 118 loss 0.06383471935987473\n",
            "epoch 33 batch 119 loss 0.06093258410692215\n",
            "epoch 33 batch 120 loss 0.06392655521631241\n",
            "epoch 33 batch 121 loss 0.06113818660378456\n",
            "epoch 33 batch 122 loss 0.06227874755859375\n",
            "epoch 33 batch 123 loss 0.06227567046880722\n",
            "epoch 33 batch 124 loss 0.06019705906510353\n",
            "epoch 33 batch 125 loss 0.06386358290910721\n",
            "epoch 33 batch 126 loss 0.05982699617743492\n",
            "epoch 33 batch 127 loss 0.058153070509433746\n",
            "epoch 33 batch 128 loss 0.0635254830121994\n",
            "epoch 33 batch 129 loss 0.058716557919979095\n",
            "epoch 33 batch 130 loss 0.06097074970602989\n",
            "epoch 33 batch 131 loss 0.06224193423986435\n",
            "epoch 33 batch 132 loss 0.06853006035089493\n",
            "epoch 33 batch 133 loss 0.06507080048322678\n",
            "epoch 33 batch 134 loss 0.06622979789972305\n",
            "epoch 33 batch 135 loss 0.06346901506185532\n",
            "epoch 33 batch 136 loss 0.05932796373963356\n",
            "epoch 33 batch 137 loss 0.06486506760120392\n",
            "epoch 33 batch 138 loss 0.06607656180858612\n",
            "epoch 33 batch 139 loss 0.06645768135786057\n",
            "epoch 33 batch 140 loss 0.06462933123111725\n",
            "epoch 33 batch 141 loss 0.06929321587085724\n",
            "epoch 33 batch 142 loss 0.07047326117753983\n",
            "epoch 33 batch 143 loss 0.06940188258886337\n",
            "epoch 33 batch 144 loss 0.06766999512910843\n",
            "epoch 33 batch 145 loss 0.06742198020219803\n",
            "epoch 33 batch 146 loss 0.06299710273742676\n",
            "epoch 33 batch 147 loss 0.06270523369312286\n",
            "epoch 33 batch 148 loss 0.06493057310581207\n",
            "epoch 33 batch 149 loss 0.063376285135746\n",
            "epoch 33 batch 150 loss 0.06310321390628815\n",
            "epoch 33 batch 151 loss 0.06076284497976303\n",
            "epoch 33 batch 152 loss 0.06628800183534622\n",
            "epoch 33 batch 153 loss 0.06475938856601715\n",
            "epoch 33 batch 154 loss 0.06576674431562424\n",
            "epoch 33 batch 155 loss 0.06356659531593323\n",
            "epoch 33 batch 156 loss 0.06255756318569183\n",
            "epoch 33 batch 157 loss 0.06815977394580841\n",
            "epoch 33 batch 158 loss 0.060647353529930115\n",
            "epoch 33 batch 159 loss 0.05671072006225586\n",
            "epoch 33 batch 160 loss 0.06153007969260216\n",
            "epoch 33 batch 161 loss 0.06642621755599976\n",
            "epoch 33 batch 162 loss 0.06616898626089096\n",
            "epoch 33 batch 163 loss 0.06907713413238525\n",
            "epoch 33 batch 164 loss 0.062282051891088486\n",
            "epoch 33 batch 165 loss 0.06405970454216003\n",
            "epoch 33 batch 166 loss 0.064288429915905\n",
            "epoch 33 batch 167 loss 0.0645388811826706\n",
            "epoch 33 batch 168 loss 0.06057055667042732\n",
            "epoch 33 batch 169 loss 0.06137216463685036\n",
            "epoch 33 batch 170 loss 0.0640881210565567\n",
            "epoch 33 batch 171 loss 0.06631377339363098\n",
            "epoch 33 batch 172 loss 0.06203712522983551\n",
            "epoch 33 batch 173 loss 0.0629778578877449\n",
            "epoch 33 batch 174 loss 0.06562002748250961\n",
            "epoch 33 batch 175 loss 0.06010144576430321\n",
            "epoch 33 batch 176 loss 0.060726676136255264\n",
            "epoch 33 batch 177 loss 0.06376779824495316\n",
            "epoch 33 batch 178 loss 0.06295078992843628\n",
            "epoch 33 batch 179 loss 0.07110875099897385\n",
            "epoch 33 batch 180 loss 0.06449035555124283\n",
            "epoch 33 batch 181 loss 0.06257212907075882\n",
            "epoch 33 batch 182 loss 0.06571074575185776\n",
            "epoch 33 batch 183 loss 0.0606558620929718\n",
            "epoch 33 batch 184 loss 0.05934036150574684\n",
            "epoch 33 batch 185 loss 0.05617237091064453\n",
            "epoch 33 batch 186 loss 0.06759173423051834\n",
            "epoch 33 batch 187 loss 0.06702558696269989\n",
            "epoch 33 batch 188 loss 0.061456725001335144\n",
            "epoch 33 batch 189 loss 0.06102433055639267\n",
            "epoch 33 batch 190 loss 0.0664554163813591\n",
            "epoch 33 batch 191 loss 0.06221196427941322\n",
            "epoch 33 batch 192 loss 0.07251492142677307\n",
            "epoch 33 batch 193 loss 0.058612409979104996\n",
            "epoch 33 batch 194 loss 0.06693723052740097\n",
            "epoch 33 batch 195 loss 0.06544165313243866\n",
            "epoch 33 batch 196 loss 0.06476672738790512\n",
            "epoch 33 batch 197 loss 0.06410866230726242\n",
            "epoch 33 batch 198 loss 0.06344335526227951\n",
            "epoch 33 batch 199 loss 0.06649962067604065\n",
            "epoch 33 batch 200 loss 0.06230805814266205\n",
            "epoch 33 batch 201 loss 0.061771366745233536\n",
            "epoch 33 batch 202 loss 0.06660305708646774\n",
            "epoch 33 batch 203 loss 0.05994492769241333\n",
            "epoch 33 batch 204 loss 0.05862640216946602\n",
            "epoch 33 batch 205 loss 0.06029939651489258\n",
            "epoch 33 batch 206 loss 0.06963801383972168\n",
            "epoch 33 batch 207 loss 0.06697940081357956\n",
            "epoch 33 batch 208 loss 0.05732882767915726\n",
            "epoch 33 batch 209 loss 0.06515813618898392\n",
            "epoch 33 batch 210 loss 0.0612364187836647\n",
            "epoch 33 batch 211 loss 0.06277131289243698\n",
            "epoch 33 batch 212 loss 0.061157748103141785\n",
            "epoch 33 batch 213 loss 0.06447896361351013\n",
            "epoch 33 batch 214 loss 0.06449428200721741\n",
            "epoch 33 batch 215 loss 0.07205904275178909\n",
            "epoch 33 batch 216 loss 0.06459785252809525\n",
            "epoch 33 batch 217 loss 0.06196634843945503\n",
            "epoch 33 batch 218 loss 0.06611354649066925\n",
            "epoch 33 batch 219 loss 0.06538945436477661\n",
            "epoch 33 batch 220 loss 0.06172505021095276\n",
            "epoch 33 batch 221 loss 0.06260232627391815\n",
            "epoch 33 batch 222 loss 0.06748659908771515\n",
            "epoch 33 batch 223 loss 0.07513975352048874\n",
            "epoch 33 batch 224 loss 0.06239686906337738\n",
            "epoch 33 batch 225 loss 0.06433973461389542\n",
            "epoch 33 batch 226 loss 0.05971504747867584\n",
            "epoch 33 batch 227 loss 0.0641620084643364\n",
            "epoch 33 batch 228 loss 0.06848395615816116\n",
            "epoch 33 batch 229 loss 0.06455808132886887\n",
            "epoch 33 batch 230 loss 0.07111956179141998\n",
            "epoch 34 batch 0 loss 0.0612536184489727\n",
            "epoch 34 batch 1 loss 0.06096620485186577\n",
            "epoch 34 batch 2 loss 0.06332939118146896\n",
            "epoch 34 batch 3 loss 0.06056669354438782\n",
            "epoch 34 batch 4 loss 0.0584036260843277\n",
            "epoch 34 batch 5 loss 0.06260781735181808\n",
            "epoch 34 batch 6 loss 0.061807505786418915\n",
            "epoch 34 batch 7 loss 0.06577810645103455\n",
            "epoch 34 batch 8 loss 0.06040968745946884\n",
            "epoch 34 batch 9 loss 0.05928879603743553\n",
            "epoch 34 batch 10 loss 0.06048043072223663\n",
            "epoch 34 batch 11 loss 0.0592079721391201\n",
            "epoch 34 batch 12 loss 0.062493946403265\n",
            "epoch 34 batch 13 loss 0.06156126782298088\n",
            "epoch 34 batch 14 loss 0.06566963344812393\n",
            "epoch 34 batch 15 loss 0.059538714587688446\n",
            "epoch 34 batch 16 loss 0.06780367344617844\n",
            "epoch 34 batch 17 loss 0.06749055534601212\n",
            "epoch 34 batch 18 loss 0.061520062386989594\n",
            "epoch 34 batch 19 loss 0.06393048912286758\n",
            "epoch 34 batch 20 loss 0.06167338043451309\n",
            "epoch 34 batch 21 loss 0.061159584671258926\n",
            "epoch 34 batch 22 loss 0.060235247015953064\n",
            "epoch 34 batch 23 loss 0.06504394114017487\n",
            "epoch 34 batch 24 loss 0.06313713639974594\n",
            "epoch 34 batch 25 loss 0.06822860985994339\n",
            "epoch 34 batch 26 loss 0.06394687294960022\n",
            "epoch 34 batch 27 loss 0.06169562041759491\n",
            "epoch 34 batch 28 loss 0.059868596494197845\n",
            "epoch 34 batch 29 loss 0.06442015618085861\n",
            "epoch 34 batch 30 loss 0.0612618625164032\n",
            "epoch 34 batch 31 loss 0.05420560762286186\n",
            "epoch 34 batch 32 loss 0.058996252715587616\n",
            "epoch 34 batch 33 loss 0.05732208862900734\n",
            "epoch 34 batch 34 loss 0.05708931013941765\n",
            "epoch 34 batch 35 loss 0.061578139662742615\n",
            "epoch 34 batch 36 loss 0.061055634170770645\n",
            "epoch 34 batch 37 loss 0.06274843961000443\n",
            "epoch 34 batch 38 loss 0.0649387389421463\n",
            "epoch 34 batch 39 loss 0.06000049039721489\n",
            "epoch 34 batch 40 loss 0.06411179155111313\n",
            "epoch 34 batch 41 loss 0.05662964656949043\n",
            "epoch 34 batch 42 loss 0.059318508952856064\n",
            "epoch 34 batch 43 loss 0.06085464358329773\n",
            "epoch 34 batch 44 loss 0.05573321506381035\n",
            "epoch 34 batch 45 loss 0.06522691249847412\n",
            "epoch 34 batch 46 loss 0.06374611705541611\n",
            "epoch 34 batch 47 loss 0.0643727108836174\n",
            "epoch 34 batch 48 loss 0.062508724629879\n",
            "epoch 34 batch 49 loss 0.06698798388242722\n",
            "epoch 34 batch 50 loss 0.060979727655649185\n",
            "epoch 34 batch 51 loss 0.06377124786376953\n",
            "epoch 34 batch 52 loss 0.06789036840200424\n",
            "epoch 34 batch 53 loss 0.06463846564292908\n",
            "epoch 34 batch 54 loss 0.062388889491558075\n",
            "epoch 34 batch 55 loss 0.06058724224567413\n",
            "epoch 34 batch 56 loss 0.0599665530025959\n",
            "epoch 34 batch 57 loss 0.0673426017165184\n",
            "epoch 34 batch 58 loss 0.06576727330684662\n",
            "epoch 34 batch 59 loss 0.057389527559280396\n",
            "epoch 34 batch 60 loss 0.06415354460477829\n",
            "epoch 34 batch 61 loss 0.056248098611831665\n",
            "epoch 34 batch 62 loss 0.060597456991672516\n",
            "epoch 34 batch 63 loss 0.06074363738298416\n",
            "epoch 34 batch 64 loss 0.06015044450759888\n",
            "epoch 34 batch 65 loss 0.06568092107772827\n",
            "epoch 34 batch 66 loss 0.06676671653985977\n",
            "epoch 34 batch 67 loss 0.06195931136608124\n",
            "epoch 34 batch 68 loss 0.05975707247853279\n",
            "epoch 34 batch 69 loss 0.0643199011683464\n",
            "epoch 34 batch 70 loss 0.06832311302423477\n",
            "epoch 34 batch 71 loss 0.05590200424194336\n",
            "epoch 34 batch 72 loss 0.06894499063491821\n",
            "epoch 34 batch 73 loss 0.06343202292919159\n",
            "epoch 34 batch 74 loss 0.061698317527770996\n",
            "epoch 34 batch 75 loss 0.0591161735355854\n",
            "epoch 34 batch 76 loss 0.06116336211562157\n",
            "epoch 34 batch 77 loss 0.06340059638023376\n",
            "epoch 34 batch 78 loss 0.06700583547353745\n",
            "epoch 34 batch 79 loss 0.06495087593793869\n",
            "epoch 34 batch 80 loss 0.06403668969869614\n",
            "epoch 34 batch 81 loss 0.06089215725660324\n",
            "epoch 34 batch 82 loss 0.06822416931390762\n",
            "epoch 34 batch 83 loss 0.057779669761657715\n",
            "epoch 34 batch 84 loss 0.059554386883974075\n",
            "epoch 34 batch 85 loss 0.05798232555389404\n",
            "epoch 34 batch 86 loss 0.06233058124780655\n",
            "epoch 34 batch 87 loss 0.061809856444597244\n",
            "epoch 34 batch 88 loss 0.062366217374801636\n",
            "epoch 34 batch 89 loss 0.06327209621667862\n",
            "epoch 34 batch 90 loss 0.05702086165547371\n",
            "epoch 34 batch 91 loss 0.06614649295806885\n",
            "epoch 34 batch 92 loss 0.058691322803497314\n",
            "epoch 34 batch 93 loss 0.06885599344968796\n",
            "epoch 34 batch 94 loss 0.06410393118858337\n",
            "epoch 34 batch 95 loss 0.06423915177583694\n",
            "epoch 34 batch 96 loss 0.05896361172199249\n",
            "epoch 34 batch 97 loss 0.06019005551934242\n",
            "epoch 34 batch 98 loss 0.06639489531517029\n",
            "epoch 34 batch 99 loss 0.06777649372816086\n",
            "epoch 34 batch 100 loss 0.0583227276802063\n",
            "epoch 34 batch 101 loss 0.06783979386091232\n",
            "epoch 34 batch 102 loss 0.060450825840234756\n",
            "epoch 34 batch 103 loss 0.060889147222042084\n",
            "epoch 34 batch 104 loss 0.06297475844621658\n",
            "epoch 34 batch 105 loss 0.06398066878318787\n",
            "epoch 34 batch 106 loss 0.060794271528720856\n",
            "epoch 34 batch 107 loss 0.06367617100477219\n",
            "epoch 34 batch 108 loss 0.05556924268603325\n",
            "epoch 34 batch 109 loss 0.0631241500377655\n",
            "epoch 34 batch 110 loss 0.06057511642575264\n",
            "epoch 34 batch 111 loss 0.05915878340601921\n",
            "epoch 34 batch 112 loss 0.06144125759601593\n",
            "epoch 34 batch 113 loss 0.06524115055799484\n",
            "epoch 34 batch 114 loss 0.060101237148046494\n",
            "epoch 34 batch 115 loss 0.0734482854604721\n",
            "epoch 34 batch 116 loss 0.06177433580160141\n",
            "epoch 34 batch 117 loss 0.060777630656957626\n",
            "epoch 34 batch 118 loss 0.0651041641831398\n",
            "epoch 34 batch 119 loss 0.06212015077471733\n",
            "epoch 34 batch 120 loss 0.061294130980968475\n",
            "epoch 34 batch 121 loss 0.0722774863243103\n",
            "epoch 34 batch 122 loss 0.062341418117284775\n",
            "epoch 34 batch 123 loss 0.06431687623262405\n",
            "epoch 34 batch 124 loss 0.06041533872485161\n",
            "epoch 34 batch 125 loss 0.061593443155288696\n",
            "epoch 34 batch 126 loss 0.06545040011405945\n",
            "epoch 34 batch 127 loss 0.06769723445177078\n",
            "epoch 34 batch 128 loss 0.06376249343156815\n",
            "epoch 34 batch 129 loss 0.06281742453575134\n",
            "epoch 34 batch 130 loss 0.06548163294792175\n",
            "epoch 34 batch 131 loss 0.06657621264457703\n",
            "epoch 34 batch 132 loss 0.05995425581932068\n",
            "epoch 34 batch 133 loss 0.05960814282298088\n",
            "epoch 34 batch 134 loss 0.05966045334935188\n",
            "epoch 34 batch 135 loss 0.05638977512717247\n",
            "epoch 34 batch 136 loss 0.06709308922290802\n",
            "epoch 34 batch 137 loss 0.06078418716788292\n",
            "epoch 34 batch 138 loss 0.06350547820329666\n",
            "epoch 34 batch 139 loss 0.06519588828086853\n",
            "epoch 34 batch 140 loss 0.06448864936828613\n",
            "epoch 34 batch 141 loss 0.05993408337235451\n",
            "epoch 34 batch 142 loss 0.060609545558691025\n",
            "epoch 34 batch 143 loss 0.06754031777381897\n",
            "epoch 34 batch 144 loss 0.06584912538528442\n",
            "epoch 34 batch 145 loss 0.06519437581300735\n",
            "epoch 34 batch 146 loss 0.06503045558929443\n",
            "epoch 34 batch 147 loss 0.06757435947656631\n",
            "epoch 34 batch 148 loss 0.06349847465753555\n",
            "epoch 34 batch 149 loss 0.0664975494146347\n",
            "epoch 34 batch 150 loss 0.062450580298900604\n",
            "epoch 34 batch 151 loss 0.0658455491065979\n",
            "epoch 34 batch 152 loss 0.05749005451798439\n",
            "epoch 34 batch 153 loss 0.07008226215839386\n",
            "epoch 34 batch 154 loss 0.07051844894886017\n",
            "epoch 34 batch 155 loss 0.06566663831472397\n",
            "epoch 34 batch 156 loss 0.06420134752988815\n",
            "epoch 34 batch 157 loss 0.06545434147119522\n",
            "epoch 34 batch 158 loss 0.06728214770555496\n",
            "epoch 34 batch 159 loss 0.06146586686372757\n",
            "epoch 34 batch 160 loss 0.059918567538261414\n",
            "epoch 34 batch 161 loss 0.06223553791642189\n",
            "epoch 34 batch 162 loss 0.0614917129278183\n",
            "epoch 34 batch 163 loss 0.05886245146393776\n",
            "epoch 34 batch 164 loss 0.06112249195575714\n",
            "epoch 34 batch 165 loss 0.07137957960367203\n",
            "epoch 34 batch 166 loss 0.06846819072961807\n",
            "epoch 34 batch 167 loss 0.06072883680462837\n",
            "epoch 34 batch 168 loss 0.06187233701348305\n",
            "epoch 34 batch 169 loss 0.06197110563516617\n",
            "epoch 34 batch 170 loss 0.060061968863010406\n",
            "epoch 34 batch 171 loss 0.07082775235176086\n",
            "epoch 34 batch 172 loss 0.0609598271548748\n",
            "epoch 34 batch 173 loss 0.06109556183218956\n",
            "epoch 34 batch 174 loss 0.06393278390169144\n",
            "epoch 34 batch 175 loss 0.06834032386541367\n",
            "epoch 34 batch 176 loss 0.06558124721050262\n",
            "epoch 34 batch 177 loss 0.06352114677429199\n",
            "epoch 34 batch 178 loss 0.061431508511304855\n",
            "epoch 34 batch 179 loss 0.07234521955251694\n",
            "epoch 34 batch 180 loss 0.06877606362104416\n",
            "epoch 34 batch 181 loss 0.05996285751461983\n",
            "epoch 34 batch 182 loss 0.06469758599996567\n",
            "epoch 34 batch 183 loss 0.0655154287815094\n",
            "epoch 34 batch 184 loss 0.0671946182847023\n",
            "epoch 34 batch 185 loss 0.06340005993843079\n",
            "epoch 34 batch 186 loss 0.06699862331151962\n",
            "epoch 34 batch 187 loss 0.06423059850931168\n",
            "epoch 34 batch 188 loss 0.06172062084078789\n",
            "epoch 34 batch 189 loss 0.06469516456127167\n",
            "epoch 34 batch 190 loss 0.06108896806836128\n",
            "epoch 34 batch 191 loss 0.060773517936468124\n",
            "epoch 34 batch 192 loss 0.06279390305280685\n",
            "epoch 34 batch 193 loss 0.06791294366121292\n",
            "epoch 34 batch 194 loss 0.058873120695352554\n",
            "epoch 34 batch 195 loss 0.06323958933353424\n",
            "epoch 34 batch 196 loss 0.06200579181313515\n",
            "epoch 34 batch 197 loss 0.0654565691947937\n",
            "epoch 34 batch 198 loss 0.06452462077140808\n",
            "epoch 34 batch 199 loss 0.06417853385210037\n",
            "epoch 34 batch 200 loss 0.0626327395439148\n",
            "epoch 34 batch 201 loss 0.06762576103210449\n",
            "epoch 34 batch 202 loss 0.06484395265579224\n",
            "epoch 34 batch 203 loss 0.0657949298620224\n",
            "epoch 34 batch 204 loss 0.06292156130075455\n",
            "epoch 34 batch 205 loss 0.06349994987249374\n",
            "epoch 34 batch 206 loss 0.061412300914525986\n",
            "epoch 34 batch 207 loss 0.06661272794008255\n",
            "epoch 34 batch 208 loss 0.0716475173830986\n",
            "epoch 34 batch 209 loss 0.06377136707305908\n",
            "epoch 34 batch 210 loss 0.0698719248175621\n",
            "epoch 34 batch 211 loss 0.06508263200521469\n",
            "epoch 34 batch 212 loss 0.06421425193548203\n",
            "epoch 34 batch 213 loss 0.0641506165266037\n",
            "epoch 34 batch 214 loss 0.06340973824262619\n",
            "epoch 34 batch 215 loss 0.06237773969769478\n",
            "epoch 34 batch 216 loss 0.06964559853076935\n",
            "epoch 34 batch 217 loss 0.06061691418290138\n",
            "epoch 34 batch 218 loss 0.06214740127325058\n",
            "epoch 34 batch 219 loss 0.06196972355246544\n",
            "epoch 34 batch 220 loss 0.061078477650880814\n",
            "epoch 34 batch 221 loss 0.06795312464237213\n",
            "epoch 34 batch 222 loss 0.05610085278749466\n",
            "epoch 34 batch 223 loss 0.06223660334944725\n",
            "epoch 34 batch 224 loss 0.06597428768873215\n",
            "epoch 34 batch 225 loss 0.0616794191300869\n",
            "epoch 34 batch 226 loss 0.06359518319368362\n",
            "epoch 34 batch 227 loss 0.06680278480052948\n",
            "epoch 34 batch 228 loss 0.0681062787771225\n",
            "epoch 34 batch 229 loss 0.06945335865020752\n",
            "epoch 34 batch 230 loss 0.0696621909737587\n",
            "epoch 35 batch 0 loss 0.06402864307165146\n",
            "epoch 35 batch 1 loss 0.06049931049346924\n",
            "epoch 35 batch 2 loss 0.062706358730793\n",
            "epoch 35 batch 3 loss 0.05975046753883362\n",
            "epoch 35 batch 4 loss 0.05977853015065193\n",
            "epoch 35 batch 5 loss 0.062273234128952026\n",
            "epoch 35 batch 6 loss 0.06247057765722275\n",
            "epoch 35 batch 7 loss 0.0630374327301979\n",
            "epoch 35 batch 8 loss 0.059700481593608856\n",
            "epoch 35 batch 9 loss 0.05982968583703041\n",
            "epoch 35 batch 10 loss 0.06256451457738876\n",
            "epoch 35 batch 11 loss 0.06808652728796005\n",
            "epoch 35 batch 12 loss 0.061247069388628006\n",
            "epoch 35 batch 13 loss 0.05875124782323837\n",
            "epoch 35 batch 14 loss 0.06609968096017838\n",
            "epoch 35 batch 15 loss 0.05680656433105469\n",
            "epoch 35 batch 16 loss 0.06515852361917496\n",
            "epoch 35 batch 17 loss 0.05862530320882797\n",
            "epoch 35 batch 18 loss 0.05877169221639633\n",
            "epoch 35 batch 19 loss 0.06312864273786545\n",
            "epoch 35 batch 20 loss 0.0562710203230381\n",
            "epoch 35 batch 21 loss 0.06059279292821884\n",
            "epoch 35 batch 22 loss 0.06324950605630875\n",
            "epoch 35 batch 23 loss 0.06243458390235901\n",
            "epoch 35 batch 24 loss 0.06096825748682022\n",
            "epoch 35 batch 25 loss 0.06254958361387253\n",
            "epoch 35 batch 26 loss 0.06172306090593338\n",
            "epoch 35 batch 27 loss 0.06597717106342316\n",
            "epoch 35 batch 28 loss 0.062240950763225555\n",
            "epoch 35 batch 29 loss 0.06307058781385422\n",
            "epoch 35 batch 30 loss 0.06508740037679672\n",
            "epoch 35 batch 31 loss 0.05534568056464195\n",
            "epoch 35 batch 32 loss 0.06443677097558975\n",
            "epoch 35 batch 33 loss 0.05815798416733742\n",
            "epoch 35 batch 34 loss 0.06293565779924393\n",
            "epoch 35 batch 35 loss 0.06351414322853088\n",
            "epoch 35 batch 36 loss 0.06463242322206497\n",
            "epoch 35 batch 37 loss 0.06053793057799339\n",
            "epoch 35 batch 38 loss 0.05846444517374039\n",
            "epoch 35 batch 39 loss 0.054284967482089996\n",
            "epoch 35 batch 40 loss 0.061337389051914215\n",
            "epoch 35 batch 41 loss 0.05989508330821991\n",
            "epoch 35 batch 42 loss 0.06606246531009674\n",
            "epoch 35 batch 43 loss 0.06046906113624573\n",
            "epoch 35 batch 44 loss 0.06577032804489136\n",
            "epoch 35 batch 45 loss 0.06813986599445343\n",
            "epoch 35 batch 46 loss 0.05836180970072746\n",
            "epoch 35 batch 47 loss 0.06258232891559601\n",
            "epoch 35 batch 48 loss 0.05388624221086502\n",
            "epoch 35 batch 49 loss 0.0592510811984539\n",
            "epoch 35 batch 50 loss 0.05593523755669594\n",
            "epoch 35 batch 51 loss 0.05901813507080078\n",
            "epoch 35 batch 52 loss 0.061334528028964996\n",
            "epoch 35 batch 53 loss 0.05942579358816147\n",
            "epoch 35 batch 54 loss 0.062328141182661057\n",
            "epoch 35 batch 55 loss 0.06273119896650314\n",
            "epoch 35 batch 56 loss 0.06549035757780075\n",
            "epoch 35 batch 57 loss 0.0595991313457489\n",
            "epoch 35 batch 58 loss 0.06066424772143364\n",
            "epoch 35 batch 59 loss 0.06153872236609459\n",
            "epoch 35 batch 60 loss 0.05763169005513191\n",
            "epoch 35 batch 61 loss 0.061196379363536835\n",
            "epoch 35 batch 62 loss 0.06282615661621094\n",
            "epoch 35 batch 63 loss 0.0637168139219284\n",
            "epoch 35 batch 64 loss 0.06065080687403679\n",
            "epoch 35 batch 65 loss 0.05839810147881508\n",
            "epoch 35 batch 66 loss 0.06689897924661636\n",
            "epoch 35 batch 67 loss 0.0669267550110817\n",
            "epoch 35 batch 68 loss 0.06964182108640671\n",
            "epoch 35 batch 69 loss 0.06285364925861359\n",
            "epoch 35 batch 70 loss 0.06132595241069794\n",
            "epoch 35 batch 71 loss 0.06031550094485283\n",
            "epoch 35 batch 72 loss 0.061470258980989456\n",
            "epoch 35 batch 73 loss 0.06610681116580963\n",
            "epoch 35 batch 74 loss 0.062119998037815094\n",
            "epoch 35 batch 75 loss 0.059521496295928955\n",
            "epoch 35 batch 76 loss 0.05813605710864067\n",
            "epoch 35 batch 77 loss 0.05923309177160263\n",
            "epoch 35 batch 78 loss 0.06295987218618393\n",
            "epoch 35 batch 79 loss 0.0655190646648407\n",
            "epoch 35 batch 80 loss 0.06273384392261505\n",
            "epoch 35 batch 81 loss 0.05601432919502258\n",
            "epoch 35 batch 82 loss 0.06421062350273132\n",
            "epoch 35 batch 83 loss 0.06492707878351212\n",
            "epoch 35 batch 84 loss 0.061210811138153076\n",
            "epoch 35 batch 85 loss 0.06224663928151131\n",
            "epoch 35 batch 86 loss 0.062579445540905\n",
            "epoch 35 batch 87 loss 0.06363780051469803\n",
            "epoch 35 batch 88 loss 0.06108017638325691\n",
            "epoch 35 batch 89 loss 0.06608442962169647\n",
            "epoch 35 batch 90 loss 0.06127570569515228\n",
            "epoch 35 batch 91 loss 0.06519439816474915\n",
            "epoch 35 batch 92 loss 0.06321410834789276\n",
            "epoch 35 batch 93 loss 0.060270678251981735\n",
            "epoch 35 batch 94 loss 0.06355772167444229\n",
            "epoch 35 batch 95 loss 0.0643649771809578\n",
            "epoch 35 batch 96 loss 0.0622091218829155\n",
            "epoch 35 batch 97 loss 0.06787524372339249\n",
            "epoch 35 batch 98 loss 0.06621947139501572\n",
            "epoch 35 batch 99 loss 0.06388625502586365\n",
            "epoch 35 batch 100 loss 0.05884489789605141\n",
            "epoch 35 batch 101 loss 0.05939130857586861\n",
            "epoch 35 batch 102 loss 0.06191718950867653\n",
            "epoch 35 batch 103 loss 0.06696534156799316\n",
            "epoch 35 batch 104 loss 0.06675467640161514\n",
            "epoch 35 batch 105 loss 0.061121005564928055\n",
            "epoch 35 batch 106 loss 0.0683746412396431\n",
            "epoch 35 batch 107 loss 0.06298816949129105\n",
            "epoch 35 batch 108 loss 0.06533646583557129\n",
            "epoch 35 batch 109 loss 0.06374532729387283\n",
            "epoch 35 batch 110 loss 0.06332013756036758\n",
            "epoch 35 batch 111 loss 0.058424677699804306\n",
            "epoch 35 batch 112 loss 0.06302895396947861\n",
            "epoch 35 batch 113 loss 0.062462881207466125\n",
            "epoch 35 batch 114 loss 0.06334894150495529\n",
            "epoch 35 batch 115 loss 0.06503941118717194\n",
            "epoch 35 batch 116 loss 0.06274810433387756\n",
            "epoch 35 batch 117 loss 0.06830117851495743\n",
            "epoch 35 batch 118 loss 0.06354904174804688\n",
            "epoch 35 batch 119 loss 0.05999789386987686\n",
            "epoch 35 batch 120 loss 0.06624050438404083\n",
            "epoch 35 batch 121 loss 0.06343813240528107\n",
            "epoch 35 batch 122 loss 0.06450831890106201\n",
            "epoch 35 batch 123 loss 0.06459718197584152\n",
            "epoch 35 batch 124 loss 0.06571369618177414\n",
            "epoch 35 batch 125 loss 0.06040728837251663\n",
            "epoch 35 batch 126 loss 0.06252918392419815\n",
            "epoch 35 batch 127 loss 0.06021885573863983\n",
            "epoch 35 batch 128 loss 0.06254255771636963\n",
            "epoch 35 batch 129 loss 0.06265959143638611\n",
            "epoch 35 batch 130 loss 0.06321726739406586\n",
            "epoch 35 batch 131 loss 0.06317496299743652\n",
            "epoch 35 batch 132 loss 0.060745466500520706\n",
            "epoch 35 batch 133 loss 0.06259144097566605\n",
            "epoch 35 batch 134 loss 0.05558691546320915\n",
            "epoch 35 batch 135 loss 0.06460890173912048\n",
            "epoch 35 batch 136 loss 0.07019669562578201\n",
            "epoch 35 batch 137 loss 0.06973225623369217\n",
            "epoch 35 batch 138 loss 0.06820585578680038\n",
            "epoch 35 batch 139 loss 0.06390222162008286\n",
            "epoch 35 batch 140 loss 0.05948301777243614\n",
            "epoch 35 batch 141 loss 0.06067092344164848\n",
            "epoch 35 batch 142 loss 0.06273317337036133\n",
            "epoch 35 batch 143 loss 0.06176545470952988\n",
            "epoch 35 batch 144 loss 0.06242993846535683\n",
            "epoch 35 batch 145 loss 0.06524400413036346\n",
            "epoch 35 batch 146 loss 0.05845038592815399\n",
            "epoch 35 batch 147 loss 0.06066783145070076\n",
            "epoch 35 batch 148 loss 0.06335698813199997\n",
            "epoch 35 batch 149 loss 0.061049167066812515\n",
            "epoch 35 batch 150 loss 0.07047194987535477\n",
            "epoch 35 batch 151 loss 0.06423991173505783\n",
            "epoch 35 batch 152 loss 0.0705362856388092\n",
            "epoch 35 batch 153 loss 0.06939872354269028\n",
            "epoch 35 batch 154 loss 0.06389641016721725\n",
            "epoch 35 batch 155 loss 0.06545787304639816\n",
            "epoch 35 batch 156 loss 0.06059516221284866\n",
            "epoch 35 batch 157 loss 0.05805148929357529\n",
            "epoch 35 batch 158 loss 0.06351736187934875\n",
            "epoch 35 batch 159 loss 0.06216566264629364\n",
            "epoch 35 batch 160 loss 0.06517627835273743\n",
            "epoch 35 batch 161 loss 0.06482291966676712\n",
            "epoch 35 batch 162 loss 0.061012279242277145\n",
            "epoch 35 batch 163 loss 0.06032850965857506\n",
            "epoch 35 batch 164 loss 0.06343567371368408\n",
            "epoch 35 batch 165 loss 0.06701068580150604\n",
            "epoch 35 batch 166 loss 0.05987036973237991\n",
            "epoch 35 batch 167 loss 0.06584522873163223\n",
            "epoch 35 batch 168 loss 0.06262897700071335\n",
            "epoch 35 batch 169 loss 0.06077771633863449\n",
            "epoch 35 batch 170 loss 0.06263072043657303\n",
            "epoch 35 batch 171 loss 0.06763620674610138\n",
            "epoch 35 batch 172 loss 0.06712603569030762\n",
            "epoch 35 batch 173 loss 0.06305666267871857\n",
            "epoch 35 batch 174 loss 0.060433145612478256\n",
            "epoch 35 batch 175 loss 0.0613136924803257\n",
            "epoch 35 batch 176 loss 0.06442546099424362\n",
            "epoch 35 batch 177 loss 0.0719187781214714\n",
            "epoch 35 batch 178 loss 0.06636517494916916\n",
            "epoch 35 batch 179 loss 0.06392442435026169\n",
            "epoch 35 batch 180 loss 0.0624748170375824\n",
            "epoch 35 batch 181 loss 0.06436281651258469\n",
            "epoch 35 batch 182 loss 0.06328633427619934\n",
            "epoch 35 batch 183 loss 0.06233183667063713\n",
            "epoch 35 batch 184 loss 0.06304769217967987\n",
            "epoch 35 batch 185 loss 0.06275415420532227\n",
            "epoch 35 batch 186 loss 0.05919932201504707\n",
            "epoch 35 batch 187 loss 0.06542493402957916\n",
            "epoch 35 batch 188 loss 0.06279901415109634\n",
            "epoch 35 batch 189 loss 0.05701887607574463\n",
            "epoch 35 batch 190 loss 0.06329000741243362\n",
            "epoch 35 batch 191 loss 0.06088726222515106\n",
            "epoch 35 batch 192 loss 0.06623119860887527\n",
            "epoch 35 batch 193 loss 0.061618734151124954\n",
            "epoch 35 batch 194 loss 0.0626940205693245\n",
            "epoch 35 batch 195 loss 0.06491658836603165\n",
            "epoch 35 batch 196 loss 0.06354819238185883\n",
            "epoch 35 batch 197 loss 0.05895261839032173\n",
            "epoch 35 batch 198 loss 0.06154987961053848\n",
            "epoch 35 batch 199 loss 0.06481746584177017\n",
            "epoch 35 batch 200 loss 0.06472925841808319\n",
            "epoch 35 batch 201 loss 0.06369127333164215\n",
            "epoch 35 batch 202 loss 0.06897369772195816\n",
            "epoch 35 batch 203 loss 0.06480278819799423\n",
            "epoch 35 batch 204 loss 0.0629613921046257\n",
            "epoch 35 batch 205 loss 0.05918858200311661\n",
            "epoch 35 batch 206 loss 0.062358882278203964\n",
            "epoch 35 batch 207 loss 0.06310530751943588\n",
            "epoch 35 batch 208 loss 0.06361208111047745\n",
            "epoch 35 batch 209 loss 0.06729098409414291\n",
            "epoch 35 batch 210 loss 0.06748978793621063\n",
            "epoch 35 batch 211 loss 0.06303974241018295\n",
            "epoch 35 batch 212 loss 0.059565793722867966\n",
            "epoch 35 batch 213 loss 0.06529532372951508\n",
            "epoch 35 batch 214 loss 0.06138912960886955\n",
            "epoch 35 batch 215 loss 0.061941713094711304\n",
            "epoch 35 batch 216 loss 0.07011715322732925\n",
            "epoch 35 batch 217 loss 0.06828988343477249\n",
            "epoch 35 batch 218 loss 0.0657901018857956\n",
            "epoch 35 batch 219 loss 0.062411967664957047\n",
            "epoch 35 batch 220 loss 0.06941140443086624\n",
            "epoch 35 batch 221 loss 0.06470075249671936\n",
            "epoch 35 batch 222 loss 0.0662643164396286\n",
            "epoch 35 batch 223 loss 0.07123031467199326\n",
            "epoch 35 batch 224 loss 0.05997389554977417\n",
            "epoch 35 batch 225 loss 0.06720723956823349\n",
            "epoch 35 batch 226 loss 0.07075617462396622\n",
            "epoch 35 batch 227 loss 0.06651420891284943\n",
            "epoch 35 batch 228 loss 0.060745976865291595\n",
            "epoch 35 batch 229 loss 0.06184111163020134\n",
            "epoch 35 batch 230 loss 0.06243845447897911\n",
            "epoch 36 batch 0 loss 0.06378249824047089\n",
            "epoch 36 batch 1 loss 0.056739967316389084\n",
            "epoch 36 batch 2 loss 0.05887148529291153\n",
            "epoch 36 batch 3 loss 0.05718046799302101\n",
            "epoch 36 batch 4 loss 0.06305935233831406\n",
            "epoch 36 batch 5 loss 0.06332027912139893\n",
            "epoch 36 batch 6 loss 0.058490097522735596\n",
            "epoch 36 batch 7 loss 0.062137775123119354\n",
            "epoch 36 batch 8 loss 0.06126716732978821\n",
            "epoch 36 batch 9 loss 0.06147105619311333\n",
            "epoch 36 batch 10 loss 0.061798714101314545\n",
            "epoch 36 batch 11 loss 0.05939061939716339\n",
            "epoch 36 batch 12 loss 0.05678645521402359\n",
            "epoch 36 batch 13 loss 0.06967581808567047\n",
            "epoch 36 batch 14 loss 0.0673123151063919\n",
            "epoch 36 batch 15 loss 0.06159016117453575\n",
            "epoch 36 batch 16 loss 0.06155679002404213\n",
            "epoch 36 batch 17 loss 0.05966191738843918\n",
            "epoch 36 batch 18 loss 0.06356003135442734\n",
            "epoch 36 batch 19 loss 0.05999387055635452\n",
            "epoch 36 batch 20 loss 0.06340604275465012\n",
            "epoch 36 batch 21 loss 0.06037849932909012\n",
            "epoch 36 batch 22 loss 0.06100497394800186\n",
            "epoch 36 batch 23 loss 0.06055093929171562\n",
            "epoch 36 batch 24 loss 0.05783528462052345\n",
            "epoch 36 batch 25 loss 0.06125092878937721\n",
            "epoch 36 batch 26 loss 0.060091596096754074\n",
            "epoch 36 batch 27 loss 0.06254184246063232\n",
            "epoch 36 batch 28 loss 0.058868758380413055\n",
            "epoch 36 batch 29 loss 0.06051722913980484\n",
            "epoch 36 batch 30 loss 0.061703160405159\n",
            "epoch 36 batch 31 loss 0.06126881390810013\n",
            "epoch 36 batch 32 loss 0.05957592651247978\n",
            "epoch 36 batch 33 loss 0.05796470493078232\n",
            "epoch 36 batch 34 loss 0.06406145542860031\n",
            "epoch 36 batch 35 loss 0.05929424986243248\n",
            "epoch 36 batch 36 loss 0.064970001578331\n",
            "epoch 36 batch 37 loss 0.056508567184209824\n",
            "epoch 36 batch 38 loss 0.06511711329221725\n",
            "epoch 36 batch 39 loss 0.06004093959927559\n",
            "epoch 36 batch 40 loss 0.06270793080329895\n",
            "epoch 36 batch 41 loss 0.06835583597421646\n",
            "epoch 36 batch 42 loss 0.06193391978740692\n",
            "epoch 36 batch 43 loss 0.06544723361730576\n",
            "epoch 36 batch 44 loss 0.06888329237699509\n",
            "epoch 36 batch 45 loss 0.058205027133226395\n",
            "epoch 36 batch 46 loss 0.05938495695590973\n",
            "epoch 36 batch 47 loss 0.062440693378448486\n",
            "epoch 36 batch 48 loss 0.06708357483148575\n",
            "epoch 36 batch 49 loss 0.06314326822757721\n",
            "epoch 36 batch 50 loss 0.060280248522758484\n",
            "epoch 36 batch 51 loss 0.06370554119348526\n",
            "epoch 36 batch 52 loss 0.05886475741863251\n",
            "epoch 36 batch 53 loss 0.0682259351015091\n",
            "epoch 36 batch 54 loss 0.06676732748746872\n",
            "epoch 36 batch 55 loss 0.0648176297545433\n",
            "epoch 36 batch 56 loss 0.057277366518974304\n",
            "epoch 36 batch 57 loss 0.06294824928045273\n",
            "epoch 36 batch 58 loss 0.05163853242993355\n",
            "epoch 36 batch 59 loss 0.057723451405763626\n",
            "epoch 36 batch 60 loss 0.06656500697135925\n",
            "epoch 36 batch 61 loss 0.06791290640830994\n",
            "epoch 36 batch 62 loss 0.06071570888161659\n",
            "epoch 36 batch 63 loss 0.05501924455165863\n",
            "epoch 36 batch 64 loss 0.06433068960905075\n",
            "epoch 36 batch 65 loss 0.05759705230593681\n",
            "epoch 36 batch 66 loss 0.06282712519168854\n",
            "epoch 36 batch 67 loss 0.0593518503010273\n",
            "epoch 36 batch 68 loss 0.05586152523756027\n",
            "epoch 36 batch 69 loss 0.06033419445157051\n",
            "epoch 36 batch 70 loss 0.06399209052324295\n",
            "epoch 36 batch 71 loss 0.06035420671105385\n",
            "epoch 36 batch 72 loss 0.06337129324674606\n",
            "epoch 36 batch 73 loss 0.06419730931520462\n",
            "epoch 36 batch 74 loss 0.05769297853112221\n",
            "epoch 36 batch 75 loss 0.05920599028468132\n",
            "epoch 36 batch 76 loss 0.06302651762962341\n",
            "epoch 36 batch 77 loss 0.06204070895910263\n",
            "epoch 36 batch 78 loss 0.06276305764913559\n",
            "epoch 36 batch 79 loss 0.06664322316646576\n",
            "epoch 36 batch 80 loss 0.060105398297309875\n",
            "epoch 36 batch 81 loss 0.06803887337446213\n",
            "epoch 36 batch 82 loss 0.06727294623851776\n",
            "epoch 36 batch 83 loss 0.06068284809589386\n",
            "epoch 36 batch 84 loss 0.0583045594394207\n",
            "epoch 36 batch 85 loss 0.0576002262532711\n",
            "epoch 36 batch 86 loss 0.0590846873819828\n",
            "epoch 36 batch 87 loss 0.06578103452920914\n",
            "epoch 36 batch 88 loss 0.06341500580310822\n",
            "epoch 36 batch 89 loss 0.06636644899845123\n",
            "epoch 36 batch 90 loss 0.06558439135551453\n",
            "epoch 36 batch 91 loss 0.060333799570798874\n",
            "epoch 36 batch 92 loss 0.0639723688364029\n",
            "epoch 36 batch 93 loss 0.059807371348142624\n",
            "epoch 36 batch 94 loss 0.0615251362323761\n",
            "epoch 36 batch 95 loss 0.060507990419864655\n",
            "epoch 36 batch 96 loss 0.06349831819534302\n",
            "epoch 36 batch 97 loss 0.06880354136228561\n",
            "epoch 36 batch 98 loss 0.06128356233239174\n",
            "epoch 36 batch 99 loss 0.06295599043369293\n",
            "epoch 36 batch 100 loss 0.06235939636826515\n",
            "epoch 36 batch 101 loss 0.06390466541051865\n",
            "epoch 36 batch 102 loss 0.06962110847234726\n",
            "epoch 36 batch 103 loss 0.06707257032394409\n",
            "epoch 36 batch 104 loss 0.06309596449136734\n",
            "epoch 36 batch 105 loss 0.0636538565158844\n",
            "epoch 36 batch 106 loss 0.06705892086029053\n",
            "epoch 36 batch 107 loss 0.06522805243730545\n",
            "epoch 36 batch 108 loss 0.05693882703781128\n",
            "epoch 36 batch 109 loss 0.0629291981458664\n",
            "epoch 36 batch 110 loss 0.06219067797064781\n",
            "epoch 36 batch 111 loss 0.06311557441949844\n",
            "epoch 36 batch 112 loss 0.05883323773741722\n",
            "epoch 36 batch 113 loss 0.06403376907110214\n",
            "epoch 36 batch 114 loss 0.0644221380352974\n",
            "epoch 36 batch 115 loss 0.06287072598934174\n",
            "epoch 36 batch 116 loss 0.054554734379053116\n",
            "epoch 36 batch 117 loss 0.06193329393863678\n",
            "epoch 36 batch 118 loss 0.053209174424409866\n",
            "epoch 36 batch 119 loss 0.06377208977937698\n",
            "epoch 36 batch 120 loss 0.06099314987659454\n",
            "epoch 36 batch 121 loss 0.06828973442316055\n",
            "epoch 36 batch 122 loss 0.06198722869157791\n",
            "epoch 36 batch 123 loss 0.06814561784267426\n",
            "epoch 36 batch 124 loss 0.06368942558765411\n",
            "epoch 36 batch 125 loss 0.06587887555360794\n",
            "epoch 36 batch 126 loss 0.06102128326892853\n",
            "epoch 36 batch 127 loss 0.05533814802765846\n",
            "epoch 36 batch 128 loss 0.07133743166923523\n",
            "epoch 36 batch 129 loss 0.06577318906784058\n",
            "epoch 36 batch 130 loss 0.05700642243027687\n",
            "epoch 36 batch 131 loss 0.06044430285692215\n",
            "epoch 36 batch 132 loss 0.06882572919130325\n",
            "epoch 36 batch 133 loss 0.06103774905204773\n",
            "epoch 36 batch 134 loss 0.06175881251692772\n",
            "epoch 36 batch 135 loss 0.06334228068590164\n",
            "epoch 36 batch 136 loss 0.06609509140253067\n",
            "epoch 36 batch 137 loss 0.06356146186590195\n",
            "epoch 36 batch 138 loss 0.06954178959131241\n",
            "epoch 36 batch 139 loss 0.060241419821977615\n",
            "epoch 36 batch 140 loss 0.06494192034006119\n",
            "epoch 36 batch 141 loss 0.0627567246556282\n",
            "epoch 36 batch 142 loss 0.06412138789892197\n",
            "epoch 36 batch 143 loss 0.058198828250169754\n",
            "epoch 36 batch 144 loss 0.06446497142314911\n",
            "epoch 36 batch 145 loss 0.06338291615247726\n",
            "epoch 36 batch 146 loss 0.062677301466465\n",
            "epoch 36 batch 147 loss 0.06544798612594604\n",
            "epoch 36 batch 148 loss 0.06479877233505249\n",
            "epoch 36 batch 149 loss 0.06295904517173767\n",
            "epoch 36 batch 150 loss 0.06543134152889252\n",
            "epoch 36 batch 151 loss 0.06686872988939285\n",
            "epoch 36 batch 152 loss 0.06664744019508362\n",
            "epoch 36 batch 153 loss 0.06756336987018585\n",
            "epoch 36 batch 154 loss 0.060870397835969925\n",
            "epoch 36 batch 155 loss 0.0629461333155632\n",
            "epoch 36 batch 156 loss 0.06507054716348648\n",
            "epoch 36 batch 157 loss 0.06374222040176392\n",
            "epoch 36 batch 158 loss 0.05862211063504219\n",
            "epoch 36 batch 159 loss 0.06368877738714218\n",
            "epoch 36 batch 160 loss 0.06348692625761032\n",
            "epoch 36 batch 161 loss 0.06951481848955154\n",
            "epoch 36 batch 162 loss 0.061475545167922974\n",
            "epoch 36 batch 163 loss 0.05931882932782173\n",
            "epoch 36 batch 164 loss 0.06370758265256882\n",
            "epoch 36 batch 165 loss 0.06053410843014717\n",
            "epoch 36 batch 166 loss 0.057985253632068634\n",
            "epoch 36 batch 167 loss 0.06666762381792068\n",
            "epoch 36 batch 168 loss 0.06332827359437943\n",
            "epoch 36 batch 169 loss 0.06232159957289696\n",
            "epoch 36 batch 170 loss 0.0681016743183136\n",
            "epoch 36 batch 171 loss 0.06644103676080704\n",
            "epoch 36 batch 172 loss 0.07177497446537018\n",
            "epoch 36 batch 173 loss 0.06537865847349167\n",
            "epoch 36 batch 174 loss 0.061312802135944366\n",
            "epoch 36 batch 175 loss 0.06407376378774643\n",
            "epoch 36 batch 176 loss 0.06777338683605194\n",
            "epoch 36 batch 177 loss 0.06588194519281387\n",
            "epoch 36 batch 178 loss 0.06423582881689072\n",
            "epoch 36 batch 179 loss 0.06196664273738861\n",
            "epoch 36 batch 180 loss 0.05570640042424202\n",
            "epoch 36 batch 181 loss 0.06586892157793045\n",
            "epoch 36 batch 182 loss 0.060468293726444244\n",
            "epoch 36 batch 183 loss 0.059669531881809235\n",
            "epoch 36 batch 184 loss 0.06192238628864288\n",
            "epoch 36 batch 185 loss 0.062429510056972504\n",
            "epoch 36 batch 186 loss 0.06632289290428162\n",
            "epoch 36 batch 187 loss 0.06043686717748642\n",
            "epoch 36 batch 188 loss 0.06132836639881134\n",
            "epoch 36 batch 189 loss 0.06925995647907257\n",
            "epoch 36 batch 190 loss 0.06404002755880356\n",
            "epoch 36 batch 191 loss 0.06019285321235657\n",
            "epoch 36 batch 192 loss 0.0641576275229454\n",
            "epoch 36 batch 193 loss 0.06204857677221298\n",
            "epoch 36 batch 194 loss 0.06621213257312775\n",
            "epoch 36 batch 195 loss 0.06557752937078476\n",
            "epoch 36 batch 196 loss 0.06842128932476044\n",
            "epoch 36 batch 197 loss 0.05956973508000374\n",
            "epoch 36 batch 198 loss 0.06719349324703217\n",
            "epoch 36 batch 199 loss 0.06622356921434402\n",
            "epoch 36 batch 200 loss 0.06067667528986931\n",
            "epoch 36 batch 201 loss 0.06328694522380829\n",
            "epoch 36 batch 202 loss 0.059947699308395386\n",
            "epoch 36 batch 203 loss 0.06564528495073318\n",
            "epoch 36 batch 204 loss 0.06557874381542206\n",
            "epoch 36 batch 205 loss 0.058791134506464005\n",
            "epoch 36 batch 206 loss 0.06185595318675041\n",
            "epoch 36 batch 207 loss 0.062014877796173096\n",
            "epoch 36 batch 208 loss 0.06769408285617828\n",
            "epoch 36 batch 209 loss 0.0621834360063076\n",
            "epoch 36 batch 210 loss 0.06323950737714767\n",
            "epoch 36 batch 211 loss 0.06634487211704254\n",
            "epoch 36 batch 212 loss 0.06111497804522514\n",
            "epoch 36 batch 213 loss 0.06594569236040115\n",
            "epoch 36 batch 214 loss 0.06926607340574265\n",
            "epoch 36 batch 215 loss 0.055848266929388046\n",
            "epoch 36 batch 216 loss 0.06315155327320099\n",
            "epoch 36 batch 217 loss 0.0591532327234745\n",
            "epoch 36 batch 218 loss 0.06162983551621437\n",
            "epoch 36 batch 219 loss 0.06046859920024872\n",
            "epoch 36 batch 220 loss 0.0674680843949318\n",
            "epoch 36 batch 221 loss 0.0625147894024849\n",
            "epoch 36 batch 222 loss 0.06249264255166054\n",
            "epoch 36 batch 223 loss 0.06385916471481323\n",
            "epoch 36 batch 224 loss 0.059445999562740326\n",
            "epoch 36 batch 225 loss 0.06039726361632347\n",
            "epoch 36 batch 226 loss 0.07024179399013519\n",
            "epoch 36 batch 227 loss 0.0627686157822609\n",
            "epoch 36 batch 228 loss 0.06956782937049866\n",
            "epoch 36 batch 229 loss 0.06612168997526169\n",
            "epoch 36 batch 230 loss 0.06501180678606033\n",
            "epoch 37 batch 0 loss 0.06270107626914978\n",
            "epoch 37 batch 1 loss 0.05833396315574646\n",
            "epoch 37 batch 2 loss 0.05703510344028473\n",
            "epoch 37 batch 3 loss 0.06097747012972832\n",
            "epoch 37 batch 4 loss 0.06465913355350494\n",
            "epoch 37 batch 5 loss 0.06315732002258301\n",
            "epoch 37 batch 6 loss 0.06438130140304565\n",
            "epoch 37 batch 7 loss 0.06526385247707367\n",
            "epoch 37 batch 8 loss 0.05448680371046066\n",
            "epoch 37 batch 9 loss 0.056391142308712006\n",
            "epoch 37 batch 10 loss 0.05993968993425369\n",
            "epoch 37 batch 11 loss 0.06078597903251648\n",
            "epoch 37 batch 12 loss 0.05668044462800026\n",
            "epoch 37 batch 13 loss 0.056355997920036316\n",
            "epoch 37 batch 14 loss 0.06610827147960663\n",
            "epoch 37 batch 15 loss 0.05785711109638214\n",
            "epoch 37 batch 16 loss 0.055813293904066086\n",
            "epoch 37 batch 17 loss 0.05782187730073929\n",
            "epoch 37 batch 18 loss 0.060170046985149384\n",
            "epoch 37 batch 19 loss 0.05757647380232811\n",
            "epoch 37 batch 20 loss 0.06549474596977234\n",
            "epoch 37 batch 21 loss 0.056434497237205505\n",
            "epoch 37 batch 22 loss 0.06979844719171524\n",
            "epoch 37 batch 23 loss 0.06538301706314087\n",
            "epoch 37 batch 24 loss 0.0634411871433258\n",
            "epoch 37 batch 25 loss 0.06596311926841736\n",
            "epoch 37 batch 26 loss 0.0591680072247982\n",
            "epoch 37 batch 27 loss 0.05750181898474693\n",
            "epoch 37 batch 28 loss 0.06326096504926682\n",
            "epoch 37 batch 29 loss 0.0662928968667984\n",
            "epoch 37 batch 30 loss 0.06479141861200333\n",
            "epoch 37 batch 31 loss 0.06327532976865768\n",
            "epoch 37 batch 32 loss 0.06124400347471237\n",
            "epoch 37 batch 33 loss 0.06567114591598511\n",
            "epoch 37 batch 34 loss 0.06611135601997375\n",
            "epoch 37 batch 35 loss 0.060228221118450165\n",
            "epoch 37 batch 36 loss 0.05950445681810379\n",
            "epoch 37 batch 37 loss 0.0682545006275177\n",
            "epoch 37 batch 38 loss 0.05525258556008339\n",
            "epoch 37 batch 39 loss 0.06184680759906769\n",
            "epoch 37 batch 40 loss 0.05923239514231682\n",
            "epoch 37 batch 41 loss 0.060337141156196594\n",
            "epoch 37 batch 42 loss 0.05831266567111015\n",
            "epoch 37 batch 43 loss 0.05974293872714043\n",
            "epoch 37 batch 44 loss 0.060172151774168015\n",
            "epoch 37 batch 45 loss 0.06181527301669121\n",
            "epoch 37 batch 46 loss 0.05820664018392563\n",
            "epoch 37 batch 47 loss 0.06013500317931175\n",
            "epoch 37 batch 48 loss 0.06351691484451294\n",
            "epoch 37 batch 49 loss 0.06350798904895782\n",
            "epoch 37 batch 50 loss 0.06124814972281456\n",
            "epoch 37 batch 51 loss 0.06262007355690002\n",
            "epoch 37 batch 52 loss 0.05612862855195999\n",
            "epoch 37 batch 53 loss 0.06041177734732628\n",
            "epoch 37 batch 54 loss 0.06210025027394295\n",
            "epoch 37 batch 55 loss 0.06118623912334442\n",
            "epoch 37 batch 56 loss 0.0614795945584774\n",
            "epoch 37 batch 57 loss 0.05860601365566254\n",
            "epoch 37 batch 58 loss 0.05940935015678406\n",
            "epoch 37 batch 59 loss 0.060758382081985474\n",
            "epoch 37 batch 60 loss 0.06266709417104721\n",
            "epoch 37 batch 61 loss 0.06298095732927322\n",
            "epoch 37 batch 62 loss 0.05849123001098633\n",
            "epoch 37 batch 63 loss 0.061119988560676575\n",
            "epoch 37 batch 64 loss 0.06610651314258575\n",
            "epoch 37 batch 65 loss 0.063560351729393\n",
            "epoch 37 batch 66 loss 0.06399467587471008\n",
            "epoch 37 batch 67 loss 0.06216875836253166\n",
            "epoch 37 batch 68 loss 0.05911973863840103\n",
            "epoch 37 batch 69 loss 0.06305947154760361\n",
            "epoch 37 batch 70 loss 0.05669102817773819\n",
            "epoch 37 batch 71 loss 0.06258266419172287\n",
            "epoch 37 batch 72 loss 0.05777616426348686\n",
            "epoch 37 batch 73 loss 0.06308658421039581\n",
            "epoch 37 batch 74 loss 0.06170026212930679\n",
            "epoch 37 batch 75 loss 0.06371495872735977\n",
            "epoch 37 batch 76 loss 0.059726275503635406\n",
            "epoch 37 batch 77 loss 0.05347238481044769\n",
            "epoch 37 batch 78 loss 0.0644964948296547\n",
            "epoch 37 batch 79 loss 0.06340187788009644\n",
            "epoch 37 batch 80 loss 0.05586713179945946\n",
            "epoch 37 batch 81 loss 0.05831684172153473\n",
            "epoch 37 batch 82 loss 0.05545886233448982\n",
            "epoch 37 batch 83 loss 0.06623295694589615\n",
            "epoch 37 batch 84 loss 0.05086853355169296\n",
            "epoch 37 batch 85 loss 0.06766069680452347\n",
            "epoch 37 batch 86 loss 0.0611250065267086\n",
            "epoch 37 batch 87 loss 0.059966329485177994\n",
            "epoch 37 batch 88 loss 0.0641426220536232\n",
            "epoch 37 batch 89 loss 0.06395348906517029\n",
            "epoch 37 batch 90 loss 0.05540815368294716\n",
            "epoch 37 batch 91 loss 0.06382220983505249\n",
            "epoch 37 batch 92 loss 0.05928664654493332\n",
            "epoch 37 batch 93 loss 0.0628914088010788\n",
            "epoch 37 batch 94 loss 0.05984904617071152\n",
            "epoch 37 batch 95 loss 0.061888162046670914\n",
            "epoch 37 batch 96 loss 0.05590081587433815\n",
            "epoch 37 batch 97 loss 0.06586901098489761\n",
            "epoch 37 batch 98 loss 0.0648975744843483\n",
            "epoch 37 batch 99 loss 0.06756635755300522\n",
            "epoch 37 batch 100 loss 0.06533905863761902\n",
            "epoch 37 batch 101 loss 0.0614141970872879\n",
            "epoch 37 batch 102 loss 0.05973391979932785\n",
            "epoch 37 batch 103 loss 0.06670457124710083\n",
            "epoch 37 batch 104 loss 0.06124744564294815\n",
            "epoch 37 batch 105 loss 0.06286954134702682\n",
            "epoch 37 batch 106 loss 0.05625178664922714\n",
            "epoch 37 batch 107 loss 0.05867144092917442\n",
            "epoch 37 batch 108 loss 0.06060197204351425\n",
            "epoch 37 batch 109 loss 0.06415243446826935\n",
            "epoch 37 batch 110 loss 0.06513828784227371\n",
            "epoch 37 batch 111 loss 0.0680663213133812\n",
            "epoch 37 batch 112 loss 0.06958542764186859\n",
            "epoch 37 batch 113 loss 0.06594584137201309\n",
            "epoch 37 batch 114 loss 0.054970480501651764\n",
            "epoch 37 batch 115 loss 0.06453914940357208\n",
            "epoch 37 batch 116 loss 0.06127443164587021\n",
            "epoch 37 batch 117 loss 0.06464220583438873\n",
            "epoch 37 batch 118 loss 0.0611170195043087\n",
            "epoch 37 batch 119 loss 0.06545331329107285\n",
            "epoch 37 batch 120 loss 0.05730579420924187\n",
            "epoch 37 batch 121 loss 0.06051225587725639\n",
            "epoch 37 batch 122 loss 0.06008211150765419\n",
            "epoch 37 batch 123 loss 0.07083433121442795\n",
            "epoch 37 batch 124 loss 0.06874481588602066\n",
            "epoch 37 batch 125 loss 0.06297627091407776\n",
            "epoch 37 batch 126 loss 0.06868655234575272\n",
            "epoch 37 batch 127 loss 0.06735912710428238\n",
            "epoch 37 batch 128 loss 0.0670139491558075\n",
            "epoch 37 batch 129 loss 0.06536121666431427\n",
            "epoch 37 batch 130 loss 0.06261546164751053\n",
            "epoch 37 batch 131 loss 0.06432802230119705\n",
            "epoch 37 batch 132 loss 0.06529564410448074\n",
            "epoch 37 batch 133 loss 0.05773516371846199\n",
            "epoch 37 batch 134 loss 0.06378275901079178\n",
            "epoch 37 batch 135 loss 0.06877613067626953\n",
            "epoch 37 batch 136 loss 0.06374837458133698\n",
            "epoch 37 batch 137 loss 0.05913239344954491\n",
            "epoch 37 batch 138 loss 0.062083370983600616\n",
            "epoch 37 batch 139 loss 0.06528124213218689\n",
            "epoch 37 batch 140 loss 0.06338980793952942\n",
            "epoch 37 batch 141 loss 0.07009798288345337\n",
            "epoch 37 batch 142 loss 0.059328414499759674\n",
            "epoch 37 batch 143 loss 0.06179513782262802\n",
            "epoch 37 batch 144 loss 0.06022423878312111\n",
            "epoch 37 batch 145 loss 0.06181694567203522\n",
            "epoch 37 batch 146 loss 0.06059113144874573\n",
            "epoch 37 batch 147 loss 0.061521898955106735\n",
            "epoch 37 batch 148 loss 0.05751785263419151\n",
            "epoch 37 batch 149 loss 0.06750181317329407\n",
            "epoch 37 batch 150 loss 0.05936461314558983\n",
            "epoch 37 batch 151 loss 0.06468021124601364\n",
            "epoch 37 batch 152 loss 0.06675544381141663\n",
            "epoch 37 batch 153 loss 0.061280492693185806\n",
            "epoch 37 batch 154 loss 0.06408589333295822\n",
            "epoch 37 batch 155 loss 0.062367603182792664\n",
            "epoch 37 batch 156 loss 0.06417524069547653\n",
            "epoch 37 batch 157 loss 0.05643679201602936\n",
            "epoch 37 batch 158 loss 0.0623188391327858\n",
            "epoch 37 batch 159 loss 0.05943883955478668\n",
            "epoch 37 batch 160 loss 0.06521725654602051\n",
            "epoch 37 batch 161 loss 0.06473413109779358\n",
            "epoch 37 batch 162 loss 0.0675414428114891\n",
            "epoch 37 batch 163 loss 0.06509191542863846\n",
            "epoch 37 batch 164 loss 0.06307931244373322\n",
            "epoch 37 batch 165 loss 0.06162562221288681\n",
            "epoch 37 batch 166 loss 0.05717084929347038\n",
            "epoch 37 batch 167 loss 0.06450667977333069\n",
            "epoch 37 batch 168 loss 0.06156665086746216\n",
            "epoch 37 batch 169 loss 0.061510052531957626\n",
            "epoch 37 batch 170 loss 0.05912669375538826\n",
            "epoch 37 batch 171 loss 0.06577367335557938\n",
            "epoch 37 batch 172 loss 0.060160014778375626\n",
            "epoch 37 batch 173 loss 0.06732650846242905\n",
            "epoch 37 batch 174 loss 0.06878358870744705\n",
            "epoch 37 batch 175 loss 0.05730701610445976\n",
            "epoch 37 batch 176 loss 0.058620378375053406\n",
            "epoch 37 batch 177 loss 0.057211894541978836\n",
            "epoch 37 batch 178 loss 0.06102205440402031\n",
            "epoch 37 batch 179 loss 0.0670241191983223\n",
            "epoch 37 batch 180 loss 0.06966381520032883\n",
            "epoch 37 batch 181 loss 0.06508035957813263\n",
            "epoch 37 batch 182 loss 0.06273449212312698\n",
            "epoch 37 batch 183 loss 0.06486216932535172\n",
            "epoch 37 batch 184 loss 0.06387970596551895\n",
            "epoch 37 batch 185 loss 0.06098128482699394\n",
            "epoch 37 batch 186 loss 0.0673060417175293\n",
            "epoch 37 batch 187 loss 0.061874594539403915\n",
            "epoch 37 batch 188 loss 0.061349429190158844\n",
            "epoch 37 batch 189 loss 0.05961500108242035\n",
            "epoch 37 batch 190 loss 0.06652596592903137\n",
            "epoch 37 batch 191 loss 0.07345444709062576\n",
            "epoch 37 batch 192 loss 0.06359858810901642\n",
            "epoch 37 batch 193 loss 0.061216481029987335\n",
            "epoch 37 batch 194 loss 0.06178787350654602\n",
            "epoch 37 batch 195 loss 0.06059516221284866\n",
            "epoch 37 batch 196 loss 0.06072219833731651\n",
            "epoch 37 batch 197 loss 0.06568258255720139\n",
            "epoch 37 batch 198 loss 0.05512557923793793\n",
            "epoch 37 batch 199 loss 0.06547330319881439\n",
            "epoch 37 batch 200 loss 0.06476620584726334\n",
            "epoch 37 batch 201 loss 0.06235453113913536\n",
            "epoch 37 batch 202 loss 0.07131512463092804\n",
            "epoch 37 batch 203 loss 0.0627012550830841\n",
            "epoch 37 batch 204 loss 0.06988316029310226\n",
            "epoch 37 batch 205 loss 0.05796802043914795\n",
            "epoch 37 batch 206 loss 0.06843395531177521\n",
            "epoch 37 batch 207 loss 0.06630383431911469\n",
            "epoch 37 batch 208 loss 0.06467214226722717\n",
            "epoch 37 batch 209 loss 0.06321720778942108\n",
            "epoch 37 batch 210 loss 0.05848065763711929\n",
            "epoch 37 batch 211 loss 0.06415323168039322\n",
            "epoch 37 batch 212 loss 0.0694940984249115\n",
            "epoch 37 batch 213 loss 0.06303837150335312\n",
            "epoch 37 batch 214 loss 0.0646207332611084\n",
            "epoch 37 batch 215 loss 0.06334094703197479\n",
            "epoch 37 batch 216 loss 0.06402083486318588\n",
            "epoch 37 batch 217 loss 0.06509608030319214\n",
            "epoch 37 batch 218 loss 0.06247555464506149\n",
            "epoch 37 batch 219 loss 0.0614800788462162\n",
            "epoch 37 batch 220 loss 0.0653286874294281\n",
            "epoch 37 batch 221 loss 0.06061025708913803\n",
            "epoch 37 batch 222 loss 0.0649186372756958\n",
            "epoch 37 batch 223 loss 0.06567017734050751\n",
            "epoch 37 batch 224 loss 0.0698813647031784\n",
            "epoch 37 batch 225 loss 0.057570476084947586\n",
            "epoch 37 batch 226 loss 0.06060842052102089\n",
            "epoch 37 batch 227 loss 0.06762447208166122\n",
            "epoch 37 batch 228 loss 0.06811564415693283\n",
            "epoch 37 batch 229 loss 0.06820456683635712\n",
            "epoch 37 batch 230 loss 0.0652099996805191\n",
            "epoch 38 batch 0 loss 0.06201432645320892\n",
            "epoch 38 batch 1 loss 0.06140324845910072\n",
            "epoch 38 batch 2 loss 0.057431794703006744\n",
            "epoch 38 batch 3 loss 0.05710561200976372\n",
            "epoch 38 batch 4 loss 0.07079993188381195\n",
            "epoch 38 batch 5 loss 0.059999916702508926\n",
            "epoch 38 batch 6 loss 0.05819597840309143\n",
            "epoch 38 batch 7 loss 0.05788583680987358\n",
            "epoch 38 batch 8 loss 0.06419851630926132\n",
            "epoch 38 batch 9 loss 0.06318812072277069\n",
            "epoch 38 batch 10 loss 0.05886990949511528\n",
            "epoch 38 batch 11 loss 0.060704439878463745\n",
            "epoch 38 batch 12 loss 0.05839300900697708\n",
            "epoch 38 batch 13 loss 0.06347504258155823\n",
            "epoch 38 batch 14 loss 0.05935845151543617\n",
            "epoch 38 batch 15 loss 0.058036163449287415\n",
            "epoch 38 batch 16 loss 0.06555711477994919\n",
            "epoch 38 batch 17 loss 0.05939430370926857\n",
            "epoch 38 batch 18 loss 0.06428872048854828\n",
            "epoch 38 batch 19 loss 0.055236078798770905\n",
            "epoch 38 batch 20 loss 0.06675101071596146\n",
            "epoch 38 batch 21 loss 0.05947014316916466\n",
            "epoch 38 batch 22 loss 0.06478571891784668\n",
            "epoch 38 batch 23 loss 0.05994896590709686\n",
            "epoch 38 batch 24 loss 0.059617988765239716\n",
            "epoch 38 batch 25 loss 0.06213344633579254\n",
            "epoch 38 batch 26 loss 0.05926690250635147\n",
            "epoch 38 batch 27 loss 0.06255265325307846\n",
            "epoch 38 batch 28 loss 0.06047210842370987\n",
            "epoch 38 batch 29 loss 0.06160936504602432\n",
            "epoch 38 batch 30 loss 0.06006668880581856\n",
            "epoch 38 batch 31 loss 0.06306688487529755\n",
            "epoch 38 batch 32 loss 0.062362536787986755\n",
            "epoch 38 batch 33 loss 0.06525975465774536\n",
            "epoch 38 batch 34 loss 0.05718596279621124\n",
            "epoch 38 batch 35 loss 0.05428531765937805\n",
            "epoch 38 batch 36 loss 0.05851557478308678\n",
            "epoch 38 batch 37 loss 0.059225089848041534\n",
            "epoch 38 batch 38 loss 0.05892447009682655\n",
            "epoch 38 batch 39 loss 0.06114870682358742\n",
            "epoch 38 batch 40 loss 0.06255395710468292\n",
            "epoch 38 batch 41 loss 0.060801804065704346\n",
            "epoch 38 batch 42 loss 0.06160173937678337\n",
            "epoch 38 batch 43 loss 0.0661226436495781\n",
            "epoch 38 batch 44 loss 0.06091732159256935\n",
            "epoch 38 batch 45 loss 0.058708373457193375\n",
            "epoch 38 batch 46 loss 0.060670483857393265\n",
            "epoch 38 batch 47 loss 0.06072062626481056\n",
            "epoch 38 batch 48 loss 0.06086060777306557\n",
            "epoch 38 batch 49 loss 0.06382910162210464\n",
            "epoch 38 batch 50 loss 0.05935783311724663\n",
            "epoch 38 batch 51 loss 0.06315121054649353\n",
            "epoch 38 batch 52 loss 0.06377021223306656\n",
            "epoch 38 batch 53 loss 0.061186302453279495\n",
            "epoch 38 batch 54 loss 0.05937930569052696\n",
            "epoch 38 batch 55 loss 0.06313952058553696\n",
            "epoch 38 batch 56 loss 0.05832098051905632\n",
            "epoch 38 batch 57 loss 0.06498222798109055\n",
            "epoch 38 batch 58 loss 0.059795081615448\n",
            "epoch 38 batch 59 loss 0.061338044703006744\n",
            "epoch 38 batch 60 loss 0.056847698986530304\n",
            "epoch 38 batch 61 loss 0.06220375746488571\n",
            "epoch 38 batch 62 loss 0.06186962127685547\n",
            "epoch 38 batch 63 loss 0.06686549633741379\n",
            "epoch 38 batch 64 loss 0.05522102490067482\n",
            "epoch 38 batch 65 loss 0.0577983595430851\n",
            "epoch 38 batch 66 loss 0.06020049378275871\n",
            "epoch 38 batch 67 loss 0.061985377222299576\n",
            "epoch 38 batch 68 loss 0.06550776213407516\n",
            "epoch 38 batch 69 loss 0.06147058308124542\n",
            "epoch 38 batch 70 loss 0.05768467113375664\n",
            "epoch 38 batch 71 loss 0.060410596430301666\n",
            "epoch 38 batch 72 loss 0.06627393513917923\n",
            "epoch 38 batch 73 loss 0.05867742374539375\n",
            "epoch 38 batch 74 loss 0.05934809893369675\n",
            "epoch 38 batch 75 loss 0.0643843337893486\n",
            "epoch 38 batch 76 loss 0.06385476142168045\n",
            "epoch 38 batch 77 loss 0.06603015214204788\n",
            "epoch 38 batch 78 loss 0.06539084762334824\n",
            "epoch 38 batch 79 loss 0.06569492816925049\n",
            "epoch 38 batch 80 loss 0.06092938035726547\n",
            "epoch 38 batch 81 loss 0.06193896755576134\n",
            "epoch 38 batch 82 loss 0.06310898065567017\n",
            "epoch 38 batch 83 loss 0.05983198806643486\n",
            "epoch 38 batch 84 loss 0.05522795394062996\n",
            "epoch 38 batch 85 loss 0.06233932450413704\n",
            "epoch 38 batch 86 loss 0.06341594457626343\n",
            "epoch 38 batch 87 loss 0.0652661845088005\n",
            "epoch 38 batch 88 loss 0.061229806393384933\n",
            "epoch 38 batch 89 loss 0.05952117219567299\n",
            "epoch 38 batch 90 loss 0.06366486847400665\n",
            "epoch 38 batch 91 loss 0.0627300888299942\n",
            "epoch 38 batch 92 loss 0.05979043245315552\n",
            "epoch 38 batch 93 loss 0.0651165023446083\n",
            "epoch 38 batch 94 loss 0.06347579509019852\n",
            "epoch 38 batch 95 loss 0.05543974041938782\n",
            "epoch 38 batch 96 loss 0.060834936797618866\n",
            "epoch 38 batch 97 loss 0.06424294412136078\n",
            "epoch 38 batch 98 loss 0.053180500864982605\n",
            "epoch 38 batch 99 loss 0.06590008735656738\n",
            "epoch 38 batch 100 loss 0.06299025565385818\n",
            "epoch 38 batch 101 loss 0.06450800597667694\n",
            "epoch 38 batch 102 loss 0.06382398307323456\n",
            "epoch 38 batch 103 loss 0.06215729936957359\n",
            "epoch 38 batch 104 loss 0.05953928083181381\n",
            "epoch 38 batch 105 loss 0.06312812119722366\n",
            "epoch 38 batch 106 loss 0.06647958606481552\n",
            "epoch 38 batch 107 loss 0.06592626869678497\n",
            "epoch 38 batch 108 loss 0.06127176433801651\n",
            "epoch 38 batch 109 loss 0.06171821057796478\n",
            "epoch 38 batch 110 loss 0.06238359957933426\n",
            "epoch 38 batch 111 loss 0.0608823262155056\n",
            "epoch 38 batch 112 loss 0.06319338828325272\n",
            "epoch 38 batch 113 loss 0.061703428626060486\n",
            "epoch 38 batch 114 loss 0.06375566124916077\n",
            "epoch 38 batch 115 loss 0.06765686720609665\n",
            "epoch 38 batch 116 loss 0.06582552194595337\n",
            "epoch 38 batch 117 loss 0.0594574473798275\n",
            "epoch 38 batch 118 loss 0.06434829533100128\n",
            "epoch 38 batch 119 loss 0.06033184751868248\n",
            "epoch 38 batch 120 loss 0.05695924907922745\n",
            "epoch 38 batch 121 loss 0.05921006575226784\n",
            "epoch 38 batch 122 loss 0.06595717370510101\n",
            "epoch 38 batch 123 loss 0.06244894117116928\n",
            "epoch 38 batch 124 loss 0.05845111981034279\n",
            "epoch 38 batch 125 loss 0.0528227724134922\n",
            "epoch 38 batch 126 loss 0.059338055551052094\n",
            "epoch 38 batch 127 loss 0.05878685787320137\n",
            "epoch 38 batch 128 loss 0.06539619714021683\n",
            "epoch 38 batch 129 loss 0.06786535680294037\n",
            "epoch 38 batch 130 loss 0.0639646053314209\n",
            "epoch 38 batch 131 loss 0.057743385434150696\n",
            "epoch 38 batch 132 loss 0.05679502338171005\n",
            "epoch 38 batch 133 loss 0.06829360127449036\n",
            "epoch 38 batch 134 loss 0.06634967029094696\n",
            "epoch 38 batch 135 loss 0.0610770620405674\n",
            "epoch 38 batch 136 loss 0.06411252170801163\n",
            "epoch 38 batch 137 loss 0.0664050355553627\n",
            "epoch 38 batch 138 loss 0.06103949248790741\n",
            "epoch 38 batch 139 loss 0.06364226341247559\n",
            "epoch 38 batch 140 loss 0.06934712082147598\n",
            "epoch 38 batch 141 loss 0.06176105514168739\n",
            "epoch 38 batch 142 loss 0.05946706235408783\n",
            "epoch 38 batch 143 loss 0.07103068381547928\n",
            "epoch 38 batch 144 loss 0.06104975566267967\n",
            "epoch 38 batch 145 loss 0.06440111249685287\n",
            "epoch 38 batch 146 loss 0.06766851991415024\n",
            "epoch 38 batch 147 loss 0.06451165676116943\n",
            "epoch 38 batch 148 loss 0.06227448955178261\n",
            "epoch 38 batch 149 loss 0.06251317262649536\n",
            "epoch 38 batch 150 loss 0.0656881183385849\n",
            "epoch 38 batch 151 loss 0.05488799512386322\n",
            "epoch 38 batch 152 loss 0.06124977767467499\n",
            "epoch 38 batch 153 loss 0.0614507682621479\n",
            "epoch 38 batch 154 loss 0.061729978770017624\n",
            "epoch 38 batch 155 loss 0.06629316508769989\n",
            "epoch 38 batch 156 loss 0.05836170166730881\n",
            "epoch 38 batch 157 loss 0.0572628453373909\n",
            "epoch 38 batch 158 loss 0.06629864126443863\n",
            "epoch 38 batch 159 loss 0.062216706573963165\n",
            "epoch 38 batch 160 loss 0.05849611759185791\n",
            "epoch 38 batch 161 loss 0.058165695518255234\n",
            "epoch 38 batch 162 loss 0.06649193167686462\n",
            "epoch 38 batch 163 loss 0.061247166246175766\n",
            "epoch 38 batch 164 loss 0.05825722962617874\n",
            "epoch 38 batch 165 loss 0.06437975913286209\n",
            "epoch 38 batch 166 loss 0.059894680976867676\n",
            "epoch 38 batch 167 loss 0.06542728841304779\n",
            "epoch 38 batch 168 loss 0.057979386299848557\n",
            "epoch 38 batch 169 loss 0.061996039003133774\n",
            "epoch 38 batch 170 loss 0.06337042152881622\n",
            "epoch 38 batch 171 loss 0.06731833517551422\n",
            "epoch 38 batch 172 loss 0.06320605427026749\n",
            "epoch 38 batch 173 loss 0.059252288192510605\n",
            "epoch 38 batch 174 loss 0.06120247021317482\n",
            "epoch 38 batch 175 loss 0.06134158372879028\n",
            "epoch 38 batch 176 loss 0.06758257746696472\n",
            "epoch 38 batch 177 loss 0.06187479570508003\n",
            "epoch 38 batch 178 loss 0.06656313687562943\n",
            "epoch 38 batch 179 loss 0.0648183822631836\n",
            "epoch 38 batch 180 loss 0.06780161708593369\n",
            "epoch 38 batch 181 loss 0.06537935137748718\n",
            "epoch 38 batch 182 loss 0.059188734740018845\n",
            "epoch 38 batch 183 loss 0.057408515363931656\n",
            "epoch 38 batch 184 loss 0.06285291910171509\n",
            "epoch 38 batch 185 loss 0.06883986294269562\n",
            "epoch 38 batch 186 loss 0.06015783175826073\n",
            "epoch 38 batch 187 loss 0.06313992291688919\n",
            "epoch 38 batch 188 loss 0.06361997872591019\n",
            "epoch 38 batch 189 loss 0.06314598768949509\n",
            "epoch 38 batch 190 loss 0.06300915032625198\n",
            "epoch 38 batch 191 loss 0.06627978384494781\n",
            "epoch 38 batch 192 loss 0.0540720634162426\n",
            "epoch 38 batch 193 loss 0.06946340203285217\n",
            "epoch 38 batch 194 loss 0.059109751135110855\n",
            "epoch 38 batch 195 loss 0.05371261015534401\n",
            "epoch 38 batch 196 loss 0.06464559584856033\n",
            "epoch 38 batch 197 loss 0.06315667182207108\n",
            "epoch 38 batch 198 loss 0.06314948946237564\n",
            "epoch 38 batch 199 loss 0.06012555956840515\n",
            "epoch 38 batch 200 loss 0.06938084214925766\n",
            "epoch 38 batch 201 loss 0.0702032670378685\n",
            "epoch 38 batch 202 loss 0.06491541117429733\n",
            "epoch 38 batch 203 loss 0.0614246167242527\n",
            "epoch 38 batch 204 loss 0.06218617781996727\n",
            "epoch 38 batch 205 loss 0.07516241073608398\n",
            "epoch 38 batch 206 loss 0.06386101990938187\n",
            "epoch 38 batch 207 loss 0.0598592683672905\n",
            "epoch 38 batch 208 loss 0.06671302020549774\n",
            "epoch 38 batch 209 loss 0.06382157653570175\n",
            "epoch 38 batch 210 loss 0.05991911515593529\n",
            "epoch 38 batch 211 loss 0.06436742097139359\n",
            "epoch 38 batch 212 loss 0.05857903137803078\n",
            "epoch 38 batch 213 loss 0.061767578125\n",
            "epoch 38 batch 214 loss 0.06556336581707001\n",
            "epoch 38 batch 215 loss 0.06922994554042816\n",
            "epoch 38 batch 216 loss 0.07505422830581665\n",
            "epoch 38 batch 217 loss 0.0670032650232315\n",
            "epoch 38 batch 218 loss 0.05888277664780617\n",
            "epoch 38 batch 219 loss 0.06227210536599159\n",
            "epoch 38 batch 220 loss 0.06472999602556229\n",
            "epoch 38 batch 221 loss 0.06554020196199417\n",
            "epoch 38 batch 222 loss 0.0664723739027977\n",
            "epoch 38 batch 223 loss 0.060736801475286484\n",
            "epoch 38 batch 224 loss 0.06543278694152832\n",
            "epoch 38 batch 225 loss 0.06631480157375336\n",
            "epoch 38 batch 226 loss 0.06066513806581497\n",
            "epoch 38 batch 227 loss 0.05943867936730385\n",
            "epoch 38 batch 228 loss 0.0637567788362503\n",
            "epoch 38 batch 229 loss 0.06353405117988586\n",
            "epoch 38 batch 230 loss 0.04766225069761276\n",
            "epoch 39 batch 0 loss 0.06169338896870613\n",
            "epoch 39 batch 1 loss 0.05815434083342552\n",
            "epoch 39 batch 2 loss 0.050257422029972076\n",
            "epoch 39 batch 3 loss 0.05705345794558525\n",
            "epoch 39 batch 4 loss 0.06223129853606224\n",
            "epoch 39 batch 5 loss 0.05942993983626366\n",
            "epoch 39 batch 6 loss 0.0566815584897995\n",
            "epoch 39 batch 7 loss 0.0589761920273304\n",
            "epoch 39 batch 8 loss 0.061483580619096756\n",
            "epoch 39 batch 9 loss 0.06011851131916046\n",
            "epoch 39 batch 10 loss 0.05767403170466423\n",
            "epoch 39 batch 11 loss 0.05610644072294235\n",
            "epoch 39 batch 12 loss 0.06485957652330399\n",
            "epoch 39 batch 13 loss 0.06412796676158905\n",
            "epoch 39 batch 14 loss 0.05795816332101822\n",
            "epoch 39 batch 15 loss 0.058067016303539276\n",
            "epoch 39 batch 16 loss 0.06398942321538925\n",
            "epoch 39 batch 17 loss 0.058309394866228104\n",
            "epoch 39 batch 18 loss 0.06565556675195694\n",
            "epoch 39 batch 19 loss 0.06475897133350372\n",
            "epoch 39 batch 20 loss 0.05832750350236893\n",
            "epoch 39 batch 21 loss 0.06016814708709717\n",
            "epoch 39 batch 22 loss 0.06318389624357224\n",
            "epoch 39 batch 23 loss 0.06808196008205414\n",
            "epoch 39 batch 24 loss 0.05503961443901062\n",
            "epoch 39 batch 25 loss 0.059428565204143524\n",
            "epoch 39 batch 26 loss 0.060446593910455704\n",
            "epoch 39 batch 27 loss 0.05640491470694542\n",
            "epoch 39 batch 28 loss 0.0604291707277298\n",
            "epoch 39 batch 29 loss 0.06371064484119415\n",
            "epoch 39 batch 30 loss 0.059118468314409256\n",
            "epoch 39 batch 31 loss 0.06136368587613106\n",
            "epoch 39 batch 32 loss 0.06266889721155167\n",
            "epoch 39 batch 33 loss 0.06329695880413055\n",
            "epoch 39 batch 34 loss 0.06297136098146439\n",
            "epoch 39 batch 35 loss 0.06347337365150452\n",
            "epoch 39 batch 36 loss 0.05617415904998779\n",
            "epoch 39 batch 37 loss 0.06044645980000496\n",
            "epoch 39 batch 38 loss 0.06218210235238075\n",
            "epoch 39 batch 39 loss 0.058971017599105835\n",
            "epoch 39 batch 40 loss 0.06123980134725571\n",
            "epoch 39 batch 41 loss 0.05843173339962959\n",
            "epoch 39 batch 42 loss 0.05766381695866585\n",
            "epoch 39 batch 43 loss 0.07193449884653091\n",
            "epoch 39 batch 44 loss 0.06834449619054794\n",
            "epoch 39 batch 45 loss 0.06295766681432724\n",
            "epoch 39 batch 46 loss 0.0611380897462368\n",
            "epoch 39 batch 47 loss 0.05644385144114494\n",
            "epoch 39 batch 48 loss 0.058402638882398605\n",
            "epoch 39 batch 49 loss 0.05803783982992172\n",
            "epoch 39 batch 50 loss 0.05925998091697693\n",
            "epoch 39 batch 51 loss 0.06411930173635483\n",
            "epoch 39 batch 52 loss 0.06432899832725525\n",
            "epoch 39 batch 53 loss 0.0638158768415451\n",
            "epoch 39 batch 54 loss 0.06100170686841011\n",
            "epoch 39 batch 55 loss 0.05776328966021538\n",
            "epoch 39 batch 56 loss 0.05954775586724281\n",
            "epoch 39 batch 57 loss 0.06710045784711838\n",
            "epoch 39 batch 58 loss 0.06081436946988106\n",
            "epoch 39 batch 59 loss 0.05887098237872124\n",
            "epoch 39 batch 60 loss 0.05716238543391228\n",
            "epoch 39 batch 61 loss 0.059733860194683075\n",
            "epoch 39 batch 62 loss 0.06297380477190018\n",
            "epoch 39 batch 63 loss 0.06097733974456787\n",
            "epoch 39 batch 64 loss 0.06360745429992676\n",
            "epoch 39 batch 65 loss 0.05790618434548378\n",
            "epoch 39 batch 66 loss 0.06679324060678482\n",
            "epoch 39 batch 67 loss 0.0633736327290535\n",
            "epoch 39 batch 68 loss 0.05900489166378975\n",
            "epoch 39 batch 69 loss 0.06177840009331703\n",
            "epoch 39 batch 70 loss 0.06878077238798141\n",
            "epoch 39 batch 71 loss 0.061465054750442505\n",
            "epoch 39 batch 72 loss 0.0631139948964119\n",
            "epoch 39 batch 73 loss 0.06441959738731384\n",
            "epoch 39 batch 74 loss 0.05944569408893585\n",
            "epoch 39 batch 75 loss 0.06319178640842438\n",
            "epoch 39 batch 76 loss 0.06880386173725128\n",
            "epoch 39 batch 77 loss 0.06124252453446388\n",
            "epoch 39 batch 78 loss 0.061242759227752686\n",
            "epoch 39 batch 79 loss 0.06184357777237892\n",
            "epoch 39 batch 80 loss 0.060163263231515884\n",
            "epoch 39 batch 81 loss 0.0635213628411293\n",
            "epoch 39 batch 82 loss 0.06311745196580887\n",
            "epoch 39 batch 83 loss 0.05983331799507141\n",
            "epoch 39 batch 84 loss 0.05868789181113243\n",
            "epoch 39 batch 85 loss 0.06534134596586227\n",
            "epoch 39 batch 86 loss 0.06468035280704498\n",
            "epoch 39 batch 87 loss 0.061578866094350815\n",
            "epoch 39 batch 88 loss 0.06278548389673233\n",
            "epoch 39 batch 89 loss 0.0639881044626236\n",
            "epoch 39 batch 90 loss 0.05869284272193909\n",
            "epoch 39 batch 91 loss 0.059362269937992096\n",
            "epoch 39 batch 92 loss 0.06383731961250305\n",
            "epoch 39 batch 93 loss 0.06183891370892525\n",
            "epoch 39 batch 94 loss 0.0730372965335846\n",
            "epoch 39 batch 95 loss 0.06519301235675812\n",
            "epoch 39 batch 96 loss 0.05718182399868965\n",
            "epoch 39 batch 97 loss 0.06331063061952591\n",
            "epoch 39 batch 98 loss 0.06323383748531342\n",
            "epoch 39 batch 99 loss 0.07016565650701523\n",
            "epoch 39 batch 100 loss 0.06638974696397781\n",
            "epoch 39 batch 101 loss 0.05816926434636116\n",
            "epoch 39 batch 102 loss 0.062430959194898605\n",
            "epoch 39 batch 103 loss 0.06344571709632874\n",
            "epoch 39 batch 104 loss 0.060276344418525696\n",
            "epoch 39 batch 105 loss 0.06544371694326401\n",
            "epoch 39 batch 106 loss 0.05808870494365692\n",
            "epoch 39 batch 107 loss 0.06210269406437874\n",
            "epoch 39 batch 108 loss 0.06089010089635849\n",
            "epoch 39 batch 109 loss 0.059968724846839905\n",
            "epoch 39 batch 110 loss 0.07080523669719696\n",
            "epoch 39 batch 111 loss 0.06194330379366875\n",
            "epoch 39 batch 112 loss 0.05336496978998184\n",
            "epoch 39 batch 113 loss 0.055622559040784836\n",
            "epoch 39 batch 114 loss 0.06410042196512222\n",
            "epoch 39 batch 115 loss 0.059837568551301956\n",
            "epoch 39 batch 116 loss 0.0664079561829567\n",
            "epoch 39 batch 117 loss 0.057494450360536575\n",
            "epoch 39 batch 118 loss 0.06002092361450195\n",
            "epoch 39 batch 119 loss 0.06330496817827225\n",
            "epoch 39 batch 120 loss 0.06343436241149902\n",
            "epoch 39 batch 121 loss 0.060942165553569794\n",
            "epoch 39 batch 122 loss 0.05645198002457619\n",
            "epoch 39 batch 123 loss 0.06485272943973541\n",
            "epoch 39 batch 124 loss 0.06415338814258575\n",
            "epoch 39 batch 125 loss 0.06296250969171524\n",
            "epoch 39 batch 126 loss 0.06466526538133621\n",
            "epoch 39 batch 127 loss 0.0588807538151741\n",
            "epoch 39 batch 128 loss 0.05825166776776314\n",
            "epoch 39 batch 129 loss 0.06210542842745781\n",
            "epoch 39 batch 130 loss 0.06267888844013214\n",
            "epoch 39 batch 131 loss 0.05719531327486038\n",
            "epoch 39 batch 132 loss 0.06389423459768295\n",
            "epoch 39 batch 133 loss 0.0613916851580143\n",
            "epoch 39 batch 134 loss 0.06437437236309052\n",
            "epoch 39 batch 135 loss 0.062378767877817154\n",
            "epoch 39 batch 136 loss 0.06729552894830704\n",
            "epoch 39 batch 137 loss 0.058242157101631165\n",
            "epoch 39 batch 138 loss 0.06590881943702698\n",
            "epoch 39 batch 139 loss 0.06919190287590027\n",
            "epoch 39 batch 140 loss 0.06516720354557037\n",
            "epoch 39 batch 141 loss 0.0584154948592186\n",
            "epoch 39 batch 142 loss 0.0637943372130394\n",
            "epoch 39 batch 143 loss 0.06670801341533661\n",
            "epoch 39 batch 144 loss 0.058115340769290924\n",
            "epoch 39 batch 145 loss 0.060583196580410004\n",
            "epoch 39 batch 146 loss 0.06524547934532166\n",
            "epoch 39 batch 147 loss 0.06420028209686279\n",
            "epoch 39 batch 148 loss 0.06816352903842926\n",
            "epoch 39 batch 149 loss 0.06351649016141891\n",
            "epoch 39 batch 150 loss 0.06215514615178108\n",
            "epoch 39 batch 151 loss 0.06995003670454025\n",
            "epoch 39 batch 152 loss 0.06283679604530334\n",
            "epoch 39 batch 153 loss 0.06614149361848831\n",
            "epoch 39 batch 154 loss 0.0611787848174572\n",
            "epoch 39 batch 155 loss 0.06325341761112213\n",
            "epoch 39 batch 156 loss 0.06296522170305252\n",
            "epoch 39 batch 157 loss 0.0668725073337555\n",
            "epoch 39 batch 158 loss 0.058246586471796036\n",
            "epoch 39 batch 159 loss 0.06446852535009384\n",
            "epoch 39 batch 160 loss 0.07076878100633621\n",
            "epoch 39 batch 161 loss 0.06388740986585617\n",
            "epoch 39 batch 162 loss 0.06805932521820068\n",
            "epoch 39 batch 163 loss 0.06715017557144165\n",
            "epoch 39 batch 164 loss 0.059758260846138\n",
            "epoch 39 batch 165 loss 0.06292006373405457\n",
            "epoch 39 batch 166 loss 0.06397119164466858\n",
            "epoch 39 batch 167 loss 0.060677267611026764\n",
            "epoch 39 batch 168 loss 0.058099862188100815\n",
            "epoch 39 batch 169 loss 0.06012525036931038\n",
            "epoch 39 batch 170 loss 0.061346568167209625\n",
            "epoch 39 batch 171 loss 0.059773463755846024\n",
            "epoch 39 batch 172 loss 0.0654686763882637\n",
            "epoch 39 batch 173 loss 0.06416798382997513\n",
            "epoch 39 batch 174 loss 0.07027355581521988\n",
            "epoch 39 batch 175 loss 0.06348691135644913\n",
            "epoch 39 batch 176 loss 0.06449878960847855\n",
            "epoch 39 batch 177 loss 0.059384096413850784\n",
            "epoch 39 batch 178 loss 0.060570187866687775\n",
            "epoch 39 batch 179 loss 0.06645920127630234\n",
            "epoch 39 batch 180 loss 0.05569266155362129\n",
            "epoch 39 batch 181 loss 0.06121876463294029\n",
            "epoch 39 batch 182 loss 0.06532933562994003\n",
            "epoch 39 batch 183 loss 0.06392564624547958\n",
            "epoch 39 batch 184 loss 0.0631827712059021\n",
            "epoch 39 batch 185 loss 0.06507642567157745\n",
            "epoch 39 batch 186 loss 0.057784564793109894\n",
            "epoch 39 batch 187 loss 0.06321530789136887\n",
            "epoch 39 batch 188 loss 0.06567329168319702\n",
            "epoch 39 batch 189 loss 0.06495413929224014\n",
            "epoch 39 batch 190 loss 0.06567113101482391\n",
            "epoch 39 batch 191 loss 0.06263864785432816\n",
            "epoch 39 batch 192 loss 0.0590837299823761\n",
            "epoch 39 batch 193 loss 0.0626470223069191\n",
            "epoch 39 batch 194 loss 0.05630975589156151\n",
            "epoch 39 batch 195 loss 0.06029411777853966\n",
            "epoch 39 batch 196 loss 0.05802050232887268\n",
            "epoch 39 batch 197 loss 0.06468414515256882\n",
            "epoch 39 batch 198 loss 0.061121806502342224\n",
            "epoch 39 batch 199 loss 0.06177550181746483\n",
            "epoch 39 batch 200 loss 0.06029440462589264\n",
            "epoch 39 batch 201 loss 0.07053159177303314\n",
            "epoch 39 batch 202 loss 0.06469088047742844\n",
            "epoch 39 batch 203 loss 0.05940065532922745\n",
            "epoch 39 batch 204 loss 0.0587582029402256\n",
            "epoch 39 batch 205 loss 0.07379524409770966\n",
            "epoch 39 batch 206 loss 0.05933437868952751\n",
            "epoch 39 batch 207 loss 0.06149758771061897\n",
            "epoch 39 batch 208 loss 0.05984096601605415\n",
            "epoch 39 batch 209 loss 0.061747532337903976\n",
            "epoch 39 batch 210 loss 0.05950916185975075\n",
            "epoch 39 batch 211 loss 0.06292618811130524\n",
            "epoch 39 batch 212 loss 0.0650118738412857\n",
            "epoch 39 batch 213 loss 0.06319636106491089\n",
            "epoch 39 batch 214 loss 0.05961475521326065\n",
            "epoch 39 batch 215 loss 0.0627049133181572\n",
            "epoch 39 batch 216 loss 0.05851684510707855\n",
            "epoch 39 batch 217 loss 0.061398256570100784\n",
            "epoch 39 batch 218 loss 0.06343569606542587\n",
            "epoch 39 batch 219 loss 0.06752751767635345\n",
            "epoch 39 batch 220 loss 0.06216873973608017\n",
            "epoch 39 batch 221 loss 0.05846215412020683\n",
            "epoch 39 batch 222 loss 0.06796612590551376\n",
            "epoch 39 batch 223 loss 0.06474323570728302\n",
            "epoch 39 batch 224 loss 0.05681051313877106\n",
            "epoch 39 batch 225 loss 0.06686443835496902\n",
            "epoch 39 batch 226 loss 0.06119421496987343\n",
            "epoch 39 batch 227 loss 0.0630614310503006\n",
            "epoch 39 batch 228 loss 0.053942061960697174\n",
            "epoch 39 batch 229 loss 0.06692159920930862\n",
            "epoch 39 batch 230 loss 0.06932534277439117\n",
            "epoch 40 batch 0 loss 0.06069817394018173\n",
            "epoch 40 batch 1 loss 0.0592699758708477\n",
            "epoch 40 batch 2 loss 0.058922216296195984\n",
            "epoch 40 batch 3 loss 0.05365215986967087\n",
            "epoch 40 batch 4 loss 0.058889828622341156\n",
            "epoch 40 batch 5 loss 0.061664897948503494\n",
            "epoch 40 batch 6 loss 0.06314733624458313\n",
            "epoch 40 batch 7 loss 0.06158648803830147\n",
            "epoch 40 batch 8 loss 0.06781240552663803\n",
            "epoch 40 batch 9 loss 0.05605641007423401\n",
            "epoch 40 batch 10 loss 0.06103745847940445\n",
            "epoch 40 batch 11 loss 0.05866437777876854\n",
            "epoch 40 batch 12 loss 0.057163339108228683\n",
            "epoch 40 batch 13 loss 0.055959682911634445\n",
            "epoch 40 batch 14 loss 0.06106419861316681\n",
            "epoch 40 batch 15 loss 0.06774590164422989\n",
            "epoch 40 batch 16 loss 0.06069602444767952\n",
            "epoch 40 batch 17 loss 0.05811622738838196\n",
            "epoch 40 batch 18 loss 0.06133875995874405\n",
            "epoch 40 batch 19 loss 0.06524686515331268\n",
            "epoch 40 batch 20 loss 0.060793355107307434\n",
            "epoch 40 batch 21 loss 0.0598791129887104\n",
            "epoch 40 batch 22 loss 0.06059454753994942\n",
            "epoch 40 batch 23 loss 0.06321315467357635\n",
            "epoch 40 batch 24 loss 0.06393452733755112\n",
            "epoch 40 batch 25 loss 0.059864189475774765\n",
            "epoch 40 batch 26 loss 0.06508632749319077\n",
            "epoch 40 batch 27 loss 0.05894264951348305\n",
            "epoch 40 batch 28 loss 0.06269091367721558\n",
            "epoch 40 batch 29 loss 0.060888972133398056\n",
            "epoch 40 batch 30 loss 0.06036270782351494\n",
            "epoch 40 batch 31 loss 0.06440240889787674\n",
            "epoch 40 batch 32 loss 0.057255007326602936\n",
            "epoch 40 batch 33 loss 0.05907677486538887\n",
            "epoch 40 batch 34 loss 0.06221450865268707\n",
            "epoch 40 batch 35 loss 0.060616735368967056\n",
            "epoch 40 batch 36 loss 0.06084384769201279\n",
            "epoch 40 batch 37 loss 0.05943199247121811\n",
            "epoch 40 batch 38 loss 0.060440488159656525\n",
            "epoch 40 batch 39 loss 0.05747457966208458\n",
            "epoch 40 batch 40 loss 0.062006887048482895\n",
            "epoch 40 batch 41 loss 0.057776667177677155\n",
            "epoch 40 batch 42 loss 0.06344357132911682\n",
            "epoch 40 batch 43 loss 0.0624302476644516\n",
            "epoch 40 batch 44 loss 0.05525306239724159\n",
            "epoch 40 batch 45 loss 0.06710711866617203\n",
            "epoch 40 batch 46 loss 0.06572456657886505\n",
            "epoch 40 batch 47 loss 0.058415234088897705\n",
            "epoch 40 batch 48 loss 0.059645142406225204\n",
            "epoch 40 batch 49 loss 0.06136681139469147\n",
            "epoch 40 batch 50 loss 0.06498909741640091\n",
            "epoch 40 batch 51 loss 0.061151277273893356\n",
            "epoch 40 batch 52 loss 0.06202339008450508\n",
            "epoch 40 batch 53 loss 0.06517364084720612\n",
            "epoch 40 batch 54 loss 0.056174296885728836\n",
            "epoch 40 batch 55 loss 0.06067115068435669\n",
            "epoch 40 batch 56 loss 0.05631091445684433\n",
            "epoch 40 batch 57 loss 0.06381742656230927\n",
            "epoch 40 batch 58 loss 0.057825375348329544\n",
            "epoch 40 batch 59 loss 0.059841517359018326\n",
            "epoch 40 batch 60 loss 0.061566345393657684\n",
            "epoch 40 batch 61 loss 0.06074773147702217\n",
            "epoch 40 batch 62 loss 0.06680930405855179\n",
            "epoch 40 batch 63 loss 0.059307582676410675\n",
            "epoch 40 batch 64 loss 0.06528577208518982\n",
            "epoch 40 batch 65 loss 0.05849626660346985\n",
            "epoch 40 batch 66 loss 0.05758367478847504\n",
            "epoch 40 batch 67 loss 0.057867392897605896\n",
            "epoch 40 batch 68 loss 0.06306339055299759\n",
            "epoch 40 batch 69 loss 0.05583139508962631\n",
            "epoch 40 batch 70 loss 0.061641331762075424\n",
            "epoch 40 batch 71 loss 0.05941963940858841\n",
            "epoch 40 batch 72 loss 0.06787225604057312\n",
            "epoch 40 batch 73 loss 0.06680931895971298\n",
            "epoch 40 batch 74 loss 0.061611074954271317\n",
            "epoch 40 batch 75 loss 0.058873251080513\n",
            "epoch 40 batch 76 loss 0.059647660702466965\n",
            "epoch 40 batch 77 loss 0.06754075735807419\n",
            "epoch 40 batch 78 loss 0.06269477307796478\n",
            "epoch 40 batch 79 loss 0.05993223190307617\n",
            "epoch 40 batch 80 loss 0.05705111101269722\n",
            "epoch 40 batch 81 loss 0.06911720335483551\n",
            "epoch 40 batch 82 loss 0.052404239773750305\n",
            "epoch 40 batch 83 loss 0.06336407363414764\n",
            "epoch 40 batch 84 loss 0.06346823275089264\n",
            "epoch 40 batch 85 loss 0.059907130897045135\n",
            "epoch 40 batch 86 loss 0.059874001890420914\n",
            "epoch 40 batch 87 loss 0.05679220333695412\n",
            "epoch 40 batch 88 loss 0.05554348602890968\n",
            "epoch 40 batch 89 loss 0.06108963117003441\n",
            "epoch 40 batch 90 loss 0.06413581222295761\n",
            "epoch 40 batch 91 loss 0.05879462510347366\n",
            "epoch 40 batch 92 loss 0.05945823714137077\n",
            "epoch 40 batch 93 loss 0.06252061575651169\n",
            "epoch 40 batch 94 loss 0.061213161796331406\n",
            "epoch 40 batch 95 loss 0.06166379898786545\n",
            "epoch 40 batch 96 loss 0.060504939407110214\n",
            "epoch 40 batch 97 loss 0.06096939742565155\n",
            "epoch 40 batch 98 loss 0.06438015401363373\n",
            "epoch 40 batch 99 loss 0.0543552003800869\n",
            "epoch 40 batch 100 loss 0.059587281197309494\n",
            "epoch 40 batch 101 loss 0.06018286943435669\n",
            "epoch 40 batch 102 loss 0.06210210919380188\n",
            "epoch 40 batch 103 loss 0.061208177357912064\n",
            "epoch 40 batch 104 loss 0.06489425897598267\n",
            "epoch 40 batch 105 loss 0.06011831387877464\n",
            "epoch 40 batch 106 loss 0.06547340005636215\n",
            "epoch 40 batch 107 loss 0.060421720147132874\n",
            "epoch 40 batch 108 loss 0.05508074536919594\n",
            "epoch 40 batch 109 loss 0.05982397869229317\n",
            "epoch 40 batch 110 loss 0.06548486649990082\n",
            "epoch 40 batch 111 loss 0.06304370611906052\n",
            "epoch 40 batch 112 loss 0.06504376232624054\n",
            "epoch 40 batch 113 loss 0.061097633093595505\n",
            "epoch 40 batch 114 loss 0.05887241289019585\n",
            "epoch 40 batch 115 loss 0.05964979901909828\n",
            "epoch 40 batch 116 loss 0.05900596082210541\n",
            "epoch 40 batch 117 loss 0.06411246210336685\n",
            "epoch 40 batch 118 loss 0.06369905173778534\n",
            "epoch 40 batch 119 loss 0.06327211856842041\n",
            "epoch 40 batch 120 loss 0.06589390337467194\n",
            "epoch 40 batch 121 loss 0.068141870200634\n",
            "epoch 40 batch 122 loss 0.05922018736600876\n",
            "epoch 40 batch 123 loss 0.05998886004090309\n",
            "epoch 40 batch 124 loss 0.06315456330776215\n",
            "epoch 40 batch 125 loss 0.06230241805315018\n",
            "epoch 40 batch 126 loss 0.056701138615608215\n",
            "epoch 40 batch 127 loss 0.060855306684970856\n",
            "epoch 40 batch 128 loss 0.05912146344780922\n",
            "epoch 40 batch 129 loss 0.06062621250748634\n",
            "epoch 40 batch 130 loss 0.07035184651613235\n",
            "epoch 40 batch 131 loss 0.06492698937654495\n",
            "epoch 40 batch 132 loss 0.062492791563272476\n",
            "epoch 40 batch 133 loss 0.06073582172393799\n",
            "epoch 40 batch 134 loss 0.06192455813288689\n",
            "epoch 40 batch 135 loss 0.065755195915699\n",
            "epoch 40 batch 136 loss 0.06178521364927292\n",
            "epoch 40 batch 137 loss 0.06039814651012421\n",
            "epoch 40 batch 138 loss 0.06256235390901566\n",
            "epoch 40 batch 139 loss 0.06386013329029083\n",
            "epoch 40 batch 140 loss 0.06259628385305405\n",
            "epoch 40 batch 141 loss 0.06064915284514427\n",
            "epoch 40 batch 142 loss 0.06057857349514961\n",
            "epoch 40 batch 143 loss 0.061515629291534424\n",
            "epoch 40 batch 144 loss 0.06326402723789215\n",
            "epoch 40 batch 145 loss 0.06538794189691544\n",
            "epoch 40 batch 146 loss 0.06756655126810074\n",
            "epoch 40 batch 147 loss 0.058345723897218704\n",
            "epoch 40 batch 148 loss 0.06278046220541\n",
            "epoch 40 batch 149 loss 0.058896686881780624\n",
            "epoch 40 batch 150 loss 0.0633772537112236\n",
            "epoch 40 batch 151 loss 0.06881421059370041\n",
            "epoch 40 batch 152 loss 0.06483770161867142\n",
            "epoch 40 batch 153 loss 0.06304757297039032\n",
            "epoch 40 batch 154 loss 0.05979296937584877\n",
            "epoch 40 batch 155 loss 0.06030020862817764\n",
            "epoch 40 batch 156 loss 0.054622191935777664\n",
            "epoch 40 batch 157 loss 0.06364590674638748\n",
            "epoch 40 batch 158 loss 0.06262131035327911\n",
            "epoch 40 batch 159 loss 0.06442839652299881\n",
            "epoch 40 batch 160 loss 0.060429897159338\n",
            "epoch 40 batch 161 loss 0.06467811018228531\n",
            "epoch 40 batch 162 loss 0.060711491852998734\n",
            "epoch 40 batch 163 loss 0.06458264589309692\n",
            "epoch 40 batch 164 loss 0.06166814640164375\n",
            "epoch 40 batch 165 loss 0.05507860705256462\n",
            "epoch 40 batch 166 loss 0.06884652376174927\n",
            "epoch 40 batch 167 loss 0.061251673847436905\n",
            "epoch 40 batch 168 loss 0.06278309971094131\n",
            "epoch 40 batch 169 loss 0.06259601563215256\n",
            "epoch 40 batch 170 loss 0.06483723968267441\n",
            "epoch 40 batch 171 loss 0.06798253208398819\n",
            "epoch 40 batch 172 loss 0.0702514797449112\n",
            "epoch 40 batch 173 loss 0.06313608586788177\n",
            "epoch 40 batch 174 loss 0.06062616407871246\n",
            "epoch 40 batch 175 loss 0.05966811999678612\n",
            "epoch 40 batch 176 loss 0.06983311474323273\n",
            "epoch 40 batch 177 loss 0.057643819600343704\n",
            "epoch 40 batch 178 loss 0.06278175115585327\n",
            "epoch 40 batch 179 loss 0.05655203014612198\n",
            "epoch 40 batch 180 loss 0.0643259733915329\n",
            "epoch 40 batch 181 loss 0.06734149158000946\n",
            "epoch 40 batch 182 loss 0.06434991955757141\n",
            "epoch 40 batch 183 loss 0.06091172620654106\n",
            "epoch 40 batch 184 loss 0.06150441989302635\n",
            "epoch 40 batch 185 loss 0.06043749675154686\n",
            "epoch 40 batch 186 loss 0.06645794957876205\n",
            "epoch 40 batch 187 loss 0.06812608242034912\n",
            "epoch 40 batch 188 loss 0.06252774596214294\n",
            "epoch 40 batch 189 loss 0.060448646545410156\n",
            "epoch 40 batch 190 loss 0.06742475181818008\n",
            "epoch 40 batch 191 loss 0.06566214561462402\n",
            "epoch 40 batch 192 loss 0.054837919771671295\n",
            "epoch 40 batch 193 loss 0.06281554698944092\n",
            "epoch 40 batch 194 loss 0.06323063373565674\n",
            "epoch 40 batch 195 loss 0.06715898960828781\n",
            "epoch 40 batch 196 loss 0.06343869864940643\n",
            "epoch 40 batch 197 loss 0.06049345061182976\n",
            "epoch 40 batch 198 loss 0.06471972167491913\n",
            "epoch 40 batch 199 loss 0.06094928830862045\n",
            "epoch 40 batch 200 loss 0.06240912154316902\n",
            "epoch 40 batch 201 loss 0.06287387013435364\n",
            "epoch 40 batch 202 loss 0.05936133861541748\n",
            "epoch 40 batch 203 loss 0.06847090274095535\n",
            "epoch 40 batch 204 loss 0.06418143957853317\n",
            "epoch 40 batch 205 loss 0.0650874674320221\n",
            "epoch 40 batch 206 loss 0.0597994290292263\n",
            "epoch 40 batch 207 loss 0.05684380978345871\n",
            "epoch 40 batch 208 loss 0.06516054272651672\n",
            "epoch 40 batch 209 loss 0.06053747981786728\n",
            "epoch 40 batch 210 loss 0.05856769531965256\n",
            "epoch 40 batch 211 loss 0.06849443912506104\n",
            "epoch 40 batch 212 loss 0.060082316398620605\n",
            "epoch 40 batch 213 loss 0.06291088461875916\n",
            "epoch 40 batch 214 loss 0.061714574694633484\n",
            "epoch 40 batch 215 loss 0.06191037595272064\n",
            "epoch 40 batch 216 loss 0.0558149591088295\n",
            "epoch 40 batch 217 loss 0.06788292527198792\n",
            "epoch 40 batch 218 loss 0.05626603960990906\n",
            "epoch 40 batch 219 loss 0.0621165968477726\n",
            "epoch 40 batch 220 loss 0.06777670234441757\n",
            "epoch 40 batch 221 loss 0.06519430130720139\n",
            "epoch 40 batch 222 loss 0.06476191431283951\n",
            "epoch 40 batch 223 loss 0.06495214253664017\n",
            "epoch 40 batch 224 loss 0.05887790769338608\n",
            "epoch 40 batch 225 loss 0.06371578574180603\n",
            "epoch 40 batch 226 loss 0.06792041659355164\n",
            "epoch 40 batch 227 loss 0.0658528059720993\n",
            "epoch 40 batch 228 loss 0.06550037115812302\n",
            "epoch 40 batch 229 loss 0.059423960745334625\n",
            "epoch 40 batch 230 loss 0.06157075613737106\n",
            "epoch 41 batch 0 loss 0.05782092362642288\n",
            "epoch 41 batch 1 loss 0.0631144717335701\n",
            "epoch 41 batch 2 loss 0.05714481323957443\n",
            "epoch 41 batch 3 loss 0.06298447400331497\n",
            "epoch 41 batch 4 loss 0.06274556368589401\n",
            "epoch 41 batch 5 loss 0.060686685144901276\n",
            "epoch 41 batch 6 loss 0.06172456219792366\n",
            "epoch 41 batch 7 loss 0.06156233325600624\n",
            "epoch 41 batch 8 loss 0.05969807878136635\n",
            "epoch 41 batch 9 loss 0.06227698549628258\n",
            "epoch 41 batch 10 loss 0.06077834591269493\n",
            "epoch 41 batch 11 loss 0.06668607890605927\n",
            "epoch 41 batch 12 loss 0.06090860441327095\n",
            "epoch 41 batch 13 loss 0.057192593812942505\n",
            "epoch 41 batch 14 loss 0.0550447516143322\n",
            "epoch 41 batch 15 loss 0.05814310163259506\n",
            "epoch 41 batch 16 loss 0.053974978625774384\n",
            "epoch 41 batch 17 loss 0.0574435293674469\n",
            "epoch 41 batch 18 loss 0.061152055859565735\n",
            "epoch 41 batch 19 loss 0.0586087591946125\n",
            "epoch 41 batch 20 loss 0.06291764974594116\n",
            "epoch 41 batch 21 loss 0.0541948564350605\n",
            "epoch 41 batch 22 loss 0.06319039314985275\n",
            "epoch 41 batch 23 loss 0.054618995636701584\n",
            "epoch 41 batch 24 loss 0.05955827608704567\n",
            "epoch 41 batch 25 loss 0.06283294409513474\n",
            "epoch 41 batch 26 loss 0.05584158003330231\n",
            "epoch 41 batch 27 loss 0.05806634947657585\n",
            "epoch 41 batch 28 loss 0.05446315184235573\n",
            "epoch 41 batch 29 loss 0.05515200272202492\n",
            "epoch 41 batch 30 loss 0.05968873202800751\n",
            "epoch 41 batch 31 loss 0.0605783648788929\n",
            "epoch 41 batch 32 loss 0.058525823056697845\n",
            "epoch 41 batch 33 loss 0.0579603835940361\n",
            "epoch 41 batch 34 loss 0.0561455599963665\n",
            "epoch 41 batch 35 loss 0.054735418409109116\n",
            "epoch 41 batch 36 loss 0.05769899860024452\n",
            "epoch 41 batch 37 loss 0.061343543231487274\n",
            "epoch 41 batch 38 loss 0.059971582144498825\n",
            "epoch 41 batch 39 loss 0.057489361613988876\n",
            "epoch 41 batch 40 loss 0.05796750262379646\n",
            "epoch 41 batch 41 loss 0.058109983801841736\n",
            "epoch 41 batch 42 loss 0.057732682675123215\n",
            "epoch 41 batch 43 loss 0.05656035244464874\n",
            "epoch 41 batch 44 loss 0.060651637613773346\n",
            "epoch 41 batch 45 loss 0.05657365173101425\n",
            "epoch 41 batch 46 loss 0.057082876563072205\n",
            "epoch 41 batch 47 loss 0.06461413949728012\n",
            "epoch 41 batch 48 loss 0.05902668461203575\n",
            "epoch 41 batch 49 loss 0.06131649762392044\n",
            "epoch 41 batch 50 loss 0.05607825517654419\n",
            "epoch 41 batch 51 loss 0.058376166969537735\n",
            "epoch 41 batch 52 loss 0.06254961341619492\n",
            "epoch 41 batch 53 loss 0.06504988670349121\n",
            "epoch 41 batch 54 loss 0.056817539036273956\n",
            "epoch 41 batch 55 loss 0.06406093388795853\n",
            "epoch 41 batch 56 loss 0.061680082231760025\n",
            "epoch 41 batch 57 loss 0.060447871685028076\n",
            "epoch 41 batch 58 loss 0.05816115066409111\n",
            "epoch 41 batch 59 loss 0.060135483741760254\n",
            "epoch 41 batch 60 loss 0.06207225099205971\n",
            "epoch 41 batch 61 loss 0.05687675625085831\n",
            "epoch 41 batch 62 loss 0.0611935555934906\n",
            "epoch 41 batch 63 loss 0.05909303203225136\n",
            "epoch 41 batch 64 loss 0.05613083019852638\n",
            "epoch 41 batch 65 loss 0.06173859164118767\n",
            "epoch 41 batch 66 loss 0.05826246738433838\n",
            "epoch 41 batch 67 loss 0.07265529781579971\n",
            "epoch 41 batch 68 loss 0.061347536742687225\n",
            "epoch 41 batch 69 loss 0.06219060719013214\n",
            "epoch 41 batch 70 loss 0.0661613941192627\n",
            "epoch 41 batch 71 loss 0.05815140902996063\n",
            "epoch 41 batch 72 loss 0.05815395340323448\n",
            "epoch 41 batch 73 loss 0.06610599905252457\n",
            "epoch 41 batch 74 loss 0.06071097031235695\n",
            "epoch 41 batch 75 loss 0.058410026133060455\n",
            "epoch 41 batch 76 loss 0.06317725777626038\n",
            "epoch 41 batch 77 loss 0.06297660619020462\n",
            "epoch 41 batch 78 loss 0.05798908323049545\n",
            "epoch 41 batch 79 loss 0.0640680268406868\n",
            "epoch 41 batch 80 loss 0.06094908341765404\n",
            "epoch 41 batch 81 loss 0.06663622707128525\n",
            "epoch 41 batch 82 loss 0.059127699583768845\n",
            "epoch 41 batch 83 loss 0.05868510529398918\n",
            "epoch 41 batch 84 loss 0.05919864773750305\n",
            "epoch 41 batch 85 loss 0.06205698102712631\n",
            "epoch 41 batch 86 loss 0.05944155529141426\n",
            "epoch 41 batch 87 loss 0.060370370745658875\n",
            "epoch 41 batch 88 loss 0.06445053964853287\n",
            "epoch 41 batch 89 loss 0.06432455778121948\n",
            "epoch 41 batch 90 loss 0.06568063795566559\n",
            "epoch 41 batch 91 loss 0.060045305639505386\n",
            "epoch 41 batch 92 loss 0.06143763288855553\n",
            "epoch 41 batch 93 loss 0.06118695065379143\n",
            "epoch 41 batch 94 loss 0.05871669203042984\n",
            "epoch 41 batch 95 loss 0.06392877548933029\n",
            "epoch 41 batch 96 loss 0.06539531797170639\n",
            "epoch 41 batch 97 loss 0.05618320405483246\n",
            "epoch 41 batch 98 loss 0.05878296494483948\n",
            "epoch 41 batch 99 loss 0.05989867448806763\n",
            "epoch 41 batch 100 loss 0.06513775885105133\n",
            "epoch 41 batch 101 loss 0.06251972168684006\n",
            "epoch 41 batch 102 loss 0.06312268227338791\n",
            "epoch 41 batch 103 loss 0.061633337289094925\n",
            "epoch 41 batch 104 loss 0.058868344873189926\n",
            "epoch 41 batch 105 loss 0.06337855011224747\n",
            "epoch 41 batch 106 loss 0.06302227824926376\n",
            "epoch 41 batch 107 loss 0.06392570585012436\n",
            "epoch 41 batch 108 loss 0.061380863189697266\n",
            "epoch 41 batch 109 loss 0.06175460293889046\n",
            "epoch 41 batch 110 loss 0.060486651957035065\n",
            "epoch 41 batch 111 loss 0.06326232850551605\n",
            "epoch 41 batch 112 loss 0.0714123323559761\n",
            "epoch 41 batch 113 loss 0.0665627047419548\n",
            "epoch 41 batch 114 loss 0.06231532245874405\n",
            "epoch 41 batch 115 loss 0.06850632280111313\n",
            "epoch 41 batch 116 loss 0.06739548593759537\n",
            "epoch 41 batch 117 loss 0.07110844552516937\n",
            "epoch 41 batch 118 loss 0.07024667412042618\n",
            "epoch 41 batch 119 loss 0.060666751116514206\n",
            "epoch 41 batch 120 loss 0.06072809174656868\n",
            "epoch 41 batch 121 loss 0.06441953033208847\n",
            "epoch 41 batch 122 loss 0.058779526501894\n",
            "epoch 41 batch 123 loss 0.06262405961751938\n",
            "epoch 41 batch 124 loss 0.061757177114486694\n",
            "epoch 41 batch 125 loss 0.06717151403427124\n",
            "epoch 41 batch 126 loss 0.060862038284540176\n",
            "epoch 41 batch 127 loss 0.058998189866542816\n",
            "epoch 41 batch 128 loss 0.0573013499379158\n",
            "epoch 41 batch 129 loss 0.06595462560653687\n",
            "epoch 41 batch 130 loss 0.06194505840539932\n",
            "epoch 41 batch 131 loss 0.06589515507221222\n",
            "epoch 41 batch 132 loss 0.06952624768018723\n",
            "epoch 41 batch 133 loss 0.06374257057905197\n",
            "epoch 41 batch 134 loss 0.06135082244873047\n",
            "epoch 41 batch 135 loss 0.07221010327339172\n",
            "epoch 41 batch 136 loss 0.057441189885139465\n",
            "epoch 41 batch 137 loss 0.06614226847887039\n",
            "epoch 41 batch 138 loss 0.06274572759866714\n",
            "epoch 41 batch 139 loss 0.06664466112852097\n",
            "epoch 41 batch 140 loss 0.06378910690546036\n",
            "epoch 41 batch 141 loss 0.057758186012506485\n",
            "epoch 41 batch 142 loss 0.05817190930247307\n",
            "epoch 41 batch 143 loss 0.06228667497634888\n",
            "epoch 41 batch 144 loss 0.06357231736183167\n",
            "epoch 41 batch 145 loss 0.05815854296088219\n",
            "epoch 41 batch 146 loss 0.06399908661842346\n",
            "epoch 41 batch 147 loss 0.06245812028646469\n",
            "epoch 41 batch 148 loss 0.06813642382621765\n",
            "epoch 41 batch 149 loss 0.0632038339972496\n",
            "epoch 41 batch 150 loss 0.06665293127298355\n",
            "epoch 41 batch 151 loss 0.06274210661649704\n",
            "epoch 41 batch 152 loss 0.061632443219423294\n",
            "epoch 41 batch 153 loss 0.06538555771112442\n",
            "epoch 41 batch 154 loss 0.05847911164164543\n",
            "epoch 41 batch 155 loss 0.06513015925884247\n",
            "epoch 41 batch 156 loss 0.06173212081193924\n",
            "epoch 41 batch 157 loss 0.06607507169246674\n",
            "epoch 41 batch 158 loss 0.065481036901474\n",
            "epoch 41 batch 159 loss 0.05953508988022804\n",
            "epoch 41 batch 160 loss 0.06302762776613235\n",
            "epoch 41 batch 161 loss 0.06247076019644737\n",
            "epoch 41 batch 162 loss 0.06126838177442551\n",
            "epoch 41 batch 163 loss 0.06734772026538849\n",
            "epoch 41 batch 164 loss 0.05995432659983635\n",
            "epoch 41 batch 165 loss 0.06128176301717758\n",
            "epoch 41 batch 166 loss 0.06839750707149506\n",
            "epoch 41 batch 167 loss 0.06436881422996521\n",
            "epoch 41 batch 168 loss 0.06213638186454773\n",
            "epoch 41 batch 169 loss 0.06395862251520157\n",
            "epoch 41 batch 170 loss 0.0647624135017395\n",
            "epoch 41 batch 171 loss 0.06064548343420029\n",
            "epoch 41 batch 172 loss 0.06142667308449745\n",
            "epoch 41 batch 173 loss 0.05477672070264816\n",
            "epoch 41 batch 174 loss 0.06483214348554611\n",
            "epoch 41 batch 175 loss 0.06389940530061722\n",
            "epoch 41 batch 176 loss 0.06184826046228409\n",
            "epoch 41 batch 177 loss 0.05652790144085884\n",
            "epoch 41 batch 178 loss 0.06032666563987732\n",
            "epoch 41 batch 179 loss 0.06004774570465088\n",
            "epoch 41 batch 180 loss 0.060064710676670074\n",
            "epoch 41 batch 181 loss 0.060078199952840805\n",
            "epoch 41 batch 182 loss 0.05441761016845703\n",
            "epoch 41 batch 183 loss 0.05923165753483772\n",
            "epoch 41 batch 184 loss 0.06596659868955612\n",
            "epoch 41 batch 185 loss 0.06454169005155563\n",
            "epoch 41 batch 186 loss 0.06325844675302505\n",
            "epoch 41 batch 187 loss 0.06464032083749771\n",
            "epoch 41 batch 188 loss 0.05863489955663681\n",
            "epoch 41 batch 189 loss 0.0672287717461586\n",
            "epoch 41 batch 190 loss 0.06112576648592949\n",
            "epoch 41 batch 191 loss 0.05963413417339325\n",
            "epoch 41 batch 192 loss 0.0639214739203453\n",
            "epoch 41 batch 193 loss 0.06366174668073654\n",
            "epoch 41 batch 194 loss 0.0682530477643013\n",
            "epoch 41 batch 195 loss 0.06397340446710587\n",
            "epoch 41 batch 196 loss 0.06477388739585876\n",
            "epoch 41 batch 197 loss 0.06114707142114639\n",
            "epoch 41 batch 198 loss 0.06411471962928772\n",
            "epoch 41 batch 199 loss 0.0559699647128582\n",
            "epoch 41 batch 200 loss 0.06638375669717789\n",
            "epoch 41 batch 201 loss 0.05694109573960304\n",
            "epoch 41 batch 202 loss 0.05890240520238876\n",
            "epoch 41 batch 203 loss 0.056283097714185715\n",
            "epoch 41 batch 204 loss 0.06707426905632019\n",
            "epoch 41 batch 205 loss 0.06427682191133499\n",
            "epoch 41 batch 206 loss 0.06198566406965256\n",
            "epoch 41 batch 207 loss 0.06773018091917038\n",
            "epoch 41 batch 208 loss 0.06217344105243683\n",
            "epoch 41 batch 209 loss 0.06793908774852753\n",
            "epoch 41 batch 210 loss 0.06562534719705582\n",
            "epoch 41 batch 211 loss 0.061008479446172714\n",
            "epoch 41 batch 212 loss 0.06139955669641495\n",
            "epoch 41 batch 213 loss 0.063624806702137\n",
            "epoch 41 batch 214 loss 0.06082604080438614\n",
            "epoch 41 batch 215 loss 0.06480859965085983\n",
            "epoch 41 batch 216 loss 0.06613707542419434\n",
            "epoch 41 batch 217 loss 0.06302007287740707\n",
            "epoch 41 batch 218 loss 0.0616571418941021\n",
            "epoch 41 batch 219 loss 0.06534864008426666\n",
            "epoch 41 batch 220 loss 0.059066303074359894\n",
            "epoch 41 batch 221 loss 0.062413621693849564\n",
            "epoch 41 batch 222 loss 0.0641791820526123\n",
            "epoch 41 batch 223 loss 0.056447334587574005\n",
            "epoch 41 batch 224 loss 0.06823596358299255\n",
            "epoch 41 batch 225 loss 0.05579720810055733\n",
            "epoch 41 batch 226 loss 0.061399225145578384\n",
            "epoch 41 batch 227 loss 0.057027410715818405\n",
            "epoch 41 batch 228 loss 0.0652059018611908\n",
            "epoch 41 batch 229 loss 0.06231359392404556\n",
            "epoch 41 batch 230 loss 0.06352955847978592\n",
            "epoch 42 batch 0 loss 0.05328323692083359\n",
            "epoch 42 batch 1 loss 0.05686432495713234\n",
            "epoch 42 batch 2 loss 0.05408719554543495\n",
            "epoch 42 batch 3 loss 0.055798906832933426\n",
            "epoch 42 batch 4 loss 0.05930302292108536\n",
            "epoch 42 batch 5 loss 0.057806968688964844\n",
            "epoch 42 batch 6 loss 0.05743197724223137\n",
            "epoch 42 batch 7 loss 0.05900336056947708\n",
            "epoch 42 batch 8 loss 0.06671196967363358\n",
            "epoch 42 batch 9 loss 0.05696733295917511\n",
            "epoch 42 batch 10 loss 0.056298740208148956\n",
            "epoch 42 batch 11 loss 0.0629609078168869\n",
            "epoch 42 batch 12 loss 0.05901366472244263\n",
            "epoch 42 batch 13 loss 0.05748536437749863\n",
            "epoch 42 batch 14 loss 0.06601586937904358\n",
            "epoch 42 batch 15 loss 0.06318207085132599\n",
            "epoch 42 batch 16 loss 0.06232462450861931\n",
            "epoch 42 batch 17 loss 0.06152580305933952\n",
            "epoch 42 batch 18 loss 0.05601697415113449\n",
            "epoch 42 batch 19 loss 0.06391841918230057\n",
            "epoch 42 batch 20 loss 0.07037442177534103\n",
            "epoch 42 batch 21 loss 0.06300230324268341\n",
            "epoch 42 batch 22 loss 0.05400209501385689\n",
            "epoch 42 batch 23 loss 0.06144610792398453\n",
            "epoch 42 batch 24 loss 0.06544892489910126\n",
            "epoch 42 batch 25 loss 0.05863216519355774\n",
            "epoch 42 batch 26 loss 0.06466883420944214\n",
            "epoch 42 batch 27 loss 0.058424923568964005\n",
            "epoch 42 batch 28 loss 0.054996851831674576\n",
            "epoch 42 batch 29 loss 0.062195852398872375\n",
            "epoch 42 batch 30 loss 0.05955640226602554\n",
            "epoch 42 batch 31 loss 0.06068206951022148\n",
            "epoch 42 batch 32 loss 0.06030495837330818\n",
            "epoch 42 batch 33 loss 0.06292517483234406\n",
            "epoch 42 batch 34 loss 0.06037630885839462\n",
            "epoch 42 batch 35 loss 0.06236380711197853\n",
            "epoch 42 batch 36 loss 0.0636756494641304\n",
            "epoch 42 batch 37 loss 0.056545231491327286\n",
            "epoch 42 batch 38 loss 0.05973470211029053\n",
            "epoch 42 batch 39 loss 0.054948948323726654\n",
            "epoch 42 batch 40 loss 0.06283023208379745\n",
            "epoch 42 batch 41 loss 0.061176981776952744\n",
            "epoch 42 batch 42 loss 0.06472238898277283\n",
            "epoch 42 batch 43 loss 0.056157030165195465\n",
            "epoch 42 batch 44 loss 0.05924845486879349\n",
            "epoch 42 batch 45 loss 0.06334619224071503\n",
            "epoch 42 batch 46 loss 0.06448177993297577\n",
            "epoch 42 batch 47 loss 0.06330635398626328\n",
            "epoch 42 batch 48 loss 0.061177000403404236\n",
            "epoch 42 batch 49 loss 0.06333361566066742\n",
            "epoch 42 batch 50 loss 0.06205035001039505\n",
            "epoch 42 batch 51 loss 0.06452780216932297\n",
            "epoch 42 batch 52 loss 0.06306283921003342\n",
            "epoch 42 batch 53 loss 0.05560564622282982\n",
            "epoch 42 batch 54 loss 0.061703041195869446\n",
            "epoch 42 batch 55 loss 0.05499877780675888\n",
            "epoch 42 batch 56 loss 0.0663933977484703\n",
            "epoch 42 batch 57 loss 0.05816780775785446\n",
            "epoch 42 batch 58 loss 0.05417024716734886\n",
            "epoch 42 batch 59 loss 0.06318359822034836\n",
            "epoch 42 batch 60 loss 0.0633089542388916\n",
            "epoch 42 batch 61 loss 0.05806663632392883\n",
            "epoch 42 batch 62 loss 0.060280054807662964\n",
            "epoch 42 batch 63 loss 0.06148909777402878\n",
            "epoch 42 batch 64 loss 0.0679546520113945\n",
            "epoch 42 batch 65 loss 0.057963013648986816\n",
            "epoch 42 batch 66 loss 0.0647662952542305\n",
            "epoch 42 batch 67 loss 0.06350107491016388\n",
            "epoch 42 batch 68 loss 0.05551506206393242\n",
            "epoch 42 batch 69 loss 0.06460830569267273\n",
            "epoch 42 batch 70 loss 0.05938396975398064\n",
            "epoch 42 batch 71 loss 0.060708243399858475\n",
            "epoch 42 batch 72 loss 0.05769198015332222\n",
            "epoch 42 batch 73 loss 0.06347829103469849\n",
            "epoch 42 batch 74 loss 0.051900193095207214\n",
            "epoch 42 batch 75 loss 0.06429717689752579\n",
            "epoch 42 batch 76 loss 0.06234113872051239\n",
            "epoch 42 batch 77 loss 0.06801819801330566\n",
            "epoch 42 batch 78 loss 0.06396989524364471\n",
            "epoch 42 batch 79 loss 0.059494610875844955\n",
            "epoch 42 batch 80 loss 0.06170596554875374\n",
            "epoch 42 batch 81 loss 0.05763261765241623\n",
            "epoch 42 batch 82 loss 0.06200040131807327\n",
            "epoch 42 batch 83 loss 0.058895353227853775\n",
            "epoch 42 batch 84 loss 0.06432158499956131\n",
            "epoch 42 batch 85 loss 0.05705346539616585\n",
            "epoch 42 batch 86 loss 0.06000291183590889\n",
            "epoch 42 batch 87 loss 0.06028199568390846\n",
            "epoch 42 batch 88 loss 0.06245160475373268\n",
            "epoch 42 batch 89 loss 0.058909885585308075\n",
            "epoch 42 batch 90 loss 0.06530594080686569\n",
            "epoch 42 batch 91 loss 0.06406699120998383\n",
            "epoch 42 batch 92 loss 0.05977119505405426\n",
            "epoch 42 batch 93 loss 0.06533241271972656\n",
            "epoch 42 batch 94 loss 0.06150917708873749\n",
            "epoch 42 batch 95 loss 0.05942092835903168\n",
            "epoch 42 batch 96 loss 0.06460341066122055\n",
            "epoch 42 batch 97 loss 0.05904504656791687\n",
            "epoch 42 batch 98 loss 0.06550343334674835\n",
            "epoch 42 batch 99 loss 0.06349590420722961\n",
            "epoch 42 batch 100 loss 0.05946758762001991\n",
            "epoch 42 batch 101 loss 0.05844324827194214\n",
            "epoch 42 batch 102 loss 0.061141565442085266\n",
            "epoch 42 batch 103 loss 0.0645723044872284\n",
            "epoch 42 batch 104 loss 0.061180707067251205\n",
            "epoch 42 batch 105 loss 0.05675015598535538\n",
            "epoch 42 batch 106 loss 0.0639818087220192\n",
            "epoch 42 batch 107 loss 0.06658443808555603\n",
            "epoch 42 batch 108 loss 0.06004802882671356\n",
            "epoch 42 batch 109 loss 0.06325100362300873\n",
            "epoch 42 batch 110 loss 0.06022287532687187\n",
            "epoch 42 batch 111 loss 0.06470981240272522\n",
            "epoch 42 batch 112 loss 0.06318855285644531\n",
            "epoch 42 batch 113 loss 0.06294417381286621\n",
            "epoch 42 batch 114 loss 0.06645284593105316\n",
            "epoch 42 batch 115 loss 0.0610702820122242\n",
            "epoch 42 batch 116 loss 0.05755017697811127\n",
            "epoch 42 batch 117 loss 0.05929746478796005\n",
            "epoch 42 batch 118 loss 0.06473519653081894\n",
            "epoch 42 batch 119 loss 0.060122959315776825\n",
            "epoch 42 batch 120 loss 0.05934194102883339\n",
            "epoch 42 batch 121 loss 0.06551925837993622\n",
            "epoch 42 batch 122 loss 0.055598899722099304\n",
            "epoch 42 batch 123 loss 0.06439085304737091\n",
            "epoch 42 batch 124 loss 0.05729633942246437\n",
            "epoch 42 batch 125 loss 0.06343519687652588\n",
            "epoch 42 batch 126 loss 0.06448248773813248\n",
            "epoch 42 batch 127 loss 0.07012965530157089\n",
            "epoch 42 batch 128 loss 0.05561523512005806\n",
            "epoch 42 batch 129 loss 0.05873647704720497\n",
            "epoch 42 batch 130 loss 0.06400227546691895\n",
            "epoch 42 batch 131 loss 0.053389836102724075\n",
            "epoch 42 batch 132 loss 0.059561893343925476\n",
            "epoch 42 batch 133 loss 0.0597737692296505\n",
            "epoch 42 batch 134 loss 0.05877959728240967\n",
            "epoch 42 batch 135 loss 0.06329613924026489\n",
            "epoch 42 batch 136 loss 0.0614704005420208\n",
            "epoch 42 batch 137 loss 0.0619797520339489\n",
            "epoch 42 batch 138 loss 0.05977268144488335\n",
            "epoch 42 batch 139 loss 0.06708043068647385\n",
            "epoch 42 batch 140 loss 0.0664321631193161\n",
            "epoch 42 batch 141 loss 0.06450817734003067\n",
            "epoch 42 batch 142 loss 0.05674615874886513\n",
            "epoch 42 batch 143 loss 0.057695064693689346\n",
            "epoch 42 batch 144 loss 0.0634099468588829\n",
            "epoch 42 batch 145 loss 0.06522464752197266\n",
            "epoch 42 batch 146 loss 0.0605543851852417\n",
            "epoch 42 batch 147 loss 0.06366220116615295\n",
            "epoch 42 batch 148 loss 0.05957576259970665\n",
            "epoch 42 batch 149 loss 0.0654691755771637\n",
            "epoch 42 batch 150 loss 0.06365758180618286\n",
            "epoch 42 batch 151 loss 0.06405936926603317\n",
            "epoch 42 batch 152 loss 0.06457371264696121\n",
            "epoch 42 batch 153 loss 0.06249446049332619\n",
            "epoch 42 batch 154 loss 0.06382082402706146\n",
            "epoch 42 batch 155 loss 0.05870695039629936\n",
            "epoch 42 batch 156 loss 0.05650687590241432\n",
            "epoch 42 batch 157 loss 0.06391575187444687\n",
            "epoch 42 batch 158 loss 0.05938759446144104\n",
            "epoch 42 batch 159 loss 0.0620749406516552\n",
            "epoch 42 batch 160 loss 0.06029309704899788\n",
            "epoch 42 batch 161 loss 0.06664261221885681\n",
            "epoch 42 batch 162 loss 0.066789910197258\n",
            "epoch 42 batch 163 loss 0.06357715278863907\n",
            "epoch 42 batch 164 loss 0.06679300218820572\n",
            "epoch 42 batch 165 loss 0.06030599772930145\n",
            "epoch 42 batch 166 loss 0.058223724365234375\n",
            "epoch 42 batch 167 loss 0.058270663022994995\n",
            "epoch 42 batch 168 loss 0.06378305703401566\n",
            "epoch 42 batch 169 loss 0.05354222655296326\n",
            "epoch 42 batch 170 loss 0.0633460059762001\n",
            "epoch 42 batch 171 loss 0.06732101738452911\n",
            "epoch 42 batch 172 loss 0.057029902935028076\n",
            "epoch 42 batch 173 loss 0.05931885540485382\n",
            "epoch 42 batch 174 loss 0.06475450843572617\n",
            "epoch 42 batch 175 loss 0.06009120121598244\n",
            "epoch 42 batch 176 loss 0.06438253819942474\n",
            "epoch 42 batch 177 loss 0.06563914567232132\n",
            "epoch 42 batch 178 loss 0.06208474561572075\n",
            "epoch 42 batch 179 loss 0.06497511267662048\n",
            "epoch 42 batch 180 loss 0.0645359680056572\n",
            "epoch 42 batch 181 loss 0.054099153727293015\n",
            "epoch 42 batch 182 loss 0.05621540546417236\n",
            "epoch 42 batch 183 loss 0.06437309086322784\n",
            "epoch 42 batch 184 loss 0.05918049439787865\n",
            "epoch 42 batch 185 loss 0.06524336338043213\n",
            "epoch 42 batch 186 loss 0.06497886031866074\n",
            "epoch 42 batch 187 loss 0.059994887560606\n",
            "epoch 42 batch 188 loss 0.06355306506156921\n",
            "epoch 42 batch 189 loss 0.05707038938999176\n",
            "epoch 42 batch 190 loss 0.06437301635742188\n",
            "epoch 42 batch 191 loss 0.06324301660060883\n",
            "epoch 42 batch 192 loss 0.061312224715948105\n",
            "epoch 42 batch 193 loss 0.06836342066526413\n",
            "epoch 42 batch 194 loss 0.06973069906234741\n",
            "epoch 42 batch 195 loss 0.06346188485622406\n",
            "epoch 42 batch 196 loss 0.06745787709951401\n",
            "epoch 42 batch 197 loss 0.06384392082691193\n",
            "epoch 42 batch 198 loss 0.05838057026267052\n",
            "epoch 42 batch 199 loss 0.06557396054267883\n",
            "epoch 42 batch 200 loss 0.0635552927851677\n",
            "epoch 42 batch 201 loss 0.06311619281768799\n",
            "epoch 42 batch 202 loss 0.06370002776384354\n",
            "epoch 42 batch 203 loss 0.058226604014635086\n",
            "epoch 42 batch 204 loss 0.06742427498102188\n",
            "epoch 42 batch 205 loss 0.06567680090665817\n",
            "epoch 42 batch 206 loss 0.06667204201221466\n",
            "epoch 42 batch 207 loss 0.05434035509824753\n",
            "epoch 42 batch 208 loss 0.05893599987030029\n",
            "epoch 42 batch 209 loss 0.061665717512369156\n",
            "epoch 42 batch 210 loss 0.06057610735297203\n",
            "epoch 42 batch 211 loss 0.05336413532495499\n",
            "epoch 42 batch 212 loss 0.06005389988422394\n",
            "epoch 42 batch 213 loss 0.06632894277572632\n",
            "epoch 42 batch 214 loss 0.05826771259307861\n",
            "epoch 42 batch 215 loss 0.0681268721818924\n",
            "epoch 42 batch 216 loss 0.06365223228931427\n",
            "epoch 42 batch 217 loss 0.0579490028321743\n",
            "epoch 42 batch 218 loss 0.05630893632769585\n",
            "epoch 42 batch 219 loss 0.06052738055586815\n",
            "epoch 42 batch 220 loss 0.06694426387548447\n",
            "epoch 42 batch 221 loss 0.06239428371191025\n",
            "epoch 42 batch 222 loss 0.06207144260406494\n",
            "epoch 42 batch 223 loss 0.06405141204595566\n",
            "epoch 42 batch 224 loss 0.06464923918247223\n",
            "epoch 42 batch 225 loss 0.06656085699796677\n",
            "epoch 42 batch 226 loss 0.0625743418931961\n",
            "epoch 42 batch 227 loss 0.0612892284989357\n",
            "epoch 42 batch 228 loss 0.062348373234272\n",
            "epoch 42 batch 229 loss 0.05781279876828194\n",
            "epoch 42 batch 230 loss 0.05954236164689064\n",
            "epoch 43 batch 0 loss 0.05701257660984993\n",
            "epoch 43 batch 1 loss 0.05783618241548538\n",
            "epoch 43 batch 2 loss 0.06259317696094513\n",
            "epoch 43 batch 3 loss 0.05115414410829544\n",
            "epoch 43 batch 4 loss 0.06588465720415115\n",
            "epoch 43 batch 5 loss 0.0603366419672966\n",
            "epoch 43 batch 6 loss 0.0617801807820797\n",
            "epoch 43 batch 7 loss 0.058161571621894836\n",
            "epoch 43 batch 8 loss 0.05942076817154884\n",
            "epoch 43 batch 9 loss 0.06008003279566765\n",
            "epoch 43 batch 10 loss 0.059715576469898224\n",
            "epoch 43 batch 11 loss 0.06769939512014389\n",
            "epoch 43 batch 12 loss 0.06211661174893379\n",
            "epoch 43 batch 13 loss 0.05447520688176155\n",
            "epoch 43 batch 14 loss 0.060524832457304\n",
            "epoch 43 batch 15 loss 0.05659332871437073\n",
            "epoch 43 batch 16 loss 0.06788951903581619\n",
            "epoch 43 batch 17 loss 0.061232730746269226\n",
            "epoch 43 batch 18 loss 0.0615982748568058\n",
            "epoch 43 batch 19 loss 0.05850239470601082\n",
            "epoch 43 batch 20 loss 0.059057243168354034\n",
            "epoch 43 batch 21 loss 0.06543071568012238\n",
            "epoch 43 batch 22 loss 0.05942639708518982\n",
            "epoch 43 batch 23 loss 0.06340064108371735\n",
            "epoch 43 batch 24 loss 0.063271664083004\n",
            "epoch 43 batch 25 loss 0.05586633086204529\n",
            "epoch 43 batch 26 loss 0.0509517639875412\n",
            "epoch 43 batch 27 loss 0.061949264258146286\n",
            "epoch 43 batch 28 loss 0.052502088248729706\n",
            "epoch 43 batch 29 loss 0.06583420187234879\n",
            "epoch 43 batch 30 loss 0.05709837004542351\n",
            "epoch 43 batch 31 loss 0.065134696662426\n",
            "epoch 43 batch 32 loss 0.06452490389347076\n",
            "epoch 43 batch 33 loss 0.06288138777017593\n",
            "epoch 43 batch 34 loss 0.06065625324845314\n",
            "epoch 43 batch 35 loss 0.06215284392237663\n",
            "epoch 43 batch 36 loss 0.06050487980246544\n",
            "epoch 43 batch 37 loss 0.058009229600429535\n",
            "epoch 43 batch 38 loss 0.06259530782699585\n",
            "epoch 43 batch 39 loss 0.05846752971410751\n",
            "epoch 43 batch 40 loss 0.058994438499212265\n",
            "epoch 43 batch 41 loss 0.05956733599305153\n",
            "epoch 43 batch 42 loss 0.05701221525669098\n",
            "epoch 43 batch 43 loss 0.056080952286720276\n",
            "epoch 43 batch 44 loss 0.05730940401554108\n",
            "epoch 43 batch 45 loss 0.061790499836206436\n",
            "epoch 43 batch 46 loss 0.05474073439836502\n",
            "epoch 43 batch 47 loss 0.05814727768301964\n",
            "epoch 43 batch 48 loss 0.061649203300476074\n",
            "epoch 43 batch 49 loss 0.06595826148986816\n",
            "epoch 43 batch 50 loss 0.059264954179525375\n",
            "epoch 43 batch 51 loss 0.05971432849764824\n",
            "epoch 43 batch 52 loss 0.05914290249347687\n",
            "epoch 43 batch 53 loss 0.06635194271802902\n",
            "epoch 43 batch 54 loss 0.05564753711223602\n",
            "epoch 43 batch 55 loss 0.06059511750936508\n",
            "epoch 43 batch 56 loss 0.06570038944482803\n",
            "epoch 43 batch 57 loss 0.057052142918109894\n",
            "epoch 43 batch 58 loss 0.06020297855138779\n",
            "epoch 43 batch 59 loss 0.05857851728796959\n",
            "epoch 43 batch 60 loss 0.060821108520030975\n",
            "epoch 43 batch 61 loss 0.0558779202401638\n",
            "epoch 43 batch 62 loss 0.055554792284965515\n",
            "epoch 43 batch 63 loss 0.06202244386076927\n",
            "epoch 43 batch 64 loss 0.06670205295085907\n",
            "epoch 43 batch 65 loss 0.06071729585528374\n",
            "epoch 43 batch 66 loss 0.056976109743118286\n",
            "epoch 43 batch 67 loss 0.0659864991903305\n",
            "epoch 43 batch 68 loss 0.06208597868680954\n",
            "epoch 43 batch 69 loss 0.06269337236881256\n",
            "epoch 43 batch 70 loss 0.05731666460633278\n",
            "epoch 43 batch 71 loss 0.05916396528482437\n",
            "epoch 43 batch 72 loss 0.06343589723110199\n",
            "epoch 43 batch 73 loss 0.05709695816040039\n",
            "epoch 43 batch 74 loss 0.056921493262052536\n",
            "epoch 43 batch 75 loss 0.059939779341220856\n",
            "epoch 43 batch 76 loss 0.06093301251530647\n",
            "epoch 43 batch 77 loss 0.06389360129833221\n",
            "epoch 43 batch 78 loss 0.060138627886772156\n",
            "epoch 43 batch 79 loss 0.05490272492170334\n",
            "epoch 43 batch 80 loss 0.05945872142910957\n",
            "epoch 43 batch 81 loss 0.06491077691316605\n",
            "epoch 43 batch 82 loss 0.059548020362854004\n",
            "epoch 43 batch 83 loss 0.058661166578531265\n",
            "epoch 43 batch 84 loss 0.07225728034973145\n",
            "epoch 43 batch 85 loss 0.06244726851582527\n",
            "epoch 43 batch 86 loss 0.05727103725075722\n",
            "epoch 43 batch 87 loss 0.05766043812036514\n",
            "epoch 43 batch 88 loss 0.06181035190820694\n",
            "epoch 43 batch 89 loss 0.06244402006268501\n",
            "epoch 43 batch 90 loss 0.0645049437880516\n",
            "epoch 43 batch 91 loss 0.05897694453597069\n",
            "epoch 43 batch 92 loss 0.060158621519804\n",
            "epoch 43 batch 93 loss 0.061127204447984695\n",
            "epoch 43 batch 94 loss 0.06452281773090363\n",
            "epoch 43 batch 95 loss 0.06752917170524597\n",
            "epoch 43 batch 96 loss 0.0641484409570694\n",
            "epoch 43 batch 97 loss 0.054973114281892776\n",
            "epoch 43 batch 98 loss 0.06660234183073044\n",
            "epoch 43 batch 99 loss 0.05745651200413704\n",
            "epoch 43 batch 100 loss 0.05972638353705406\n",
            "epoch 43 batch 101 loss 0.05933345854282379\n",
            "epoch 43 batch 102 loss 0.059542104601860046\n",
            "epoch 43 batch 103 loss 0.05537448078393936\n",
            "epoch 43 batch 104 loss 0.06013014540076256\n",
            "epoch 43 batch 105 loss 0.0596332810819149\n",
            "epoch 43 batch 106 loss 0.05960844084620476\n",
            "epoch 43 batch 107 loss 0.06244644895195961\n",
            "epoch 43 batch 108 loss 0.058036401867866516\n",
            "epoch 43 batch 109 loss 0.06487653404474258\n",
            "epoch 43 batch 110 loss 0.0611104890704155\n",
            "epoch 43 batch 111 loss 0.0612010695040226\n",
            "epoch 43 batch 112 loss 0.06278481334447861\n",
            "epoch 43 batch 113 loss 0.06186065077781677\n",
            "epoch 43 batch 114 loss 0.0682004764676094\n",
            "epoch 43 batch 115 loss 0.06314145028591156\n",
            "epoch 43 batch 116 loss 0.06231989711523056\n",
            "epoch 43 batch 117 loss 0.061744995415210724\n",
            "epoch 43 batch 118 loss 0.06333757936954498\n",
            "epoch 43 batch 119 loss 0.061414629220962524\n",
            "epoch 43 batch 120 loss 0.05947796627879143\n",
            "epoch 43 batch 121 loss 0.05887354537844658\n",
            "epoch 43 batch 122 loss 0.05697685852646828\n",
            "epoch 43 batch 123 loss 0.06212165206670761\n",
            "epoch 43 batch 124 loss 0.06847487390041351\n",
            "epoch 43 batch 125 loss 0.060940708965063095\n",
            "epoch 43 batch 126 loss 0.0645860955119133\n",
            "epoch 43 batch 127 loss 0.06019403412938118\n",
            "epoch 43 batch 128 loss 0.06152419373393059\n",
            "epoch 43 batch 129 loss 0.06864198297262192\n",
            "epoch 43 batch 130 loss 0.05501643568277359\n",
            "epoch 43 batch 131 loss 0.06233615800738335\n",
            "epoch 43 batch 132 loss 0.06076757609844208\n",
            "epoch 43 batch 133 loss 0.06097602844238281\n",
            "epoch 43 batch 134 loss 0.06396711617708206\n",
            "epoch 43 batch 135 loss 0.06333578377962112\n",
            "epoch 43 batch 136 loss 0.06358161568641663\n",
            "epoch 43 batch 137 loss 0.062344811856746674\n",
            "epoch 43 batch 138 loss 0.06121569499373436\n",
            "epoch 43 batch 139 loss 0.05567881092429161\n",
            "epoch 43 batch 140 loss 0.06867239624261856\n",
            "epoch 43 batch 141 loss 0.05672449246048927\n",
            "epoch 43 batch 142 loss 0.06558211147785187\n",
            "epoch 43 batch 143 loss 0.06228947266936302\n",
            "epoch 43 batch 144 loss 0.06698836386203766\n",
            "epoch 43 batch 145 loss 0.06102029234170914\n",
            "epoch 43 batch 146 loss 0.061674755066633224\n",
            "epoch 43 batch 147 loss 0.0612410344183445\n",
            "epoch 43 batch 148 loss 0.06825853884220123\n",
            "epoch 43 batch 149 loss 0.06570038199424744\n",
            "epoch 43 batch 150 loss 0.05983059108257294\n",
            "epoch 43 batch 151 loss 0.058916058391332626\n",
            "epoch 43 batch 152 loss 0.06338997930288315\n",
            "epoch 43 batch 153 loss 0.062088578939437866\n",
            "epoch 43 batch 154 loss 0.06763047724962234\n",
            "epoch 43 batch 155 loss 0.058416422456502914\n",
            "epoch 43 batch 156 loss 0.058795370161533356\n",
            "epoch 43 batch 157 loss 0.06989292800426483\n",
            "epoch 43 batch 158 loss 0.06107116863131523\n",
            "epoch 43 batch 159 loss 0.06488452106714249\n",
            "epoch 43 batch 160 loss 0.06296300888061523\n",
            "epoch 43 batch 161 loss 0.06012086197733879\n",
            "epoch 43 batch 162 loss 0.06734120845794678\n",
            "epoch 43 batch 163 loss 0.06974758207798004\n",
            "epoch 43 batch 164 loss 0.05538720265030861\n",
            "epoch 43 batch 165 loss 0.06214527040719986\n",
            "epoch 43 batch 166 loss 0.06489963829517365\n",
            "epoch 43 batch 167 loss 0.058801498264074326\n",
            "epoch 43 batch 168 loss 0.05872831866145134\n",
            "epoch 43 batch 169 loss 0.061533208936452866\n",
            "epoch 43 batch 170 loss 0.05768430233001709\n",
            "epoch 43 batch 171 loss 0.06553478538990021\n",
            "epoch 43 batch 172 loss 0.06037735566496849\n",
            "epoch 43 batch 173 loss 0.06268403679132462\n",
            "epoch 43 batch 174 loss 0.0588863231241703\n",
            "epoch 43 batch 175 loss 0.057299576699733734\n",
            "epoch 43 batch 176 loss 0.061504874378442764\n",
            "epoch 43 batch 177 loss 0.06895652413368225\n",
            "epoch 43 batch 178 loss 0.0609772615134716\n",
            "epoch 43 batch 179 loss 0.0630393922328949\n",
            "epoch 43 batch 180 loss 0.06366206705570221\n",
            "epoch 43 batch 181 loss 0.05955963209271431\n",
            "epoch 43 batch 182 loss 0.06250355392694473\n",
            "epoch 43 batch 183 loss 0.06463861465454102\n",
            "epoch 43 batch 184 loss 0.057632070034742355\n",
            "epoch 43 batch 185 loss 0.06131980940699577\n",
            "epoch 43 batch 186 loss 0.06532961875200272\n",
            "epoch 43 batch 187 loss 0.06265880167484283\n",
            "epoch 43 batch 188 loss 0.05888485535979271\n",
            "epoch 43 batch 189 loss 0.06256385147571564\n",
            "epoch 43 batch 190 loss 0.060681287199258804\n",
            "epoch 43 batch 191 loss 0.05801964923739433\n",
            "epoch 43 batch 192 loss 0.05855946242809296\n",
            "epoch 43 batch 193 loss 0.06205520033836365\n",
            "epoch 43 batch 194 loss 0.061334509402513504\n",
            "epoch 43 batch 195 loss 0.060334522277116776\n",
            "epoch 43 batch 196 loss 0.057257309556007385\n",
            "epoch 43 batch 197 loss 0.06012657657265663\n",
            "epoch 43 batch 198 loss 0.06533889472484589\n",
            "epoch 43 batch 199 loss 0.06206965073943138\n",
            "epoch 43 batch 200 loss 0.06373505294322968\n",
            "epoch 43 batch 201 loss 0.06567833572626114\n",
            "epoch 43 batch 202 loss 0.06374640017747879\n",
            "epoch 43 batch 203 loss 0.05902088060975075\n",
            "epoch 43 batch 204 loss 0.06424031406641006\n",
            "epoch 43 batch 205 loss 0.05686786770820618\n",
            "epoch 43 batch 206 loss 0.06047523021697998\n",
            "epoch 43 batch 207 loss 0.06641887128353119\n",
            "epoch 43 batch 208 loss 0.0595053993165493\n",
            "epoch 43 batch 209 loss 0.0635247528553009\n",
            "epoch 43 batch 210 loss 0.06155763566493988\n",
            "epoch 43 batch 211 loss 0.06421607732772827\n",
            "epoch 43 batch 212 loss 0.058504149317741394\n",
            "epoch 43 batch 213 loss 0.06079636886715889\n",
            "epoch 43 batch 214 loss 0.05955537036061287\n",
            "epoch 43 batch 215 loss 0.06055351719260216\n",
            "epoch 43 batch 216 loss 0.06591656059026718\n",
            "epoch 43 batch 217 loss 0.06069685518741608\n",
            "epoch 43 batch 218 loss 0.06868545711040497\n",
            "epoch 43 batch 219 loss 0.06494472175836563\n",
            "epoch 43 batch 220 loss 0.069157175719738\n",
            "epoch 43 batch 221 loss 0.0614246241748333\n",
            "epoch 43 batch 222 loss 0.06447315961122513\n",
            "epoch 43 batch 223 loss 0.06904604285955429\n",
            "epoch 43 batch 224 loss 0.06018414720892906\n",
            "epoch 43 batch 225 loss 0.061108119785785675\n",
            "epoch 43 batch 226 loss 0.06473644822835922\n",
            "epoch 43 batch 227 loss 0.05687221512198448\n",
            "epoch 43 batch 228 loss 0.06112169101834297\n",
            "epoch 43 batch 229 loss 0.05930924043059349\n",
            "epoch 43 batch 230 loss 0.06887556612491608\n",
            "epoch 44 batch 0 loss 0.05987163633108139\n",
            "epoch 44 batch 1 loss 0.06383834779262543\n",
            "epoch 44 batch 2 loss 0.06467875838279724\n",
            "epoch 44 batch 3 loss 0.06308519840240479\n",
            "epoch 44 batch 4 loss 0.059977684170007706\n",
            "epoch 44 batch 5 loss 0.05961863324046135\n",
            "epoch 44 batch 6 loss 0.05982358381152153\n",
            "epoch 44 batch 7 loss 0.06283385306596756\n",
            "epoch 44 batch 8 loss 0.0549578033387661\n",
            "epoch 44 batch 9 loss 0.06091858074069023\n",
            "epoch 44 batch 10 loss 0.06096062436699867\n",
            "epoch 44 batch 11 loss 0.059494126588106155\n",
            "epoch 44 batch 12 loss 0.05979309231042862\n",
            "epoch 44 batch 13 loss 0.062214337289333344\n",
            "epoch 44 batch 14 loss 0.062004927545785904\n",
            "epoch 44 batch 15 loss 0.057611409574747086\n",
            "epoch 44 batch 16 loss 0.05789734050631523\n",
            "epoch 44 batch 17 loss 0.06735853105783463\n",
            "epoch 44 batch 18 loss 0.057212550193071365\n",
            "epoch 44 batch 19 loss 0.06492355465888977\n",
            "epoch 44 batch 20 loss 0.053415633738040924\n",
            "epoch 44 batch 21 loss 0.05350436642765999\n",
            "epoch 44 batch 22 loss 0.06208314374089241\n",
            "epoch 44 batch 23 loss 0.05703578144311905\n",
            "epoch 44 batch 24 loss 0.06080958619713783\n",
            "epoch 44 batch 25 loss 0.05738651379942894\n",
            "epoch 44 batch 26 loss 0.06306461244821548\n",
            "epoch 44 batch 27 loss 0.05967284366488457\n",
            "epoch 44 batch 28 loss 0.05672301724553108\n",
            "epoch 44 batch 29 loss 0.056941624730825424\n",
            "epoch 44 batch 30 loss 0.057518862187862396\n",
            "epoch 44 batch 31 loss 0.0649350956082344\n",
            "epoch 44 batch 32 loss 0.05691339448094368\n",
            "epoch 44 batch 33 loss 0.05791405960917473\n",
            "epoch 44 batch 34 loss 0.06535537540912628\n",
            "epoch 44 batch 35 loss 0.05907503888010979\n",
            "epoch 44 batch 36 loss 0.05694014951586723\n",
            "epoch 44 batch 37 loss 0.06332115828990936\n",
            "epoch 44 batch 38 loss 0.05911041796207428\n",
            "epoch 44 batch 39 loss 0.058332327753305435\n",
            "epoch 44 batch 40 loss 0.05877046659588814\n",
            "epoch 44 batch 41 loss 0.05569225922226906\n",
            "epoch 44 batch 42 loss 0.059479884803295135\n",
            "epoch 44 batch 43 loss 0.06205659732222557\n",
            "epoch 44 batch 44 loss 0.06404731422662735\n",
            "epoch 44 batch 45 loss 0.06041329726576805\n",
            "epoch 44 batch 46 loss 0.06602725386619568\n",
            "epoch 44 batch 47 loss 0.057347286492586136\n",
            "epoch 44 batch 48 loss 0.05431767925620079\n",
            "epoch 44 batch 49 loss 0.060347430408000946\n",
            "epoch 44 batch 50 loss 0.06315919011831284\n",
            "epoch 44 batch 51 loss 0.05815252289175987\n",
            "epoch 44 batch 52 loss 0.06260024011135101\n",
            "epoch 44 batch 53 loss 0.06216950714588165\n",
            "epoch 44 batch 54 loss 0.06314875930547714\n",
            "epoch 44 batch 55 loss 0.0674644187092781\n",
            "epoch 44 batch 56 loss 0.06533993035554886\n",
            "epoch 44 batch 57 loss 0.0609709657728672\n",
            "epoch 44 batch 58 loss 0.062413159757852554\n",
            "epoch 44 batch 59 loss 0.05927269533276558\n",
            "epoch 44 batch 60 loss 0.06416574120521545\n",
            "epoch 44 batch 61 loss 0.05989719554781914\n",
            "epoch 44 batch 62 loss 0.057207267731428146\n",
            "epoch 44 batch 63 loss 0.06149335205554962\n",
            "epoch 44 batch 64 loss 0.05628955364227295\n",
            "epoch 44 batch 65 loss 0.0582149364054203\n",
            "epoch 44 batch 66 loss 0.061793360859155655\n",
            "epoch 44 batch 67 loss 0.06373251229524612\n",
            "epoch 44 batch 68 loss 0.05853055790066719\n",
            "epoch 44 batch 69 loss 0.0626211166381836\n",
            "epoch 44 batch 70 loss 0.06341056525707245\n",
            "epoch 44 batch 71 loss 0.05979600176215172\n",
            "epoch 44 batch 72 loss 0.062251701951026917\n",
            "epoch 44 batch 73 loss 0.057704754173755646\n",
            "epoch 44 batch 74 loss 0.05733051151037216\n",
            "epoch 44 batch 75 loss 0.059867437928915024\n",
            "epoch 44 batch 76 loss 0.05992875248193741\n",
            "epoch 44 batch 77 loss 0.06263261288404465\n",
            "epoch 44 batch 78 loss 0.05925517529249191\n",
            "epoch 44 batch 79 loss 0.0564432218670845\n",
            "epoch 44 batch 80 loss 0.05689290910959244\n",
            "epoch 44 batch 81 loss 0.05766269564628601\n",
            "epoch 44 batch 82 loss 0.06260380148887634\n",
            "epoch 44 batch 83 loss 0.06828860193490982\n",
            "epoch 44 batch 84 loss 0.06110531464219093\n",
            "epoch 44 batch 85 loss 0.062353771179914474\n",
            "epoch 44 batch 86 loss 0.05674273893237114\n",
            "epoch 44 batch 87 loss 0.06341000646352768\n",
            "epoch 44 batch 88 loss 0.05771534517407417\n",
            "epoch 44 batch 89 loss 0.0583774633705616\n",
            "epoch 44 batch 90 loss 0.061180997639894485\n",
            "epoch 44 batch 91 loss 0.06494805961847305\n",
            "epoch 44 batch 92 loss 0.06284905225038528\n",
            "epoch 44 batch 93 loss 0.0660240575671196\n",
            "epoch 44 batch 94 loss 0.05804773420095444\n",
            "epoch 44 batch 95 loss 0.05805756151676178\n",
            "epoch 44 batch 96 loss 0.06707826256752014\n",
            "epoch 44 batch 97 loss 0.05394108593463898\n",
            "epoch 44 batch 98 loss 0.06019190326333046\n",
            "epoch 44 batch 99 loss 0.05889331176877022\n",
            "epoch 44 batch 100 loss 0.06121937930583954\n",
            "epoch 44 batch 101 loss 0.0622233971953392\n",
            "epoch 44 batch 102 loss 0.06014595553278923\n",
            "epoch 44 batch 103 loss 0.055580392479896545\n",
            "epoch 44 batch 104 loss 0.05888907238841057\n",
            "epoch 44 batch 105 loss 0.061974912881851196\n",
            "epoch 44 batch 106 loss 0.0686328262090683\n",
            "epoch 44 batch 107 loss 0.06309832632541656\n",
            "epoch 44 batch 108 loss 0.06409629434347153\n",
            "epoch 44 batch 109 loss 0.060055509209632874\n",
            "epoch 44 batch 110 loss 0.06698029488325119\n",
            "epoch 44 batch 111 loss 0.060036785900592804\n",
            "epoch 44 batch 112 loss 0.05896688997745514\n",
            "epoch 44 batch 113 loss 0.0576358288526535\n",
            "epoch 44 batch 114 loss 0.06495420634746552\n",
            "epoch 44 batch 115 loss 0.05992848053574562\n",
            "epoch 44 batch 116 loss 0.05852299928665161\n",
            "epoch 44 batch 117 loss 0.06734360009431839\n",
            "epoch 44 batch 118 loss 0.062274932861328125\n",
            "epoch 44 batch 119 loss 0.06428717821836472\n",
            "epoch 44 batch 120 loss 0.06277991831302643\n",
            "epoch 44 batch 121 loss 0.058951593935489655\n",
            "epoch 44 batch 122 loss 0.06794950366020203\n",
            "epoch 44 batch 123 loss 0.0639394223690033\n",
            "epoch 44 batch 124 loss 0.06052520126104355\n",
            "epoch 44 batch 125 loss 0.05668140575289726\n",
            "epoch 44 batch 126 loss 0.058266784995794296\n",
            "epoch 44 batch 127 loss 0.062310848385095596\n",
            "epoch 44 batch 128 loss 0.061431579291820526\n",
            "epoch 44 batch 129 loss 0.06169590726494789\n",
            "epoch 44 batch 130 loss 0.0688960999250412\n",
            "epoch 44 batch 131 loss 0.06053420901298523\n",
            "epoch 44 batch 132 loss 0.05905427411198616\n",
            "epoch 44 batch 133 loss 0.05792203173041344\n",
            "epoch 44 batch 134 loss 0.06300932168960571\n",
            "epoch 44 batch 135 loss 0.061761002987623215\n",
            "epoch 44 batch 136 loss 0.06338656693696976\n",
            "epoch 44 batch 137 loss 0.06381063163280487\n",
            "epoch 44 batch 138 loss 0.0580771304666996\n",
            "epoch 44 batch 139 loss 0.0563560426235199\n",
            "epoch 44 batch 140 loss 0.06943455338478088\n",
            "epoch 44 batch 141 loss 0.062290508300065994\n",
            "epoch 44 batch 142 loss 0.06644036620855331\n",
            "epoch 44 batch 143 loss 0.06131924316287041\n",
            "epoch 44 batch 144 loss 0.06470267474651337\n",
            "epoch 44 batch 145 loss 0.06115304306149483\n",
            "epoch 44 batch 146 loss 0.05876091122627258\n",
            "epoch 44 batch 147 loss 0.05477208271622658\n",
            "epoch 44 batch 148 loss 0.06292875111103058\n",
            "epoch 44 batch 149 loss 0.06199588626623154\n",
            "epoch 44 batch 150 loss 0.057892005890607834\n",
            "epoch 44 batch 151 loss 0.057848796248435974\n",
            "epoch 44 batch 152 loss 0.0633360892534256\n",
            "epoch 44 batch 153 loss 0.06522428244352341\n",
            "epoch 44 batch 154 loss 0.061033107340335846\n",
            "epoch 44 batch 155 loss 0.0606062188744545\n",
            "epoch 44 batch 156 loss 0.06381149590015411\n",
            "epoch 44 batch 157 loss 0.05841412767767906\n",
            "epoch 44 batch 158 loss 0.05893273651599884\n",
            "epoch 44 batch 159 loss 0.05716861039400101\n",
            "epoch 44 batch 160 loss 0.06270808726549149\n",
            "epoch 44 batch 161 loss 0.06207588315010071\n",
            "epoch 44 batch 162 loss 0.06117555499076843\n",
            "epoch 44 batch 163 loss 0.06140563264489174\n",
            "epoch 44 batch 164 loss 0.06351866573095322\n",
            "epoch 44 batch 165 loss 0.06059064716100693\n",
            "epoch 44 batch 166 loss 0.05953206494450569\n",
            "epoch 44 batch 167 loss 0.06409309059381485\n",
            "epoch 44 batch 168 loss 0.05998987704515457\n",
            "epoch 44 batch 169 loss 0.06084848567843437\n",
            "epoch 44 batch 170 loss 0.06489886343479156\n",
            "epoch 44 batch 171 loss 0.06046352908015251\n",
            "epoch 44 batch 172 loss 0.0617503859102726\n",
            "epoch 44 batch 173 loss 0.06315675377845764\n",
            "epoch 44 batch 174 loss 0.059839870780706406\n",
            "epoch 44 batch 175 loss 0.06404823809862137\n",
            "epoch 44 batch 176 loss 0.06368732452392578\n",
            "epoch 44 batch 177 loss 0.06762266159057617\n",
            "epoch 44 batch 178 loss 0.06220201030373573\n",
            "epoch 44 batch 179 loss 0.059025295078754425\n",
            "epoch 44 batch 180 loss 0.0580238476395607\n",
            "epoch 44 batch 181 loss 0.06537218391895294\n",
            "epoch 44 batch 182 loss 0.06779634952545166\n",
            "epoch 44 batch 183 loss 0.05659108608961105\n",
            "epoch 44 batch 184 loss 0.06166469678282738\n",
            "epoch 44 batch 185 loss 0.06476230174303055\n",
            "epoch 44 batch 186 loss 0.06264498829841614\n",
            "epoch 44 batch 187 loss 0.06553525477647781\n",
            "epoch 44 batch 188 loss 0.05808887630701065\n",
            "epoch 44 batch 189 loss 0.0636228695511818\n",
            "epoch 44 batch 190 loss 0.0678175836801529\n",
            "epoch 44 batch 191 loss 0.06527716666460037\n",
            "epoch 44 batch 192 loss 0.07477524131536484\n",
            "epoch 44 batch 193 loss 0.06314379721879959\n",
            "epoch 44 batch 194 loss 0.06192157790064812\n",
            "epoch 44 batch 195 loss 0.06362759321928024\n",
            "epoch 44 batch 196 loss 0.06312023848295212\n",
            "epoch 44 batch 197 loss 0.058060746639966965\n",
            "epoch 44 batch 198 loss 0.05835582688450813\n",
            "epoch 44 batch 199 loss 0.05885942280292511\n",
            "epoch 44 batch 200 loss 0.06495556235313416\n",
            "epoch 44 batch 201 loss 0.0703737661242485\n",
            "epoch 44 batch 202 loss 0.06796341389417648\n",
            "epoch 44 batch 203 loss 0.06185048446059227\n",
            "epoch 44 batch 204 loss 0.062163181602954865\n",
            "epoch 44 batch 205 loss 0.0659194141626358\n",
            "epoch 44 batch 206 loss 0.06083815172314644\n",
            "epoch 44 batch 207 loss 0.0595429427921772\n",
            "epoch 44 batch 208 loss 0.05573134124279022\n",
            "epoch 44 batch 209 loss 0.056635238230228424\n",
            "epoch 44 batch 210 loss 0.055994823575019836\n",
            "epoch 44 batch 211 loss 0.05558812618255615\n",
            "epoch 44 batch 212 loss 0.06230437010526657\n",
            "epoch 44 batch 213 loss 0.06368730962276459\n",
            "epoch 44 batch 214 loss 0.06320904940366745\n",
            "epoch 44 batch 215 loss 0.06762978434562683\n",
            "epoch 44 batch 216 loss 0.06262209266424179\n",
            "epoch 44 batch 217 loss 0.07033791393041611\n",
            "epoch 44 batch 218 loss 0.062261808663606644\n",
            "epoch 44 batch 219 loss 0.061935801059007645\n",
            "epoch 44 batch 220 loss 0.056050676852464676\n",
            "epoch 44 batch 221 loss 0.06688550114631653\n",
            "epoch 44 batch 222 loss 0.05760696530342102\n",
            "epoch 44 batch 223 loss 0.061820950359106064\n",
            "epoch 44 batch 224 loss 0.05962610989809036\n",
            "epoch 44 batch 225 loss 0.060020796954631805\n",
            "epoch 44 batch 226 loss 0.05919463559985161\n",
            "epoch 44 batch 227 loss 0.06433195620775223\n",
            "epoch 44 batch 228 loss 0.06157553195953369\n",
            "epoch 44 batch 229 loss 0.06329929828643799\n",
            "epoch 44 batch 230 loss 0.05902247875928879\n",
            "epoch 45 batch 0 loss 0.05729370191693306\n",
            "epoch 45 batch 1 loss 0.06304442137479782\n",
            "epoch 45 batch 2 loss 0.053844064474105835\n",
            "epoch 45 batch 3 loss 0.05969865620136261\n",
            "epoch 45 batch 4 loss 0.05141536891460419\n",
            "epoch 45 batch 5 loss 0.06187715008854866\n",
            "epoch 45 batch 6 loss 0.05513652786612511\n",
            "epoch 45 batch 7 loss 0.05789300799369812\n",
            "epoch 45 batch 8 loss 0.06376107782125473\n",
            "epoch 45 batch 9 loss 0.061889369040727615\n",
            "epoch 45 batch 10 loss 0.06071668490767479\n",
            "epoch 45 batch 11 loss 0.06259871274232864\n",
            "epoch 45 batch 12 loss 0.05938206985592842\n",
            "epoch 45 batch 13 loss 0.05796993896365166\n",
            "epoch 45 batch 14 loss 0.055116742849349976\n",
            "epoch 45 batch 15 loss 0.05812307819724083\n",
            "epoch 45 batch 16 loss 0.05808940902352333\n",
            "epoch 45 batch 17 loss 0.05834079906344414\n",
            "epoch 45 batch 18 loss 0.05971180647611618\n",
            "epoch 45 batch 19 loss 0.06050626188516617\n",
            "epoch 45 batch 20 loss 0.0551912747323513\n",
            "epoch 45 batch 21 loss 0.059181809425354004\n",
            "epoch 45 batch 22 loss 0.061303138732910156\n",
            "epoch 45 batch 23 loss 0.05959603562951088\n",
            "epoch 45 batch 24 loss 0.06046147644519806\n",
            "epoch 45 batch 25 loss 0.06165314465761185\n",
            "epoch 45 batch 26 loss 0.059700578451156616\n",
            "epoch 45 batch 27 loss 0.06254802644252777\n",
            "epoch 45 batch 28 loss 0.057522907853126526\n",
            "epoch 45 batch 29 loss 0.06419120728969574\n",
            "epoch 45 batch 30 loss 0.05753977596759796\n",
            "epoch 45 batch 31 loss 0.06221373751759529\n",
            "epoch 45 batch 32 loss 0.06242907792329788\n",
            "epoch 45 batch 33 loss 0.062201499938964844\n",
            "epoch 45 batch 34 loss 0.06418269127607346\n",
            "epoch 45 batch 35 loss 0.06171836331486702\n",
            "epoch 45 batch 36 loss 0.05613378435373306\n",
            "epoch 45 batch 37 loss 0.05794678255915642\n",
            "epoch 45 batch 38 loss 0.05974379926919937\n",
            "epoch 45 batch 39 loss 0.07125905901193619\n",
            "epoch 45 batch 40 loss 0.062073688954114914\n",
            "epoch 45 batch 41 loss 0.06308582425117493\n",
            "epoch 45 batch 42 loss 0.06187238171696663\n",
            "epoch 45 batch 43 loss 0.0572119764983654\n",
            "epoch 45 batch 44 loss 0.06328966468572617\n",
            "epoch 45 batch 45 loss 0.05799953639507294\n",
            "epoch 45 batch 46 loss 0.06161895766854286\n",
            "epoch 45 batch 47 loss 0.06606461107730865\n",
            "epoch 45 batch 48 loss 0.0568733736872673\n",
            "epoch 45 batch 49 loss 0.06456293910741806\n",
            "epoch 45 batch 50 loss 0.06001228839159012\n",
            "epoch 45 batch 51 loss 0.05757652968168259\n",
            "epoch 45 batch 52 loss 0.060432594269514084\n",
            "epoch 45 batch 53 loss 0.06063921004533768\n",
            "epoch 45 batch 54 loss 0.05645358934998512\n",
            "epoch 45 batch 55 loss 0.05949966982007027\n",
            "epoch 45 batch 56 loss 0.060364507138729095\n",
            "epoch 45 batch 57 loss 0.05041300877928734\n",
            "epoch 45 batch 58 loss 0.06057388335466385\n",
            "epoch 45 batch 59 loss 0.05801408737897873\n",
            "epoch 45 batch 60 loss 0.05586336925625801\n",
            "epoch 45 batch 61 loss 0.05433713272213936\n",
            "epoch 45 batch 62 loss 0.060191404074430466\n",
            "epoch 45 batch 63 loss 0.06321153789758682\n",
            "epoch 45 batch 64 loss 0.06420719623565674\n",
            "epoch 45 batch 65 loss 0.060466866940259933\n",
            "epoch 45 batch 66 loss 0.06577622890472412\n",
            "epoch 45 batch 67 loss 0.05634111911058426\n",
            "epoch 45 batch 68 loss 0.06732041388750076\n",
            "epoch 45 batch 69 loss 0.05878229811787605\n",
            "epoch 45 batch 70 loss 0.059391118586063385\n",
            "epoch 45 batch 71 loss 0.05922876298427582\n",
            "epoch 45 batch 72 loss 0.06082484498620033\n",
            "epoch 45 batch 73 loss 0.06542379409074783\n",
            "epoch 45 batch 74 loss 0.0534847192466259\n",
            "epoch 45 batch 75 loss 0.05922303348779678\n",
            "epoch 45 batch 76 loss 0.0637618824839592\n",
            "epoch 45 batch 77 loss 0.05883780121803284\n",
            "epoch 45 batch 78 loss 0.06511616706848145\n",
            "epoch 45 batch 79 loss 0.06308922171592712\n",
            "epoch 45 batch 80 loss 0.05774074047803879\n",
            "epoch 45 batch 81 loss 0.06265359371900558\n",
            "epoch 45 batch 82 loss 0.0582629069685936\n",
            "epoch 45 batch 83 loss 0.05946218594908714\n",
            "epoch 45 batch 84 loss 0.05668891966342926\n",
            "epoch 45 batch 85 loss 0.0661761611700058\n",
            "epoch 45 batch 86 loss 0.061262231320142746\n",
            "epoch 45 batch 87 loss 0.05964670702815056\n",
            "epoch 45 batch 88 loss 0.06146696209907532\n",
            "epoch 45 batch 89 loss 0.05787588655948639\n",
            "epoch 45 batch 90 loss 0.0643036812543869\n",
            "epoch 45 batch 91 loss 0.06800656020641327\n",
            "epoch 45 batch 92 loss 0.05701970309019089\n",
            "epoch 45 batch 93 loss 0.0631878599524498\n",
            "epoch 45 batch 94 loss 0.06250057369470596\n",
            "epoch 45 batch 95 loss 0.060633353888988495\n",
            "epoch 45 batch 96 loss 0.06041421368718147\n",
            "epoch 45 batch 97 loss 0.062154706567525864\n",
            "epoch 45 batch 98 loss 0.06457831710577011\n",
            "epoch 45 batch 99 loss 0.06094617769122124\n",
            "epoch 45 batch 100 loss 0.06123363599181175\n",
            "epoch 45 batch 101 loss 0.057420991361141205\n",
            "epoch 45 batch 102 loss 0.05892205610871315\n",
            "epoch 45 batch 103 loss 0.06027503311634064\n",
            "epoch 45 batch 104 loss 0.061304181814193726\n",
            "epoch 45 batch 105 loss 0.06973972916603088\n",
            "epoch 45 batch 106 loss 0.05476764589548111\n",
            "epoch 45 batch 107 loss 0.0665283128619194\n",
            "epoch 45 batch 108 loss 0.06574159115552902\n",
            "epoch 45 batch 109 loss 0.056942615658044815\n",
            "epoch 45 batch 110 loss 0.05828629806637764\n",
            "epoch 45 batch 111 loss 0.057472679764032364\n",
            "epoch 45 batch 112 loss 0.056209105998277664\n",
            "epoch 45 batch 113 loss 0.059296417981386185\n",
            "epoch 45 batch 114 loss 0.0690852478146553\n",
            "epoch 45 batch 115 loss 0.05515465512871742\n",
            "epoch 45 batch 116 loss 0.06494265794754028\n",
            "epoch 45 batch 117 loss 0.06229273974895477\n",
            "epoch 45 batch 118 loss 0.05570461228489876\n",
            "epoch 45 batch 119 loss 0.06540365517139435\n",
            "epoch 45 batch 120 loss 0.06349757313728333\n",
            "epoch 45 batch 121 loss 0.059020061045885086\n",
            "epoch 45 batch 122 loss 0.06019860506057739\n",
            "epoch 45 batch 123 loss 0.0640900582075119\n",
            "epoch 45 batch 124 loss 0.06544654071331024\n",
            "epoch 45 batch 125 loss 0.06370212137699127\n",
            "epoch 45 batch 126 loss 0.057662442326545715\n",
            "epoch 45 batch 127 loss 0.06047280132770538\n",
            "epoch 45 batch 128 loss 0.06408187001943588\n",
            "epoch 45 batch 129 loss 0.051954369992017746\n",
            "epoch 45 batch 130 loss 0.055674195289611816\n",
            "epoch 45 batch 131 loss 0.06084664165973663\n",
            "epoch 45 batch 132 loss 0.0639711543917656\n",
            "epoch 45 batch 133 loss 0.061709001660346985\n",
            "epoch 45 batch 134 loss 0.06484014540910721\n",
            "epoch 45 batch 135 loss 0.054404117166996\n",
            "epoch 45 batch 136 loss 0.059802018105983734\n",
            "epoch 45 batch 137 loss 0.060614101588726044\n",
            "epoch 45 batch 138 loss 0.05850912258028984\n",
            "epoch 45 batch 139 loss 0.0599789060652256\n",
            "epoch 45 batch 140 loss 0.06141643598675728\n",
            "epoch 45 batch 141 loss 0.0658690482378006\n",
            "epoch 45 batch 142 loss 0.06005780026316643\n",
            "epoch 45 batch 143 loss 0.062072765082120895\n",
            "epoch 45 batch 144 loss 0.05964604765176773\n",
            "epoch 45 batch 145 loss 0.06033603101968765\n",
            "epoch 45 batch 146 loss 0.06528637558221817\n",
            "epoch 45 batch 147 loss 0.05821268633008003\n",
            "epoch 45 batch 148 loss 0.061106275767087936\n",
            "epoch 45 batch 149 loss 0.05983393266797066\n",
            "epoch 45 batch 150 loss 0.05618179216980934\n",
            "epoch 45 batch 151 loss 0.06426156312227249\n",
            "epoch 45 batch 152 loss 0.06608564406633377\n",
            "epoch 45 batch 153 loss 0.0677807554602623\n",
            "epoch 45 batch 154 loss 0.05504882335662842\n",
            "epoch 45 batch 155 loss 0.06565629690885544\n",
            "epoch 45 batch 156 loss 0.06296781450510025\n",
            "epoch 45 batch 157 loss 0.06037953868508339\n",
            "epoch 45 batch 158 loss 0.06303273886442184\n",
            "epoch 45 batch 159 loss 0.0627349391579628\n",
            "epoch 45 batch 160 loss 0.06328120827674866\n",
            "epoch 45 batch 161 loss 0.06623825430870056\n",
            "epoch 45 batch 162 loss 0.061096739023923874\n",
            "epoch 45 batch 163 loss 0.06261982023715973\n",
            "epoch 45 batch 164 loss 0.05916585773229599\n",
            "epoch 45 batch 165 loss 0.052102502435445786\n",
            "epoch 45 batch 166 loss 0.06689813733100891\n",
            "epoch 45 batch 167 loss 0.06310950219631195\n",
            "epoch 45 batch 168 loss 0.06641576439142227\n",
            "epoch 45 batch 169 loss 0.060039617121219635\n",
            "epoch 45 batch 170 loss 0.05878457427024841\n",
            "epoch 45 batch 171 loss 0.06844154000282288\n",
            "epoch 45 batch 172 loss 0.06410960853099823\n",
            "epoch 45 batch 173 loss 0.058347683399915695\n",
            "epoch 45 batch 174 loss 0.05558941513299942\n",
            "epoch 45 batch 175 loss 0.05606788024306297\n",
            "epoch 45 batch 176 loss 0.0541032999753952\n",
            "epoch 45 batch 177 loss 0.05970652773976326\n",
            "epoch 45 batch 178 loss 0.06636117398738861\n",
            "epoch 45 batch 179 loss 0.05508401244878769\n",
            "epoch 45 batch 180 loss 0.06049968674778938\n",
            "epoch 45 batch 181 loss 0.06289170682430267\n",
            "epoch 45 batch 182 loss 0.05978790298104286\n",
            "epoch 45 batch 183 loss 0.06556960195302963\n",
            "epoch 45 batch 184 loss 0.06580501049757004\n",
            "epoch 45 batch 185 loss 0.05204034596681595\n",
            "epoch 45 batch 186 loss 0.0642162635922432\n",
            "epoch 45 batch 187 loss 0.066432423889637\n",
            "epoch 45 batch 188 loss 0.06131674349308014\n",
            "epoch 45 batch 189 loss 0.06303421407938004\n",
            "epoch 45 batch 190 loss 0.057997047901153564\n",
            "epoch 45 batch 191 loss 0.05867674574255943\n",
            "epoch 45 batch 192 loss 0.0681605190038681\n",
            "epoch 45 batch 193 loss 0.06401935964822769\n",
            "epoch 45 batch 194 loss 0.06429603695869446\n",
            "epoch 45 batch 195 loss 0.06555867940187454\n",
            "epoch 45 batch 196 loss 0.0663873478770256\n",
            "epoch 45 batch 197 loss 0.06452281773090363\n",
            "epoch 45 batch 198 loss 0.06146823242306709\n",
            "epoch 45 batch 199 loss 0.06391648948192596\n",
            "epoch 45 batch 200 loss 0.05825667083263397\n",
            "epoch 45 batch 201 loss 0.06941360980272293\n",
            "epoch 45 batch 202 loss 0.05739656463265419\n",
            "epoch 45 batch 203 loss 0.06309574097394943\n",
            "epoch 45 batch 204 loss 0.06288021057844162\n",
            "epoch 45 batch 205 loss 0.06272421777248383\n",
            "epoch 45 batch 206 loss 0.061423033475875854\n",
            "epoch 45 batch 207 loss 0.06357195228338242\n",
            "epoch 45 batch 208 loss 0.0625823587179184\n",
            "epoch 45 batch 209 loss 0.0599360354244709\n",
            "epoch 45 batch 210 loss 0.058504220098257065\n",
            "epoch 45 batch 211 loss 0.06460200995206833\n",
            "epoch 45 batch 212 loss 0.06056514009833336\n",
            "epoch 45 batch 213 loss 0.06084651127457619\n",
            "epoch 45 batch 214 loss 0.06136282533407211\n",
            "epoch 45 batch 215 loss 0.06291606277227402\n",
            "epoch 45 batch 216 loss 0.07025167346000671\n",
            "epoch 45 batch 217 loss 0.06014004349708557\n",
            "epoch 45 batch 218 loss 0.06383629888296127\n",
            "epoch 45 batch 219 loss 0.06119547411799431\n",
            "epoch 45 batch 220 loss 0.06524183601140976\n",
            "epoch 45 batch 221 loss 0.056193746626377106\n",
            "epoch 45 batch 222 loss 0.06416681408882141\n",
            "epoch 45 batch 223 loss 0.0641804039478302\n",
            "epoch 45 batch 224 loss 0.05700365826487541\n",
            "epoch 45 batch 225 loss 0.06600747257471085\n",
            "epoch 45 batch 226 loss 0.06026811525225639\n",
            "epoch 45 batch 227 loss 0.0630980134010315\n",
            "epoch 45 batch 228 loss 0.059345558285713196\n",
            "epoch 45 batch 229 loss 0.05892195552587509\n",
            "epoch 45 batch 230 loss 0.05779470503330231\n",
            "epoch 46 batch 0 loss 0.05750439316034317\n",
            "epoch 46 batch 1 loss 0.056771960109472275\n",
            "epoch 46 batch 2 loss 0.0555076040327549\n",
            "epoch 46 batch 3 loss 0.06135344132781029\n",
            "epoch 46 batch 4 loss 0.05894458666443825\n",
            "epoch 46 batch 5 loss 0.06300429999828339\n",
            "epoch 46 batch 6 loss 0.06158292293548584\n",
            "epoch 46 batch 7 loss 0.05852339416742325\n",
            "epoch 46 batch 8 loss 0.059713345021009445\n",
            "epoch 46 batch 9 loss 0.0568765252828598\n",
            "epoch 46 batch 10 loss 0.052228737622499466\n",
            "epoch 46 batch 11 loss 0.05707646161317825\n",
            "epoch 46 batch 12 loss 0.062523253262043\n",
            "epoch 46 batch 13 loss 0.06006105989217758\n",
            "epoch 46 batch 14 loss 0.06399702280759811\n",
            "epoch 46 batch 15 loss 0.06672076135873795\n",
            "epoch 46 batch 16 loss 0.0677105039358139\n",
            "epoch 46 batch 17 loss 0.055837396532297134\n",
            "epoch 46 batch 18 loss 0.053444404155015945\n",
            "epoch 46 batch 19 loss 0.05893104523420334\n",
            "epoch 46 batch 20 loss 0.05996781587600708\n",
            "epoch 46 batch 21 loss 0.0685424730181694\n",
            "epoch 46 batch 22 loss 0.05110687389969826\n",
            "epoch 46 batch 23 loss 0.06545554846525192\n",
            "epoch 46 batch 24 loss 0.06326450407505035\n",
            "epoch 46 batch 25 loss 0.059546615928411484\n",
            "epoch 46 batch 26 loss 0.06568793952465057\n",
            "epoch 46 batch 27 loss 0.062360960990190506\n",
            "epoch 46 batch 28 loss 0.05844268202781677\n",
            "epoch 46 batch 29 loss 0.05857505276799202\n",
            "epoch 46 batch 30 loss 0.05563205108046532\n",
            "epoch 46 batch 31 loss 0.06062724441289902\n",
            "epoch 46 batch 32 loss 0.06088891997933388\n",
            "epoch 46 batch 33 loss 0.05712031200528145\n",
            "epoch 46 batch 34 loss 0.055349357426166534\n",
            "epoch 46 batch 35 loss 0.05547278746962547\n",
            "epoch 46 batch 36 loss 0.06024601683020592\n",
            "epoch 46 batch 37 loss 0.06286480277776718\n",
            "epoch 46 batch 38 loss 0.06072008237242699\n",
            "epoch 46 batch 39 loss 0.058854687958955765\n",
            "epoch 46 batch 40 loss 0.060750674456357956\n",
            "epoch 46 batch 41 loss 0.05960468947887421\n",
            "epoch 46 batch 42 loss 0.0630648136138916\n",
            "epoch 46 batch 43 loss 0.06201072037220001\n",
            "epoch 46 batch 44 loss 0.057743582874536514\n",
            "epoch 46 batch 45 loss 0.05828167125582695\n",
            "epoch 46 batch 46 loss 0.056320253759622574\n",
            "epoch 46 batch 47 loss 0.05773692950606346\n",
            "epoch 46 batch 48 loss 0.06485243141651154\n",
            "epoch 46 batch 49 loss 0.06031901761889458\n",
            "epoch 46 batch 50 loss 0.05405039340257645\n",
            "epoch 46 batch 51 loss 0.059279002249240875\n",
            "epoch 46 batch 52 loss 0.06155477464199066\n",
            "epoch 46 batch 53 loss 0.058437105268239975\n",
            "epoch 46 batch 54 loss 0.06290799379348755\n",
            "epoch 46 batch 55 loss 0.062332432717084885\n",
            "epoch 46 batch 56 loss 0.06298043578863144\n",
            "epoch 46 batch 57 loss 0.0621647946536541\n",
            "epoch 46 batch 58 loss 0.05577363818883896\n",
            "epoch 46 batch 59 loss 0.05802863463759422\n",
            "epoch 46 batch 60 loss 0.055967818945646286\n",
            "epoch 46 batch 61 loss 0.06183600798249245\n",
            "epoch 46 batch 62 loss 0.061909791082143784\n",
            "epoch 46 batch 63 loss 0.06261274218559265\n",
            "epoch 46 batch 64 loss 0.06298203766345978\n",
            "epoch 46 batch 65 loss 0.06070105731487274\n",
            "epoch 46 batch 66 loss 0.05791664868593216\n",
            "epoch 46 batch 67 loss 0.059387024492025375\n",
            "epoch 46 batch 68 loss 0.059512678533792496\n",
            "epoch 46 batch 69 loss 0.06026993319392204\n",
            "epoch 46 batch 70 loss 0.05724697932600975\n",
            "epoch 46 batch 71 loss 0.06250222772359848\n",
            "epoch 46 batch 72 loss 0.0632527768611908\n",
            "epoch 46 batch 73 loss 0.06100345030426979\n",
            "epoch 46 batch 74 loss 0.05644204467535019\n",
            "epoch 46 batch 75 loss 0.059453561902046204\n",
            "epoch 46 batch 76 loss 0.060381144285202026\n",
            "epoch 46 batch 77 loss 0.05532902851700783\n",
            "epoch 46 batch 78 loss 0.06054452806711197\n",
            "epoch 46 batch 79 loss 0.05681968107819557\n",
            "epoch 46 batch 80 loss 0.0577893890440464\n",
            "epoch 46 batch 81 loss 0.06180615723133087\n",
            "epoch 46 batch 82 loss 0.06096726655960083\n",
            "epoch 46 batch 83 loss 0.05927632749080658\n",
            "epoch 46 batch 84 loss 0.061514150351285934\n",
            "epoch 46 batch 85 loss 0.06232069805264473\n",
            "epoch 46 batch 86 loss 0.05269693210721016\n",
            "epoch 46 batch 87 loss 0.05620003864169121\n",
            "epoch 46 batch 88 loss 0.06460027396678925\n",
            "epoch 46 batch 89 loss 0.06353668123483658\n",
            "epoch 46 batch 90 loss 0.0613771490752697\n",
            "epoch 46 batch 91 loss 0.06640482693910599\n",
            "epoch 46 batch 92 loss 0.06872234493494034\n",
            "epoch 46 batch 93 loss 0.06022242084145546\n",
            "epoch 46 batch 94 loss 0.06455615162849426\n",
            "epoch 46 batch 95 loss 0.0611422024667263\n",
            "epoch 46 batch 96 loss 0.057106997817754745\n",
            "epoch 46 batch 97 loss 0.06097440794110298\n",
            "epoch 46 batch 98 loss 0.06230728328227997\n",
            "epoch 46 batch 99 loss 0.06265205889940262\n",
            "epoch 46 batch 100 loss 0.05729088559746742\n",
            "epoch 46 batch 101 loss 0.058833979070186615\n",
            "epoch 46 batch 102 loss 0.057413335889577866\n",
            "epoch 46 batch 103 loss 0.06094212830066681\n",
            "epoch 46 batch 104 loss 0.0646054819226265\n",
            "epoch 46 batch 105 loss 0.06396681070327759\n",
            "epoch 46 batch 106 loss 0.060732752084732056\n",
            "epoch 46 batch 107 loss 0.06922520697116852\n",
            "epoch 46 batch 108 loss 0.06579077988862991\n",
            "epoch 46 batch 109 loss 0.05829980969429016\n",
            "epoch 46 batch 110 loss 0.05964403972029686\n",
            "epoch 46 batch 111 loss 0.056134190410375595\n",
            "epoch 46 batch 112 loss 0.06308234483003616\n",
            "epoch 46 batch 113 loss 0.06042430177330971\n",
            "epoch 46 batch 114 loss 0.06190209090709686\n",
            "epoch 46 batch 115 loss 0.06561917066574097\n",
            "epoch 46 batch 116 loss 0.059332359582185745\n",
            "epoch 46 batch 117 loss 0.05992245301604271\n",
            "epoch 46 batch 118 loss 0.06919408589601517\n",
            "epoch 46 batch 119 loss 0.06371217221021652\n",
            "epoch 46 batch 120 loss 0.06388795375823975\n",
            "epoch 46 batch 121 loss 0.05540145933628082\n",
            "epoch 46 batch 122 loss 0.05979858338832855\n",
            "epoch 46 batch 123 loss 0.06318367272615433\n",
            "epoch 46 batch 124 loss 0.061972521245479584\n",
            "epoch 46 batch 125 loss 0.05978192016482353\n",
            "epoch 46 batch 126 loss 0.06575898826122284\n",
            "epoch 46 batch 127 loss 0.062351301312446594\n",
            "epoch 46 batch 128 loss 0.05935858562588692\n",
            "epoch 46 batch 129 loss 0.059975508600473404\n",
            "epoch 46 batch 130 loss 0.05772405490279198\n",
            "epoch 46 batch 131 loss 0.061088744550943375\n",
            "epoch 46 batch 132 loss 0.058727577328681946\n",
            "epoch 46 batch 133 loss 0.06145251542329788\n",
            "epoch 46 batch 134 loss 0.05810759961605072\n",
            "epoch 46 batch 135 loss 0.07013163715600967\n",
            "epoch 46 batch 136 loss 0.05974065139889717\n",
            "epoch 46 batch 137 loss 0.059655629098415375\n",
            "epoch 46 batch 138 loss 0.07001455128192902\n",
            "epoch 46 batch 139 loss 0.055544413626194\n",
            "epoch 46 batch 140 loss 0.06342034041881561\n",
            "epoch 46 batch 141 loss 0.06826847046613693\n",
            "epoch 46 batch 142 loss 0.05925706773996353\n",
            "epoch 46 batch 143 loss 0.06345782428979874\n",
            "epoch 46 batch 144 loss 0.05996599793434143\n",
            "epoch 46 batch 145 loss 0.05900599807500839\n",
            "epoch 46 batch 146 loss 0.06629408150911331\n",
            "epoch 46 batch 147 loss 0.06076003983616829\n",
            "epoch 46 batch 148 loss 0.06300088763237\n",
            "epoch 46 batch 149 loss 0.06142563000321388\n",
            "epoch 46 batch 150 loss 0.06505189836025238\n",
            "epoch 46 batch 151 loss 0.06201835721731186\n",
            "epoch 46 batch 152 loss 0.0673137828707695\n",
            "epoch 46 batch 153 loss 0.06644914299249649\n",
            "epoch 46 batch 154 loss 0.05824052914977074\n",
            "epoch 46 batch 155 loss 0.06787686049938202\n",
            "epoch 46 batch 156 loss 0.06172081083059311\n",
            "epoch 46 batch 157 loss 0.060104165226221085\n",
            "epoch 46 batch 158 loss 0.05901342257857323\n",
            "epoch 46 batch 159 loss 0.06025568023324013\n",
            "epoch 46 batch 160 loss 0.06257354468107224\n",
            "epoch 46 batch 161 loss 0.06121408939361572\n",
            "epoch 46 batch 162 loss 0.05719044432044029\n",
            "epoch 46 batch 163 loss 0.06281573325395584\n",
            "epoch 46 batch 164 loss 0.059096839278936386\n",
            "epoch 46 batch 165 loss 0.057329773902893066\n",
            "epoch 46 batch 166 loss 0.06790730357170105\n",
            "epoch 46 batch 167 loss 0.0634491965174675\n",
            "epoch 46 batch 168 loss 0.05842316895723343\n",
            "epoch 46 batch 169 loss 0.05732668563723564\n",
            "epoch 46 batch 170 loss 0.06570493429899216\n",
            "epoch 46 batch 171 loss 0.06509929150342941\n",
            "epoch 46 batch 172 loss 0.06325455009937286\n",
            "epoch 46 batch 173 loss 0.06329789012670517\n",
            "epoch 46 batch 174 loss 0.05934319272637367\n",
            "epoch 46 batch 175 loss 0.0689510703086853\n",
            "epoch 46 batch 176 loss 0.06314157694578171\n",
            "epoch 46 batch 177 loss 0.06379710137844086\n",
            "epoch 46 batch 178 loss 0.056137003004550934\n",
            "epoch 46 batch 179 loss 0.06399980187416077\n",
            "epoch 46 batch 180 loss 0.056872133165597916\n",
            "epoch 46 batch 181 loss 0.06340228766202927\n",
            "epoch 46 batch 182 loss 0.06293682754039764\n",
            "epoch 46 batch 183 loss 0.058306820690631866\n",
            "epoch 46 batch 184 loss 0.05952977016568184\n",
            "epoch 46 batch 185 loss 0.05839190632104874\n",
            "epoch 46 batch 186 loss 0.06292371451854706\n",
            "epoch 46 batch 187 loss 0.06712034344673157\n",
            "epoch 46 batch 188 loss 0.058058928698301315\n",
            "epoch 46 batch 189 loss 0.06290916353464127\n",
            "epoch 46 batch 190 loss 0.0637163296341896\n",
            "epoch 46 batch 191 loss 0.06039140745997429\n",
            "epoch 46 batch 192 loss 0.06027868762612343\n",
            "epoch 46 batch 193 loss 0.05880976840853691\n",
            "epoch 46 batch 194 loss 0.059156633913517\n",
            "epoch 46 batch 195 loss 0.05927540361881256\n",
            "epoch 46 batch 196 loss 0.06233711168169975\n",
            "epoch 46 batch 197 loss 0.057449836283922195\n",
            "epoch 46 batch 198 loss 0.058909300714731216\n",
            "epoch 46 batch 199 loss 0.05993357300758362\n",
            "epoch 46 batch 200 loss 0.0635467916727066\n",
            "epoch 46 batch 201 loss 0.06006522476673126\n",
            "epoch 46 batch 202 loss 0.06457328051328659\n",
            "epoch 46 batch 203 loss 0.06215926632285118\n",
            "epoch 46 batch 204 loss 0.06781172752380371\n",
            "epoch 46 batch 205 loss 0.06787080317735672\n",
            "epoch 46 batch 206 loss 0.06335371732711792\n",
            "epoch 46 batch 207 loss 0.06115748733282089\n",
            "epoch 46 batch 208 loss 0.0628485456109047\n",
            "epoch 46 batch 209 loss 0.06725241243839264\n",
            "epoch 46 batch 210 loss 0.05579228699207306\n",
            "epoch 46 batch 211 loss 0.05956433713436127\n",
            "epoch 46 batch 212 loss 0.05909986421465874\n",
            "epoch 46 batch 213 loss 0.06491918861865997\n",
            "epoch 46 batch 214 loss 0.058652281761169434\n",
            "epoch 46 batch 215 loss 0.05890590697526932\n",
            "epoch 46 batch 216 loss 0.06382299959659576\n",
            "epoch 46 batch 217 loss 0.05429728701710701\n",
            "epoch 46 batch 218 loss 0.05699457973241806\n",
            "epoch 46 batch 219 loss 0.06545526534318924\n",
            "epoch 46 batch 220 loss 0.06237209215760231\n",
            "epoch 46 batch 221 loss 0.06214433163404465\n",
            "epoch 46 batch 222 loss 0.05847027525305748\n",
            "epoch 46 batch 223 loss 0.058440908789634705\n",
            "epoch 46 batch 224 loss 0.0578974150121212\n",
            "epoch 46 batch 225 loss 0.05780959501862526\n",
            "epoch 46 batch 226 loss 0.0646425187587738\n",
            "epoch 46 batch 227 loss 0.061736881732940674\n",
            "epoch 46 batch 228 loss 0.06126720458269119\n",
            "epoch 46 batch 229 loss 0.06443626433610916\n",
            "epoch 46 batch 230 loss 0.06383299082517624\n",
            "epoch 47 batch 0 loss 0.05355653166770935\n",
            "epoch 47 batch 1 loss 0.05518266558647156\n",
            "epoch 47 batch 2 loss 0.05603070557117462\n",
            "epoch 47 batch 3 loss 0.06329143792390823\n",
            "epoch 47 batch 4 loss 0.06380954384803772\n",
            "epoch 47 batch 5 loss 0.06215322017669678\n",
            "epoch 47 batch 6 loss 0.06017126888036728\n",
            "epoch 47 batch 7 loss 0.05879249423742294\n",
            "epoch 47 batch 8 loss 0.055619996041059494\n",
            "epoch 47 batch 9 loss 0.06030314043164253\n",
            "epoch 47 batch 10 loss 0.05951091647148132\n",
            "epoch 47 batch 11 loss 0.05442030727863312\n",
            "epoch 47 batch 12 loss 0.061582356691360474\n",
            "epoch 47 batch 13 loss 0.05270872637629509\n",
            "epoch 47 batch 14 loss 0.05783665180206299\n",
            "epoch 47 batch 15 loss 0.058069176971912384\n",
            "epoch 47 batch 16 loss 0.058769822120666504\n",
            "epoch 47 batch 17 loss 0.05835612118244171\n",
            "epoch 47 batch 18 loss 0.05770444869995117\n",
            "epoch 47 batch 19 loss 0.05614151805639267\n",
            "epoch 47 batch 20 loss 0.05721171572804451\n",
            "epoch 47 batch 21 loss 0.057273831218481064\n",
            "epoch 47 batch 22 loss 0.058946751058101654\n",
            "epoch 47 batch 23 loss 0.06135334074497223\n",
            "epoch 47 batch 24 loss 0.06581782549619675\n",
            "epoch 47 batch 25 loss 0.06735968589782715\n",
            "epoch 47 batch 26 loss 0.05798448622226715\n",
            "epoch 47 batch 27 loss 0.05403001233935356\n",
            "epoch 47 batch 28 loss 0.05880153179168701\n",
            "epoch 47 batch 29 loss 0.058409545570611954\n",
            "epoch 47 batch 30 loss 0.0652485340833664\n",
            "epoch 47 batch 31 loss 0.058392930775880814\n",
            "epoch 47 batch 32 loss 0.06251642107963562\n",
            "epoch 47 batch 33 loss 0.06463569402694702\n",
            "epoch 47 batch 34 loss 0.060656458139419556\n",
            "epoch 47 batch 35 loss 0.057399310171604156\n",
            "epoch 47 batch 36 loss 0.06393152475357056\n",
            "epoch 47 batch 37 loss 0.060614172369241714\n",
            "epoch 47 batch 38 loss 0.06106020137667656\n",
            "epoch 47 batch 39 loss 0.06141548976302147\n",
            "epoch 47 batch 40 loss 0.0595640130341053\n",
            "epoch 47 batch 41 loss 0.06001397967338562\n",
            "epoch 47 batch 42 loss 0.05881538242101669\n",
            "epoch 47 batch 43 loss 0.055701375007629395\n",
            "epoch 47 batch 44 loss 0.0589449368417263\n",
            "epoch 47 batch 45 loss 0.0619395449757576\n",
            "epoch 47 batch 46 loss 0.05275344476103783\n",
            "epoch 47 batch 47 loss 0.05517473816871643\n",
            "epoch 47 batch 48 loss 0.06240445002913475\n",
            "epoch 47 batch 49 loss 0.06322615593671799\n",
            "epoch 47 batch 50 loss 0.06228116527199745\n",
            "epoch 47 batch 51 loss 0.058478254824876785\n",
            "epoch 47 batch 52 loss 0.06130433455109596\n",
            "epoch 47 batch 53 loss 0.06463584303855896\n",
            "epoch 47 batch 54 loss 0.06447426974773407\n",
            "epoch 47 batch 55 loss 0.06334174424409866\n",
            "epoch 47 batch 56 loss 0.06421177834272385\n",
            "epoch 47 batch 57 loss 0.0613851435482502\n",
            "epoch 47 batch 58 loss 0.05976014956831932\n",
            "epoch 47 batch 59 loss 0.06201034039258957\n",
            "epoch 47 batch 60 loss 0.06017541140317917\n",
            "epoch 47 batch 61 loss 0.05914733186364174\n",
            "epoch 47 batch 62 loss 0.05786151811480522\n",
            "epoch 47 batch 63 loss 0.06742777675390244\n",
            "epoch 47 batch 64 loss 0.05892138183116913\n",
            "epoch 47 batch 65 loss 0.05953894183039665\n",
            "epoch 47 batch 66 loss 0.058519307523965836\n",
            "epoch 47 batch 67 loss 0.057382483035326004\n",
            "epoch 47 batch 68 loss 0.05993223935365677\n",
            "epoch 47 batch 69 loss 0.05916590988636017\n",
            "epoch 47 batch 70 loss 0.05655127763748169\n",
            "epoch 47 batch 71 loss 0.06092190369963646\n",
            "epoch 47 batch 72 loss 0.05710351839661598\n",
            "epoch 47 batch 73 loss 0.06060170382261276\n",
            "epoch 47 batch 74 loss 0.06004951521754265\n",
            "epoch 47 batch 75 loss 0.06186651811003685\n",
            "epoch 47 batch 76 loss 0.06392819434404373\n",
            "epoch 47 batch 77 loss 0.05837059020996094\n",
            "epoch 47 batch 78 loss 0.05875648558139801\n",
            "epoch 47 batch 79 loss 0.05814722180366516\n",
            "epoch 47 batch 80 loss 0.06548857688903809\n",
            "epoch 47 batch 81 loss 0.059382788836956024\n",
            "epoch 47 batch 82 loss 0.061067912727594376\n",
            "epoch 47 batch 83 loss 0.056623902171850204\n",
            "epoch 47 batch 84 loss 0.06283930689096451\n",
            "epoch 47 batch 85 loss 0.06446879357099533\n",
            "epoch 47 batch 86 loss 0.06373626738786697\n",
            "epoch 47 batch 87 loss 0.06410523504018784\n",
            "epoch 47 batch 88 loss 0.056626129895448685\n",
            "epoch 47 batch 89 loss 0.0641675814986229\n",
            "epoch 47 batch 90 loss 0.057827167212963104\n",
            "epoch 47 batch 91 loss 0.061878908425569534\n",
            "epoch 47 batch 92 loss 0.061146464198827744\n",
            "epoch 47 batch 93 loss 0.06201740726828575\n",
            "epoch 47 batch 94 loss 0.05657730624079704\n",
            "epoch 47 batch 95 loss 0.05684811994433403\n",
            "epoch 47 batch 96 loss 0.06023447588086128\n",
            "epoch 47 batch 97 loss 0.05735589936375618\n",
            "epoch 47 batch 98 loss 0.060189105570316315\n",
            "epoch 47 batch 99 loss 0.06101870909333229\n",
            "epoch 47 batch 100 loss 0.0619523823261261\n",
            "epoch 47 batch 101 loss 0.06483980268239975\n",
            "epoch 47 batch 102 loss 0.05889268219470978\n",
            "epoch 47 batch 103 loss 0.057844411581754684\n",
            "epoch 47 batch 104 loss 0.06361453235149384\n",
            "epoch 47 batch 105 loss 0.05982604995369911\n",
            "epoch 47 batch 106 loss 0.06330408155918121\n",
            "epoch 47 batch 107 loss 0.06446240842342377\n",
            "epoch 47 batch 108 loss 0.056165143847465515\n",
            "epoch 47 batch 109 loss 0.055213820189237595\n",
            "epoch 47 batch 110 loss 0.06263816356658936\n",
            "epoch 47 batch 111 loss 0.059219520539045334\n",
            "epoch 47 batch 112 loss 0.0594666488468647\n",
            "epoch 47 batch 113 loss 0.0565335713326931\n",
            "epoch 47 batch 114 loss 0.06008762866258621\n",
            "epoch 47 batch 115 loss 0.06489017605781555\n",
            "epoch 47 batch 116 loss 0.05969097092747688\n",
            "epoch 47 batch 117 loss 0.05812712386250496\n",
            "epoch 47 batch 118 loss 0.06143680587410927\n",
            "epoch 47 batch 119 loss 0.05928625166416168\n",
            "epoch 47 batch 120 loss 0.05953354388475418\n",
            "epoch 47 batch 121 loss 0.059940312057733536\n",
            "epoch 47 batch 122 loss 0.06298533827066422\n",
            "epoch 47 batch 123 loss 0.06158218905329704\n",
            "epoch 47 batch 124 loss 0.06175616756081581\n",
            "epoch 47 batch 125 loss 0.056909751147031784\n",
            "epoch 47 batch 126 loss 0.0681467205286026\n",
            "epoch 47 batch 127 loss 0.056510988622903824\n",
            "epoch 47 batch 128 loss 0.0583731085062027\n",
            "epoch 47 batch 129 loss 0.059432342648506165\n",
            "epoch 47 batch 130 loss 0.05560492351651192\n",
            "epoch 47 batch 131 loss 0.061108317226171494\n",
            "epoch 47 batch 132 loss 0.062160708010196686\n",
            "epoch 47 batch 133 loss 0.06513141840696335\n",
            "epoch 47 batch 134 loss 0.06346729397773743\n",
            "epoch 47 batch 135 loss 0.06515289843082428\n",
            "epoch 47 batch 136 loss 0.06239791959524155\n",
            "epoch 47 batch 137 loss 0.06616546958684921\n",
            "epoch 47 batch 138 loss 0.05772189423441887\n",
            "epoch 47 batch 139 loss 0.06137367710471153\n",
            "epoch 47 batch 140 loss 0.06373901665210724\n",
            "epoch 47 batch 141 loss 0.0606892891228199\n",
            "epoch 47 batch 142 loss 0.06167192384600639\n",
            "epoch 47 batch 143 loss 0.06698668748140335\n",
            "epoch 47 batch 144 loss 0.06260618567466736\n",
            "epoch 47 batch 145 loss 0.058978017419576645\n",
            "epoch 47 batch 146 loss 0.056789837777614594\n",
            "epoch 47 batch 147 loss 0.057484716176986694\n",
            "epoch 47 batch 148 loss 0.06828570365905762\n",
            "epoch 47 batch 149 loss 0.0613313727080822\n",
            "epoch 47 batch 150 loss 0.05668604373931885\n",
            "epoch 47 batch 151 loss 0.05992591753602028\n",
            "epoch 47 batch 152 loss 0.06176003813743591\n",
            "epoch 47 batch 153 loss 0.06501995772123337\n",
            "epoch 47 batch 154 loss 0.06553791463375092\n",
            "epoch 47 batch 155 loss 0.06078336760401726\n",
            "epoch 47 batch 156 loss 0.05929688736796379\n",
            "epoch 47 batch 157 loss 0.06343793123960495\n",
            "epoch 47 batch 158 loss 0.060077521950006485\n",
            "epoch 47 batch 159 loss 0.062127500772476196\n",
            "epoch 47 batch 160 loss 0.05815652385354042\n",
            "epoch 47 batch 161 loss 0.06509280204772949\n",
            "epoch 47 batch 162 loss 0.06302019953727722\n",
            "epoch 47 batch 163 loss 0.06410940736532211\n",
            "epoch 47 batch 164 loss 0.06135806441307068\n",
            "epoch 47 batch 165 loss 0.05875789746642113\n",
            "epoch 47 batch 166 loss 0.06181710585951805\n",
            "epoch 47 batch 167 loss 0.06346431374549866\n",
            "epoch 47 batch 168 loss 0.06038213148713112\n",
            "epoch 47 batch 169 loss 0.05666409060359001\n",
            "epoch 47 batch 170 loss 0.0568573884665966\n",
            "epoch 47 batch 171 loss 0.059994228184223175\n",
            "epoch 47 batch 172 loss 0.06073868274688721\n",
            "epoch 47 batch 173 loss 0.06384792178869247\n",
            "epoch 47 batch 174 loss 0.06210501492023468\n",
            "epoch 47 batch 175 loss 0.05733540281653404\n",
            "epoch 47 batch 176 loss 0.05995209887623787\n",
            "epoch 47 batch 177 loss 0.06373511254787445\n",
            "epoch 47 batch 178 loss 0.05427010357379913\n",
            "epoch 47 batch 179 loss 0.06348135322332382\n",
            "epoch 47 batch 180 loss 0.06634461879730225\n",
            "epoch 47 batch 181 loss 0.058871861547231674\n",
            "epoch 47 batch 182 loss 0.06090841814875603\n",
            "epoch 47 batch 183 loss 0.06700573116540909\n",
            "epoch 47 batch 184 loss 0.06013112887740135\n",
            "epoch 47 batch 185 loss 0.062029629945755005\n",
            "epoch 47 batch 186 loss 0.06780107319355011\n",
            "epoch 47 batch 187 loss 0.06306950002908707\n",
            "epoch 47 batch 188 loss 0.059723854064941406\n",
            "epoch 47 batch 189 loss 0.06609560549259186\n",
            "epoch 47 batch 190 loss 0.0580584891140461\n",
            "epoch 47 batch 191 loss 0.05568878352642059\n",
            "epoch 47 batch 192 loss 0.06563862413167953\n",
            "epoch 47 batch 193 loss 0.05730471387505531\n",
            "epoch 47 batch 194 loss 0.060637861490249634\n",
            "epoch 47 batch 195 loss 0.06105456501245499\n",
            "epoch 47 batch 196 loss 0.061483610421419144\n",
            "epoch 47 batch 197 loss 0.05866033583879471\n",
            "epoch 47 batch 198 loss 0.06126143038272858\n",
            "epoch 47 batch 199 loss 0.06244652345776558\n",
            "epoch 47 batch 200 loss 0.06341622769832611\n",
            "epoch 47 batch 201 loss 0.05879971757531166\n",
            "epoch 47 batch 202 loss 0.06003985553979874\n",
            "epoch 47 batch 203 loss 0.05791325122117996\n",
            "epoch 47 batch 204 loss 0.05945424363017082\n",
            "epoch 47 batch 205 loss 0.06347918510437012\n",
            "epoch 47 batch 206 loss 0.06363385915756226\n",
            "epoch 47 batch 207 loss 0.057200655341148376\n",
            "epoch 47 batch 208 loss 0.059290677309036255\n",
            "epoch 47 batch 209 loss 0.06682432442903519\n",
            "epoch 47 batch 210 loss 0.056012000888586044\n",
            "epoch 47 batch 211 loss 0.06902584433555603\n",
            "epoch 47 batch 212 loss 0.06071631610393524\n",
            "epoch 47 batch 213 loss 0.060248516499996185\n",
            "epoch 47 batch 214 loss 0.05871471390128136\n",
            "epoch 47 batch 215 loss 0.05874673277139664\n",
            "epoch 47 batch 216 loss 0.06490752100944519\n",
            "epoch 47 batch 217 loss 0.062197551131248474\n",
            "epoch 47 batch 218 loss 0.059584520757198334\n",
            "epoch 47 batch 219 loss 0.06950495392084122\n",
            "epoch 47 batch 220 loss 0.06716015934944153\n",
            "epoch 47 batch 221 loss 0.06430704891681671\n",
            "epoch 47 batch 222 loss 0.05592024326324463\n",
            "epoch 47 batch 223 loss 0.06435328722000122\n",
            "epoch 47 batch 224 loss 0.06274214386940002\n",
            "epoch 47 batch 225 loss 0.05800976976752281\n",
            "epoch 47 batch 226 loss 0.05863051116466522\n",
            "epoch 47 batch 227 loss 0.06063032150268555\n",
            "epoch 47 batch 228 loss 0.06050527095794678\n",
            "epoch 47 batch 229 loss 0.06272335350513458\n",
            "epoch 47 batch 230 loss 0.057530470192432404\n",
            "epoch 48 batch 0 loss 0.05965791642665863\n",
            "epoch 48 batch 1 loss 0.06396906822919846\n",
            "epoch 48 batch 2 loss 0.054060082882642746\n",
            "epoch 48 batch 3 loss 0.053811173886060715\n",
            "epoch 48 batch 4 loss 0.06463120877742767\n",
            "epoch 48 batch 5 loss 0.06958741694688797\n",
            "epoch 48 batch 6 loss 0.057215701788663864\n",
            "epoch 48 batch 7 loss 0.05848165974020958\n",
            "epoch 48 batch 8 loss 0.056905388832092285\n",
            "epoch 48 batch 9 loss 0.06201663240790367\n",
            "epoch 48 batch 10 loss 0.05269228667020798\n",
            "epoch 48 batch 11 loss 0.05349304899573326\n",
            "epoch 48 batch 12 loss 0.05995980650186539\n",
            "epoch 48 batch 13 loss 0.05411718413233757\n",
            "epoch 48 batch 14 loss 0.062200795859098434\n",
            "epoch 48 batch 15 loss 0.0614517480134964\n",
            "epoch 48 batch 16 loss 0.0568544827401638\n",
            "epoch 48 batch 17 loss 0.05336602032184601\n",
            "epoch 48 batch 18 loss 0.05656993389129639\n",
            "epoch 48 batch 19 loss 0.06443452835083008\n",
            "epoch 48 batch 20 loss 0.05552656203508377\n",
            "epoch 48 batch 21 loss 0.05860612913966179\n",
            "epoch 48 batch 22 loss 0.060337554663419724\n",
            "epoch 48 batch 23 loss 0.062047723680734634\n",
            "epoch 48 batch 24 loss 0.05625671520829201\n",
            "epoch 48 batch 25 loss 0.059612128883600235\n",
            "epoch 48 batch 26 loss 0.06182508543133736\n",
            "epoch 48 batch 27 loss 0.06089388579130173\n",
            "epoch 48 batch 28 loss 0.06384138762950897\n",
            "epoch 48 batch 29 loss 0.05992691218852997\n",
            "epoch 48 batch 30 loss 0.056445956230163574\n",
            "epoch 48 batch 31 loss 0.06111167371273041\n",
            "epoch 48 batch 32 loss 0.06171239912509918\n",
            "epoch 48 batch 33 loss 0.06486700475215912\n",
            "epoch 48 batch 34 loss 0.0655745267868042\n",
            "epoch 48 batch 35 loss 0.06231282651424408\n",
            "epoch 48 batch 36 loss 0.06010059267282486\n",
            "epoch 48 batch 37 loss 0.06121179088950157\n",
            "epoch 48 batch 38 loss 0.05557861179113388\n",
            "epoch 48 batch 39 loss 0.06596904247999191\n",
            "epoch 48 batch 40 loss 0.06314677000045776\n",
            "epoch 48 batch 41 loss 0.057312607765197754\n",
            "epoch 48 batch 42 loss 0.058360908180475235\n",
            "epoch 48 batch 43 loss 0.06188943237066269\n",
            "epoch 48 batch 44 loss 0.05754915997385979\n",
            "epoch 48 batch 45 loss 0.05572166293859482\n",
            "epoch 48 batch 46 loss 0.06809862703084946\n",
            "epoch 48 batch 47 loss 0.05932750552892685\n",
            "epoch 48 batch 48 loss 0.062471434473991394\n",
            "epoch 48 batch 49 loss 0.059663545340299606\n",
            "epoch 48 batch 50 loss 0.0629541203379631\n",
            "epoch 48 batch 51 loss 0.06389692425727844\n",
            "epoch 48 batch 52 loss 0.058903057128190994\n",
            "epoch 48 batch 53 loss 0.05854175612330437\n",
            "epoch 48 batch 54 loss 0.06010537967085838\n",
            "epoch 48 batch 55 loss 0.05624128878116608\n",
            "epoch 48 batch 56 loss 0.05947548523545265\n",
            "epoch 48 batch 57 loss 0.056428998708724976\n",
            "epoch 48 batch 58 loss 0.0595695860683918\n",
            "epoch 48 batch 59 loss 0.05147184431552887\n",
            "epoch 48 batch 60 loss 0.06101636216044426\n",
            "epoch 48 batch 61 loss 0.05899959057569504\n",
            "epoch 48 batch 62 loss 0.057003453373909\n",
            "epoch 48 batch 63 loss 0.057385507971048355\n",
            "epoch 48 batch 64 loss 0.06105656921863556\n",
            "epoch 48 batch 65 loss 0.060510266572237015\n",
            "epoch 48 batch 66 loss 0.05492965877056122\n",
            "epoch 48 batch 67 loss 0.05921074002981186\n",
            "epoch 48 batch 68 loss 0.06058427318930626\n",
            "epoch 48 batch 69 loss 0.06100497022271156\n",
            "epoch 48 batch 70 loss 0.06018475815653801\n",
            "epoch 48 batch 71 loss 0.06081137806177139\n",
            "epoch 48 batch 72 loss 0.06356737017631531\n",
            "epoch 48 batch 73 loss 0.056474748998880386\n",
            "epoch 48 batch 74 loss 0.06427834928035736\n",
            "epoch 48 batch 75 loss 0.05922494828701019\n",
            "epoch 48 batch 76 loss 0.062135662883520126\n",
            "epoch 48 batch 77 loss 0.062465850263834\n",
            "epoch 48 batch 78 loss 0.05804882571101189\n",
            "epoch 48 batch 79 loss 0.062244344502687454\n",
            "epoch 48 batch 80 loss 0.05953631177544594\n",
            "epoch 48 batch 81 loss 0.051751509308815\n",
            "epoch 48 batch 82 loss 0.0669940784573555\n",
            "epoch 48 batch 83 loss 0.054201915860176086\n",
            "epoch 48 batch 84 loss 0.06677639484405518\n",
            "epoch 48 batch 85 loss 0.06247153505682945\n",
            "epoch 48 batch 86 loss 0.060836367309093475\n",
            "epoch 48 batch 87 loss 0.05593421310186386\n",
            "epoch 48 batch 88 loss 0.05643726512789726\n",
            "epoch 48 batch 89 loss 0.053397078067064285\n",
            "epoch 48 batch 90 loss 0.06272692233324051\n",
            "epoch 48 batch 91 loss 0.06252238154411316\n",
            "epoch 48 batch 92 loss 0.06292591243982315\n",
            "epoch 48 batch 93 loss 0.05858520045876503\n",
            "epoch 48 batch 94 loss 0.05814908817410469\n",
            "epoch 48 batch 95 loss 0.05894245207309723\n",
            "epoch 48 batch 96 loss 0.059520281851291656\n",
            "epoch 48 batch 97 loss 0.06225933879613876\n",
            "epoch 48 batch 98 loss 0.05795035511255264\n",
            "epoch 48 batch 99 loss 0.06000104919075966\n",
            "epoch 48 batch 100 loss 0.0657401755452156\n",
            "epoch 48 batch 101 loss 0.05947842448949814\n",
            "epoch 48 batch 102 loss 0.05960862338542938\n",
            "epoch 48 batch 103 loss 0.05443916842341423\n",
            "epoch 48 batch 104 loss 0.05925511568784714\n",
            "epoch 48 batch 105 loss 0.06597971171140671\n",
            "epoch 48 batch 106 loss 0.055185578763484955\n",
            "epoch 48 batch 107 loss 0.06197064742445946\n",
            "epoch 48 batch 108 loss 0.0655851885676384\n",
            "epoch 48 batch 109 loss 0.06473393738269806\n",
            "epoch 48 batch 110 loss 0.057042498141527176\n",
            "epoch 48 batch 111 loss 0.06347977370023727\n",
            "epoch 48 batch 112 loss 0.06341329962015152\n",
            "epoch 48 batch 113 loss 0.06068730726838112\n",
            "epoch 48 batch 114 loss 0.05571041256189346\n",
            "epoch 48 batch 115 loss 0.05705676227807999\n",
            "epoch 48 batch 116 loss 0.059808436781167984\n",
            "epoch 48 batch 117 loss 0.061217695474624634\n",
            "epoch 48 batch 118 loss 0.06407424062490463\n",
            "epoch 48 batch 119 loss 0.05606074631214142\n",
            "epoch 48 batch 120 loss 0.06498723477125168\n",
            "epoch 48 batch 121 loss 0.06001563370227814\n",
            "epoch 48 batch 122 loss 0.06633969396352768\n",
            "epoch 48 batch 123 loss 0.057112567126750946\n",
            "epoch 48 batch 124 loss 0.06394286453723907\n",
            "epoch 48 batch 125 loss 0.06096203997731209\n",
            "epoch 48 batch 126 loss 0.06103295087814331\n",
            "epoch 48 batch 127 loss 0.06308604031801224\n",
            "epoch 48 batch 128 loss 0.06041645631194115\n",
            "epoch 48 batch 129 loss 0.06054026260972023\n",
            "epoch 48 batch 130 loss 0.056928012520074844\n",
            "epoch 48 batch 131 loss 0.06749910116195679\n",
            "epoch 48 batch 132 loss 0.05549297854304314\n",
            "epoch 48 batch 133 loss 0.06155947223305702\n",
            "epoch 48 batch 134 loss 0.05952940136194229\n",
            "epoch 48 batch 135 loss 0.060894694179296494\n",
            "epoch 48 batch 136 loss 0.06328817456960678\n",
            "epoch 48 batch 137 loss 0.06292202323675156\n",
            "epoch 48 batch 138 loss 0.060757819563150406\n",
            "epoch 48 batch 139 loss 0.059349365532398224\n",
            "epoch 48 batch 140 loss 0.0606083944439888\n",
            "epoch 48 batch 141 loss 0.06541690975427628\n",
            "epoch 48 batch 142 loss 0.061803460121154785\n",
            "epoch 48 batch 143 loss 0.060196083039045334\n",
            "epoch 48 batch 144 loss 0.06430668383836746\n",
            "epoch 48 batch 145 loss 0.05995779111981392\n",
            "epoch 48 batch 146 loss 0.0608675517141819\n",
            "epoch 48 batch 147 loss 0.05469464510679245\n",
            "epoch 48 batch 148 loss 0.06081279739737511\n",
            "epoch 48 batch 149 loss 0.05765625461935997\n",
            "epoch 48 batch 150 loss 0.06055063381791115\n",
            "epoch 48 batch 151 loss 0.06106538698077202\n",
            "epoch 48 batch 152 loss 0.0631476566195488\n",
            "epoch 48 batch 153 loss 0.05777943879365921\n",
            "epoch 48 batch 154 loss 0.060635171830654144\n",
            "epoch 48 batch 155 loss 0.058978594839572906\n",
            "epoch 48 batch 156 loss 0.061400603502988815\n",
            "epoch 48 batch 157 loss 0.06452212482690811\n",
            "epoch 48 batch 158 loss 0.06170414760708809\n",
            "epoch 48 batch 159 loss 0.06254683434963226\n",
            "epoch 48 batch 160 loss 0.06179582700133324\n",
            "epoch 48 batch 161 loss 0.06273934245109558\n",
            "epoch 48 batch 162 loss 0.06353726983070374\n",
            "epoch 48 batch 163 loss 0.06224524974822998\n",
            "epoch 48 batch 164 loss 0.059122003614902496\n",
            "epoch 48 batch 165 loss 0.059266988188028336\n",
            "epoch 48 batch 166 loss 0.058505937457084656\n",
            "epoch 48 batch 167 loss 0.05936391279101372\n",
            "epoch 48 batch 168 loss 0.06111029163002968\n",
            "epoch 48 batch 169 loss 0.061733048409223557\n",
            "epoch 48 batch 170 loss 0.059722188860177994\n",
            "epoch 48 batch 171 loss 0.06028633192181587\n",
            "epoch 48 batch 172 loss 0.06336674839258194\n",
            "epoch 48 batch 173 loss 0.061457131057977676\n",
            "epoch 48 batch 174 loss 0.06338752806186676\n",
            "epoch 48 batch 175 loss 0.05800352618098259\n",
            "epoch 48 batch 176 loss 0.06015002727508545\n",
            "epoch 48 batch 177 loss 0.06229386478662491\n",
            "epoch 48 batch 178 loss 0.056992460042238235\n",
            "epoch 48 batch 179 loss 0.06457740813493729\n",
            "epoch 48 batch 180 loss 0.060703545808792114\n",
            "epoch 48 batch 181 loss 0.05880875885486603\n",
            "epoch 48 batch 182 loss 0.0593147911131382\n",
            "epoch 48 batch 183 loss 0.06033320724964142\n",
            "epoch 48 batch 184 loss 0.06433437764644623\n",
            "epoch 48 batch 185 loss 0.060889728367328644\n",
            "epoch 48 batch 186 loss 0.05880844220519066\n",
            "epoch 48 batch 187 loss 0.059183668345212936\n",
            "epoch 48 batch 188 loss 0.0668589174747467\n",
            "epoch 48 batch 189 loss 0.06457651406526566\n",
            "epoch 48 batch 190 loss 0.05984027683734894\n",
            "epoch 48 batch 191 loss 0.06781477481126785\n",
            "epoch 48 batch 192 loss 0.05550026521086693\n",
            "epoch 48 batch 193 loss 0.06446297466754913\n",
            "epoch 48 batch 194 loss 0.06510975956916809\n",
            "epoch 48 batch 195 loss 0.06095849350094795\n",
            "epoch 48 batch 196 loss 0.06023901700973511\n",
            "epoch 48 batch 197 loss 0.05873117968440056\n",
            "epoch 48 batch 198 loss 0.06390178948640823\n",
            "epoch 48 batch 199 loss 0.06146904453635216\n",
            "epoch 48 batch 200 loss 0.058617427945137024\n",
            "epoch 48 batch 201 loss 0.0626940056681633\n",
            "epoch 48 batch 202 loss 0.06361904740333557\n",
            "epoch 48 batch 203 loss 0.05848792567849159\n",
            "epoch 48 batch 204 loss 0.06365669518709183\n",
            "epoch 48 batch 205 loss 0.05449816957116127\n",
            "epoch 48 batch 206 loss 0.06443974375724792\n",
            "epoch 48 batch 207 loss 0.05900730937719345\n",
            "epoch 48 batch 208 loss 0.05995197221636772\n",
            "epoch 48 batch 209 loss 0.06417079269886017\n",
            "epoch 48 batch 210 loss 0.0648372620344162\n",
            "epoch 48 batch 211 loss 0.06608937680721283\n",
            "epoch 48 batch 212 loss 0.05969835817813873\n",
            "epoch 48 batch 213 loss 0.06326565891504288\n",
            "epoch 48 batch 214 loss 0.0653981938958168\n",
            "epoch 48 batch 215 loss 0.05534948408603668\n",
            "epoch 48 batch 216 loss 0.06180829182267189\n",
            "epoch 48 batch 217 loss 0.05997021868824959\n",
            "epoch 48 batch 218 loss 0.05832521617412567\n",
            "epoch 48 batch 219 loss 0.06413484364748001\n",
            "epoch 48 batch 220 loss 0.05805158615112305\n",
            "epoch 48 batch 221 loss 0.062332164496183395\n",
            "epoch 48 batch 222 loss 0.06563027948141098\n",
            "epoch 48 batch 223 loss 0.06457977741956711\n",
            "epoch 48 batch 224 loss 0.06139998883008957\n",
            "epoch 48 batch 225 loss 0.06076084077358246\n",
            "epoch 48 batch 226 loss 0.06641280651092529\n",
            "epoch 48 batch 227 loss 0.06285064667463303\n",
            "epoch 48 batch 228 loss 0.05888640135526657\n",
            "epoch 48 batch 229 loss 0.06109520420432091\n",
            "epoch 48 batch 230 loss 0.06825398653745651\n",
            "epoch 49 batch 0 loss 0.05781286209821701\n",
            "epoch 49 batch 1 loss 0.060987308621406555\n",
            "epoch 49 batch 2 loss 0.05813870579004288\n",
            "epoch 49 batch 3 loss 0.05607781186699867\n",
            "epoch 49 batch 4 loss 0.06136281415820122\n",
            "epoch 49 batch 5 loss 0.058827780187129974\n",
            "epoch 49 batch 6 loss 0.05662726238369942\n",
            "epoch 49 batch 7 loss 0.06011557951569557\n",
            "epoch 49 batch 8 loss 0.05968804657459259\n",
            "epoch 49 batch 9 loss 0.05699145793914795\n",
            "epoch 49 batch 10 loss 0.05525100603699684\n",
            "epoch 49 batch 11 loss 0.05703440308570862\n",
            "epoch 49 batch 12 loss 0.06351832300424576\n",
            "epoch 49 batch 13 loss 0.06140660122036934\n",
            "epoch 49 batch 14 loss 0.05613202229142189\n",
            "epoch 49 batch 15 loss 0.056120604276657104\n",
            "epoch 49 batch 16 loss 0.05907445773482323\n",
            "epoch 49 batch 17 loss 0.05878770351409912\n",
            "epoch 49 batch 18 loss 0.05870620161294937\n",
            "epoch 49 batch 19 loss 0.05371730774641037\n",
            "epoch 49 batch 20 loss 0.06070573627948761\n",
            "epoch 49 batch 21 loss 0.05270272120833397\n",
            "epoch 49 batch 22 loss 0.05716906115412712\n",
            "epoch 49 batch 23 loss 0.05620729550719261\n",
            "epoch 49 batch 24 loss 0.05970587953925133\n",
            "epoch 49 batch 25 loss 0.06308087706565857\n",
            "epoch 49 batch 26 loss 0.0645068809390068\n",
            "epoch 49 batch 27 loss 0.05731775984168053\n",
            "epoch 49 batch 28 loss 0.05558418855071068\n",
            "epoch 49 batch 29 loss 0.058898214250802994\n",
            "epoch 49 batch 30 loss 0.05865056440234184\n",
            "epoch 49 batch 31 loss 0.06151849776506424\n",
            "epoch 49 batch 32 loss 0.06129102036356926\n",
            "epoch 49 batch 33 loss 0.057286810129880905\n",
            "epoch 49 batch 34 loss 0.06420643627643585\n",
            "epoch 49 batch 35 loss 0.055686868727207184\n",
            "epoch 49 batch 36 loss 0.05270333215594292\n",
            "epoch 49 batch 37 loss 0.05952209606766701\n",
            "epoch 49 batch 38 loss 0.06468738615512848\n",
            "epoch 49 batch 39 loss 0.05974191799759865\n",
            "epoch 49 batch 40 loss 0.05472226440906525\n",
            "epoch 49 batch 41 loss 0.05851670727133751\n",
            "epoch 49 batch 42 loss 0.0567086860537529\n",
            "epoch 49 batch 43 loss 0.06049765273928642\n",
            "epoch 49 batch 44 loss 0.06380455940961838\n",
            "epoch 49 batch 45 loss 0.053708042949438095\n",
            "epoch 49 batch 46 loss 0.059311237186193466\n",
            "epoch 49 batch 47 loss 0.06201540678739548\n",
            "epoch 49 batch 48 loss 0.06208566203713417\n",
            "epoch 49 batch 49 loss 0.054224565625190735\n",
            "epoch 49 batch 50 loss 0.05910137668251991\n",
            "epoch 49 batch 51 loss 0.05887671932578087\n",
            "epoch 49 batch 52 loss 0.060700513422489166\n",
            "epoch 49 batch 53 loss 0.06441891938447952\n",
            "epoch 49 batch 54 loss 0.05983451381325722\n",
            "epoch 49 batch 55 loss 0.05478662997484207\n",
            "epoch 49 batch 56 loss 0.06042042374610901\n",
            "epoch 49 batch 57 loss 0.056935228407382965\n",
            "epoch 49 batch 58 loss 0.06068054214119911\n",
            "epoch 49 batch 59 loss 0.06299450248479843\n",
            "epoch 49 batch 60 loss 0.0636940449476242\n",
            "epoch 49 batch 61 loss 0.06244681775569916\n",
            "epoch 49 batch 62 loss 0.05967704951763153\n",
            "epoch 49 batch 63 loss 0.0592774897813797\n",
            "epoch 49 batch 64 loss 0.06104162707924843\n",
            "epoch 49 batch 65 loss 0.059902045875787735\n",
            "epoch 49 batch 66 loss 0.06663462519645691\n",
            "epoch 49 batch 67 loss 0.056618716567754745\n",
            "epoch 49 batch 68 loss 0.0619194470345974\n",
            "epoch 49 batch 69 loss 0.06302499771118164\n",
            "epoch 49 batch 70 loss 0.06272045522928238\n",
            "epoch 49 batch 71 loss 0.056442972272634506\n",
            "epoch 49 batch 72 loss 0.05842812359333038\n",
            "epoch 49 batch 73 loss 0.05964497849345207\n",
            "epoch 49 batch 74 loss 0.05494365096092224\n",
            "epoch 49 batch 75 loss 0.06367694586515427\n",
            "epoch 49 batch 76 loss 0.05653603374958038\n",
            "epoch 49 batch 77 loss 0.0628800243139267\n",
            "epoch 49 batch 78 loss 0.06125457212328911\n",
            "epoch 49 batch 79 loss 0.05904803425073624\n",
            "epoch 49 batch 80 loss 0.056884899735450745\n",
            "epoch 49 batch 81 loss 0.06033322587609291\n",
            "epoch 49 batch 82 loss 0.06119585782289505\n",
            "epoch 49 batch 83 loss 0.05614690110087395\n",
            "epoch 49 batch 84 loss 0.056979950517416\n",
            "epoch 49 batch 85 loss 0.0626092180609703\n",
            "epoch 49 batch 86 loss 0.060608118772506714\n",
            "epoch 49 batch 87 loss 0.05948977917432785\n",
            "epoch 49 batch 88 loss 0.0539681576192379\n",
            "epoch 49 batch 89 loss 0.05445178970694542\n",
            "epoch 49 batch 90 loss 0.06338021904230118\n",
            "epoch 49 batch 91 loss 0.07180450856685638\n",
            "epoch 49 batch 92 loss 0.06074279919266701\n",
            "epoch 49 batch 93 loss 0.06149831786751747\n",
            "epoch 49 batch 94 loss 0.06076480820775032\n",
            "epoch 49 batch 95 loss 0.05582717806100845\n",
            "epoch 49 batch 96 loss 0.05633438751101494\n",
            "epoch 49 batch 97 loss 0.05956932529807091\n",
            "epoch 49 batch 98 loss 0.06501323729753494\n",
            "epoch 49 batch 99 loss 0.06101599335670471\n",
            "epoch 49 batch 100 loss 0.06119867414236069\n",
            "epoch 49 batch 101 loss 0.06671955436468124\n",
            "epoch 49 batch 102 loss 0.06360111385583878\n",
            "epoch 49 batch 103 loss 0.06073642894625664\n",
            "epoch 49 batch 104 loss 0.0602044053375721\n",
            "epoch 49 batch 105 loss 0.05740858614444733\n",
            "epoch 49 batch 106 loss 0.06423524022102356\n",
            "epoch 49 batch 107 loss 0.0652119442820549\n",
            "epoch 49 batch 108 loss 0.058394767343997955\n",
            "epoch 49 batch 109 loss 0.05962338671088219\n",
            "epoch 49 batch 110 loss 0.057357560843229294\n",
            "epoch 49 batch 111 loss 0.05997689440846443\n",
            "epoch 49 batch 112 loss 0.06374014168977737\n",
            "epoch 49 batch 113 loss 0.05766770616173744\n",
            "epoch 49 batch 114 loss 0.06472427397966385\n",
            "epoch 49 batch 115 loss 0.05881083011627197\n",
            "epoch 49 batch 116 loss 0.06366436928510666\n",
            "epoch 49 batch 117 loss 0.06077246367931366\n",
            "epoch 49 batch 118 loss 0.06044362857937813\n",
            "epoch 49 batch 119 loss 0.05851617828011513\n",
            "epoch 49 batch 120 loss 0.05906549468636513\n",
            "epoch 49 batch 121 loss 0.05664037540555\n",
            "epoch 49 batch 122 loss 0.05982624739408493\n",
            "epoch 49 batch 123 loss 0.06370587646961212\n",
            "epoch 49 batch 124 loss 0.058343175798654556\n",
            "epoch 49 batch 125 loss 0.06406508386135101\n",
            "epoch 49 batch 126 loss 0.05780768021941185\n",
            "epoch 49 batch 127 loss 0.0691879615187645\n",
            "epoch 49 batch 128 loss 0.06331194192171097\n",
            "epoch 49 batch 129 loss 0.06341569125652313\n",
            "epoch 49 batch 130 loss 0.05432689189910889\n",
            "epoch 49 batch 131 loss 0.05583231523633003\n",
            "epoch 49 batch 132 loss 0.06512847542762756\n",
            "epoch 49 batch 133 loss 0.06122685596346855\n",
            "epoch 49 batch 134 loss 0.06210772320628166\n",
            "epoch 49 batch 135 loss 0.05729566514492035\n",
            "epoch 49 batch 136 loss 0.06545031815767288\n",
            "epoch 49 batch 137 loss 0.05979209393262863\n",
            "epoch 49 batch 138 loss 0.0653129294514656\n",
            "epoch 49 batch 139 loss 0.059513989835977554\n",
            "epoch 49 batch 140 loss 0.06168030574917793\n",
            "epoch 49 batch 141 loss 0.0606905035674572\n",
            "epoch 49 batch 142 loss 0.057242970913648605\n",
            "epoch 49 batch 143 loss 0.05847553163766861\n",
            "epoch 49 batch 144 loss 0.05815403163433075\n",
            "epoch 49 batch 145 loss 0.060094185173511505\n",
            "epoch 49 batch 146 loss 0.05788002535700798\n",
            "epoch 49 batch 147 loss 0.06113486737012863\n",
            "epoch 49 batch 148 loss 0.05884883180260658\n",
            "epoch 49 batch 149 loss 0.06253548711538315\n",
            "epoch 49 batch 150 loss 0.0659671276807785\n",
            "epoch 49 batch 151 loss 0.056615639477968216\n",
            "epoch 49 batch 152 loss 0.06174035370349884\n",
            "epoch 49 batch 153 loss 0.0597391314804554\n",
            "epoch 49 batch 154 loss 0.059584446251392365\n",
            "epoch 49 batch 155 loss 0.05762220919132233\n",
            "epoch 49 batch 156 loss 0.06066637858748436\n",
            "epoch 49 batch 157 loss 0.055650744587183\n",
            "epoch 49 batch 158 loss 0.059141725301742554\n",
            "epoch 49 batch 159 loss 0.0568883903324604\n",
            "epoch 49 batch 160 loss 0.05501271411776543\n",
            "epoch 49 batch 161 loss 0.06200570985674858\n",
            "epoch 49 batch 162 loss 0.06169183552265167\n",
            "epoch 49 batch 163 loss 0.06109624356031418\n",
            "epoch 49 batch 164 loss 0.060527827590703964\n",
            "epoch 49 batch 165 loss 0.06323736906051636\n",
            "epoch 49 batch 166 loss 0.061311352998018265\n",
            "epoch 49 batch 167 loss 0.06257404386997223\n",
            "epoch 49 batch 168 loss 0.06354600936174393\n",
            "epoch 49 batch 169 loss 0.056803397834300995\n",
            "epoch 49 batch 170 loss 0.05757860094308853\n",
            "epoch 49 batch 171 loss 0.05791359022259712\n",
            "epoch 49 batch 172 loss 0.060114599764347076\n",
            "epoch 49 batch 173 loss 0.059940192848443985\n",
            "epoch 49 batch 174 loss 0.05999406799674034\n",
            "epoch 49 batch 175 loss 0.0581093393266201\n",
            "epoch 49 batch 176 loss 0.058978550136089325\n",
            "epoch 49 batch 177 loss 0.06861233711242676\n",
            "epoch 49 batch 178 loss 0.06191849336028099\n",
            "epoch 49 batch 179 loss 0.05299034342169762\n",
            "epoch 49 batch 180 loss 0.0591704286634922\n",
            "epoch 49 batch 181 loss 0.0663418099284172\n",
            "epoch 49 batch 182 loss 0.0648164451122284\n",
            "epoch 49 batch 183 loss 0.06229424849152565\n",
            "epoch 49 batch 184 loss 0.062066588550806046\n",
            "epoch 49 batch 185 loss 0.06378373503684998\n",
            "epoch 49 batch 186 loss 0.05888891965150833\n",
            "epoch 49 batch 187 loss 0.06445325165987015\n",
            "epoch 49 batch 188 loss 0.06660307198762894\n",
            "epoch 49 batch 189 loss 0.062360987067222595\n",
            "epoch 49 batch 190 loss 0.06231851503252983\n",
            "epoch 49 batch 191 loss 0.05666996166110039\n",
            "epoch 49 batch 192 loss 0.06261606514453888\n",
            "epoch 49 batch 193 loss 0.057792942970991135\n",
            "epoch 49 batch 194 loss 0.06378249824047089\n",
            "epoch 49 batch 195 loss 0.06277251988649368\n",
            "epoch 49 batch 196 loss 0.06271543353796005\n",
            "epoch 49 batch 197 loss 0.06767436861991882\n",
            "epoch 49 batch 198 loss 0.06017863750457764\n",
            "epoch 49 batch 199 loss 0.0599554069340229\n",
            "epoch 49 batch 200 loss 0.05870622768998146\n",
            "epoch 49 batch 201 loss 0.06959899514913559\n",
            "epoch 49 batch 202 loss 0.06616707891225815\n",
            "epoch 49 batch 203 loss 0.057155247777700424\n",
            "epoch 49 batch 204 loss 0.060721732676029205\n",
            "epoch 49 batch 205 loss 0.05302939936518669\n",
            "epoch 49 batch 206 loss 0.064059779047966\n",
            "epoch 49 batch 207 loss 0.06808274239301682\n",
            "epoch 49 batch 208 loss 0.06505703926086426\n",
            "epoch 49 batch 209 loss 0.05651984363794327\n",
            "epoch 49 batch 210 loss 0.059913769364356995\n",
            "epoch 49 batch 211 loss 0.06708361208438873\n",
            "epoch 49 batch 212 loss 0.0640064924955368\n",
            "epoch 49 batch 213 loss 0.05760113522410393\n",
            "epoch 49 batch 214 loss 0.054858073592185974\n",
            "epoch 49 batch 215 loss 0.06314738839864731\n",
            "epoch 49 batch 216 loss 0.06106579303741455\n",
            "epoch 49 batch 217 loss 0.06951334327459335\n",
            "epoch 49 batch 218 loss 0.06474380195140839\n",
            "epoch 49 batch 219 loss 0.05942009016871452\n",
            "epoch 49 batch 220 loss 0.0669216588139534\n",
            "epoch 49 batch 221 loss 0.06098035350441933\n",
            "epoch 49 batch 222 loss 0.06092451140284538\n",
            "epoch 49 batch 223 loss 0.0562414675951004\n",
            "epoch 49 batch 224 loss 0.07008147239685059\n",
            "epoch 49 batch 225 loss 0.06152103468775749\n",
            "epoch 49 batch 226 loss 0.060759175568819046\n",
            "epoch 49 batch 227 loss 0.0634067952632904\n",
            "epoch 49 batch 228 loss 0.06257926672697067\n",
            "epoch 49 batch 229 loss 0.06651128083467484\n",
            "epoch 49 batch 230 loss 0.07335883378982544\n",
            "-------------------start evaluating-------------------\n",
            "Accuracy:  97.64598846435547\n"
          ]
        }
      ]
    }
  ]
}