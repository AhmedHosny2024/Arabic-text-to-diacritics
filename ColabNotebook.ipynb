{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DV7fmwYwVX_",
        "outputId": "d331f997-1157-42d0-dff4-2d1fa59a01c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "u2nZg8-K24xf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import re\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "# import stanfordnlp\n",
        "\n",
        "# Download the Arabic models for the neural pipeline\n",
        "# stanfordnlp.download('ar', force=True)\n",
        "# Build a neural pipeline using the Arabic models\n",
        "# nlp = stanfordnlp.Pipeline(lang='ar')\n",
        "\n",
        "# def split_arabic_sentences_with_stanfordnlp(corpus_text):\n",
        "#     # Process the text\n",
        "#     doc = nlp(corpus_text)\n",
        "\n",
        "#     # Extract sentences from the doc\n",
        "#     sentences = [sentence.text for sentence in doc.sentences]\n",
        "\n",
        "#     return sentences\n",
        "\n",
        "max_len=600\n",
        "\n",
        "with open('/content/drive/MyDrive/NLP project/Arabic-text-to-diacritics/files/arabic_letters.pickle', 'rb') as file:\n",
        "            ARABIC_LETTERS_LIST = pkl.load(file)\n",
        "with open('/content/drive/MyDrive/NLP project/Arabic-text-to-diacritics/files/diacritics.pickle', 'rb') as file:\n",
        "            DIACRITICS_LIST = pkl.load(file)\n",
        "\n",
        "arabic_letters=[]\n",
        "for letter in ARABIC_LETTERS_LIST:\n",
        "    arabic_letters.append(letter[0])\n",
        "arabic_letters.append(\" \")\n",
        "\n",
        "dicritics=[]\n",
        "for letter in DIACRITICS_LIST:\n",
        "    dicritics.append(letter[0])\n",
        "\n",
        "classes = {\n",
        "    'َ': 0,\n",
        "    'ُ': 1,\n",
        "    'ِ': 2,\n",
        "    'ْ': 3,\n",
        "    'ّ': 4,\n",
        "    'ً': 5,\n",
        "    'ٌ': 6,\n",
        "    'ٍ': 7,\n",
        "    'َّ': 8,\n",
        "    'ُّ': 9,\n",
        "    'ِّ': 10,\n",
        "    'ًّ': 11,\n",
        "    'ٌّ': 12,\n",
        "    'ٍّ': 13,\n",
        "    'ّّ': 14,\n",
        "    \"\":15,\n",
        "}\n",
        "\n",
        "inverted_classes = {v: k for k, v in classes.items()}\n",
        "\n",
        "##################### to delete #####################\n",
        "def read_text(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        return file.read()\n",
        "\n",
        "def write_to_file_second(dirctory,file_path, text):\n",
        "    if not os.path.exists(dirctory):\n",
        "        os.makedirs(dirctory)\n",
        "\n",
        "    with open(dirctory+\"/\"+file_path, \"a\", encoding=\"utf-8\") as file:\n",
        "        lines=text\n",
        "        for l in lines:\n",
        "            file.write(l)\n",
        "        file.write('\\n')\n",
        "def write_to_file_labels(dirctory,file_path, text):\n",
        "    if not os.path.exists(dirctory):\n",
        "        os.makedirs(dirctory)\n",
        "\n",
        "    with open(dirctory+\"/\"+file_path, \"a\", encoding=\"utf-8\") as file:\n",
        "        lines=text\n",
        "        for l in lines:\n",
        "            file.write(inverted_classes[l])\n",
        "        file.write('\\n')\n",
        "\n",
        "def write_to_file_string(dirctory,file_path, text):\n",
        "    if not os.path.exists(dirctory):\n",
        "        os.makedirs(dirctory)\n",
        "\n",
        "    with open(dirctory+\"/\"+file_path, \"a\", encoding=\"utf-8\") as file:\n",
        "        lines=text\n",
        "        file.write(lines)\n",
        "        file.write('\\n')\n",
        "#####################################################\n",
        "\n",
        "def preprocess(text):\n",
        "         # Remove URLs\n",
        "        text = re.sub(r\"http[s|S]\\S+\", \"\", text,flags=re.MULTILINE)\n",
        "        text = re.sub(r\"www\\S+\", \"\", text,flags=re.MULTILINE)\n",
        "        # Remove English letters\n",
        "        text = re.sub(r\"[A-Za-z]+\", \"\", text,flags=re.MULTILINE)\n",
        "        # Remove Kashida Arabic character\n",
        "        text = re.sub(r\"\\u0640\", \"\", text,flags=re.MULTILINE)\n",
        "        # Add space before and after the numbers\n",
        "        text = re.sub(r\"(\\d+)\", r\" \\1 \", text,flags=re.MULTILINE)\n",
        "        # removes SHIFT+J Arabic character\n",
        "        text = re.sub(r\"\\u0691\", \"\", text,flags=re.MULTILINE)\n",
        "        # remove english numbers\n",
        "        text = re.sub(r\"[0-9]+\", \"\", text,flags=re.MULTILINE)\n",
        "        # remove arabic numbers\n",
        "        text = re.sub(r\"[٠-٩]+\", \"\", text,flags=re.MULTILINE)\n",
        "         # remove brackets\n",
        "        # text = re.sub(r\"\\[.*?\\]\", \"\", text)\n",
        "        # text = re.sub(r\"\\(.*?\\)\", \"\", text)\n",
        "        return text\n",
        "\n",
        "def split_text(text):\n",
        "    text=text.split('.')\n",
        "    # split text to sentences on all arabic sparatators\n",
        "\n",
        "    data=[]\n",
        "    for t in text:\n",
        "        if(len(t)==0): continue\n",
        "        if(len(t)<max_len):\n",
        "            while(len(t)<max_len):\n",
        "                 t+=\" \"\n",
        "            data.append(t)\n",
        "        if(len(t)>max_len):\n",
        "            data.append(t[:max_len])\n",
        "            supdata=t[max_len:]\n",
        "            while(len(supdata)>max_len):\n",
        "                data.append(supdata[:max_len])\n",
        "                supdata=supdata[max_len:]\n",
        "            if(len(supdata)<max_len):\n",
        "                while(len(supdata)<max_len):\n",
        "                    supdata+=\" \"\n",
        "                data.append(supdata)\n",
        "    return data\n",
        "\n",
        "def get_data_labels(text):\n",
        "    data=\"\"\n",
        "    labels=[]\n",
        "    for i in range(len(text)):\n",
        "            if(text[i] in arabic_letters and text[i]):\n",
        "                data+=(text[i])\n",
        "                if(i+1<len(text) and text[i+1] in dicritics):\n",
        "                    if(i+2<len(text) and classes[text[i+1]]==4 and text[i+2] in dicritics):\n",
        "                        labels.append(classes[text[i+1]+text[i+2]])\n",
        "                        i+=2\n",
        "                    else:\n",
        "                        labels.append(classes[text[i+1]])\n",
        "                        i+=1\n",
        "                else:\n",
        "                    labels.append(15)\n",
        "    return data,labels\n",
        "\n",
        "# def one_hot_encoding(text):\n",
        "#     onehot_encoded=[]\n",
        "#     for i in range(len(text)):\n",
        "#          if text[i] in arabic_letters:\n",
        "#             idx=arabic_letters.index(text[i])\n",
        "#             encode=np.zeros(len(arabic_letters))\n",
        "#             encode[idx]=1\n",
        "#             onehot_encoded.append(encode)\n",
        "#     onehot_encoded=torch.tensor(onehot_encoded)\n",
        "#     return onehot_encoded\n",
        "\n",
        "def encoding(text):\n",
        "    idx=arabic_letters.index(text)\n",
        "    encode=np.zeros(len(arabic_letters))\n",
        "    encode[idx]=1\n",
        "    return torch.tensor(encode,dtype=torch.float32).to(device)\n",
        "\n",
        "\n",
        "def get_dataloader(encoded_data, encoding_labels,batch_size=1):\n",
        "    # Create TensorDataset\n",
        "    dataset = TensorDataset(encoded_data, encoding_labels)\n",
        "    # Create DataLoader\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "def get_validation():\n",
        "    text=read_text('/content/drive/MyDrive/NLP project/Arabic-text-to-diacritics/Dataset/val.txt')\n",
        "    text=preprocess(text)\n",
        "    # size=int(0.02*len(text))\n",
        "    # text=text[:size]\n",
        "    # text=split_text(text)\n",
        "    # write_to_file_second(\"test\",\"data.txt\",text)\n",
        "    # text=split_text(text)\n",
        "    data=[]\n",
        "    labels=[]\n",
        "    for i in range (len(text)):\n",
        "        if(text[i] in arabic_letters):\n",
        "                data+=(text[i])\n",
        "                if(i+1<len(text) and text[i+1] in dicritics):\n",
        "                    if(i+2<len(text) and classes[text[i+1]]==4 and text[i+2] in dicritics):\n",
        "                        labels.append(classes[text[i+1]+text[i+2]])\n",
        "                        i+=2\n",
        "                    else:\n",
        "                        labels.append(classes[text[i+1]])\n",
        "                        i+=1\n",
        "                else:\n",
        "                    labels.append(15)\n",
        "        # else:\n",
        "        #     data.append(text[i])\n",
        "        #     labels.append(15)\n",
        "    encoded_data = torch.empty(0, len(arabic_letters),dtype=torch.float32).to(device)\n",
        "\n",
        "    for letter in data:\n",
        "        if letter in arabic_letters:\n",
        "            x = encoding(letter).unsqueeze(0)\n",
        "        else:\n",
        "            x=np.zeros((1,len(arabic_letters)))\n",
        "            x=torch.tensor(x,dtype=torch.float32).to(device)\n",
        "        encoded_data = torch.cat((encoded_data, x), 0)\n",
        "    labels=torch.tensor(labels,dtype=torch.long).to(device)\n",
        "    # print(encoded_data.shape)\n",
        "    # print(labels.shape)\n",
        "    dataloader=get_dataloader(encoded_data,labels)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "def get_data(path):\n",
        "    text=read_text(path)\n",
        "    text=preprocess(text)\n",
        "    # size=int(0.02*len(text))\n",
        "    # text=text[:size]\n",
        "    print(len(text))\n",
        "\n",
        "    text=split_text(text)\n",
        "\n",
        "    # write_to_file_second(\"test\",\"data.txt\",text)\n",
        "    # # get max length of sentence in text\n",
        "    # maxdata=text.split('\\n')\n",
        "\n",
        "    # max_len=0\n",
        "    # for t in maxdata:\n",
        "    #     if(len(t)>max_len):\n",
        "    #         max_len=len(t)\n",
        "    # print(max_len)\n",
        "\n",
        "    # split text to sentences on all arabic sparatators\n",
        "    # text = split_arabic_sentences_with_stanfordnlp(text)\n",
        "\n",
        "    # Filter out empty strings or whitespace-only sentences\n",
        "    # text = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "    data=[]\n",
        "    labels=[]\n",
        "    for t in text:\n",
        "        d=\"\"\n",
        "        l=[]\n",
        "        d,l=get_data_labels(t)\n",
        "        if(len(d)<max_len):\n",
        "            while(len(d)<max_len):\n",
        "                d+=(\" \")\n",
        "                l.append(15)\n",
        "        else:\n",
        "            d=d[:max_len]\n",
        "            l=l[:max_len]\n",
        "        data.append(d)\n",
        "        labels.append(l)\n",
        "    return data,labels\n",
        "\n",
        "def get_features(data,labels):\n",
        "    encoded_data = torch.empty(0, max_len, len(arabic_letters),dtype=torch.float32).to(device)\n",
        "    for d in data:\n",
        "        enc = torch.empty(0, len(arabic_letters),dtype=torch.float32).to(device)\n",
        "        for letter in d:\n",
        "            x = encoding(letter).unsqueeze(0)\n",
        "            enc = torch.cat((enc, x), 0)\n",
        "        encoded_data = torch.cat((encoded_data, enc.unsqueeze(0)), 0)\n",
        "    # print(encoded_data.shape)\n",
        "    encoding_labels=torch.tensor(labels,dtype=torch.long).to(device)\n",
        "    # print(encoding_labels.shape)\n",
        "    return encoded_data,encoding_labels\n",
        "\n",
        "class DataSet():\n",
        "\n",
        "    def __init__(self,path,batch_size=1) :\n",
        "        print(\"Loading data...\")\n",
        "        data,labels=get_data(path)\n",
        "        # now labels is list of list [[1,2,3,4,5,15,15,0],[1,2,3,4,5,15,15,0]]\n",
        "        # data is list of string ['احمد','محمد']\n",
        "        print(\"Extracting features...\")\n",
        "        data,labels=get_features(data,labels)\n",
        "        # now the data and labels are tensor\n",
        "        # data is tensor of shape (number of sentences,600,37)\n",
        "        # labels is tensor of shape (number of sentences,600)\n",
        "        print(\"Creating dataloader...\")\n",
        "        dataloader=get_dataloader(data,labels,batch_size)\n",
        "        self.x=data\n",
        "        self.y=labels\n",
        "        self.dataloader=dataloader\n",
        "        print(\"Done data creation !\")\n",
        "\n",
        "    def __len__(self):\n",
        "         return len(self.y)\n",
        "    def item(self,idx):\n",
        "         return encoding(self.x[idx]),self.y[idx]\n",
        "    def getdata(self):\n",
        "        return self.dataloader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "C5LvCIPR2eVx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, inp_vocab_size: int, hidden_dim: int = 256, seq_len: int = 600, num_classes: int = 16):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(inp_vocab_size, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # Output layer for 0 to 16 integers\n",
        "\n",
        "    def forward(self, input_sequence: torch.Tensor):\n",
        "        output, _ = self.lstm(input_sequence)\n",
        "        output = self.fc(output.reshape(-1, output.size(-1)))\n",
        "        return output\n",
        "\n",
        "def train(train_dl, model):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    for epoch in range(20):\n",
        "        for i, (inputs, targets) in enumerate(train_dl):\n",
        "            optimizer.zero_grad()\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            yhat = model(inputs)\n",
        "            yhat = yhat.view(inputs.size(0), inputs.size(1), -1)\n",
        "            yhat = yhat.view(-1, yhat.size(2))\n",
        "            targets = targets.view(-1)\n",
        "            loss = criterion(yhat, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            print(f'epoch {epoch} batch {i} loss {loss.item()}')\n",
        "    torch.save(model.state_dict(), 'model.pth')\n",
        "\n",
        "def calculate_DER(actual_labels, predicted_labels):\n",
        "    if not isinstance(actual_labels, torch.Tensor):\n",
        "        actual_labels = torch.tensor(actual_labels)\n",
        "    if not isinstance(predicted_labels, torch.Tensor):\n",
        "        predicted_labels = torch.tensor(predicted_labels)\n",
        "\n",
        "    if len(actual_labels) != len(predicted_labels):\n",
        "        raise ValueError(\"Lengths of actual and predicted labels should match.\")\n",
        "\n",
        "    total_errors = torch.sum(actual_labels != predicted_labels)\n",
        "    total_frames = len(actual_labels)\n",
        "    DER = (1-(total_errors / total_frames)) * 100.0\n",
        "    return DER.item()\n",
        "\n",
        "def evaluate_model(test_dl, model):\n",
        "    predictions, actuals = [], []\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    for i, (inputs, targets) in enumerate(test_dl):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        yhat = model(inputs)\n",
        "        yhat = yhat.detach().cpu().numpy()\n",
        "        yhat = yhat.reshape(-1, yhat.shape[-1])\n",
        "        predicted_classes = np.argmax(yhat, axis=1)\n",
        "        targets = targets.cpu().numpy().reshape(-1)\n",
        "        predictions.extend(predicted_classes.tolist())\n",
        "        actuals.extend(targets.tolist())\n",
        "    acc = calculate_DER(np.array(actuals), np.array(predictions))\n",
        "    return acc\n"
      ],
      "metadata": {
        "id": "qhy617nk2VPI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from data import DataSet, get_validation\n",
        "# from model import LSTM, train, evaluate_model\n",
        "\n",
        "Traindata = DataSet(\"/content/drive/MyDrive/NLP project/Arabic-text-to-diacritics/Dataset/val.txt\", batch_size=256)\n",
        "Traindataloader = Traindata.getdata()\n",
        "Validationdataloader=get_validation()\n",
        "\n",
        "inp_vocab_size = 37\n",
        "hidden_dim = 256\n",
        "seq_len = 600\n",
        "num_classes = 16\n",
        "\n",
        "model = LSTM(inp_vocab_size, hidden_dim, seq_len, num_classes).to(device)\n",
        "print(\"-------------------start training-------------------\")\n",
        "train(Traindataloader, model)\n",
        "\n",
        "# Validation=DataSet('Dataset/val.txt',batch_size=1)\n",
        "# Validationdataloader=Validation.getdata()\n",
        "print(\"-------------------start evaluating-------------------\")\n",
        "acc = evaluate_model(Validationdataloader, model)\n",
        "print(\"Accuracy: \", acc)\n",
        "\n",
        "# validation=get_validation()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqgXGly72RLu",
        "outputId": "1023343e-342e-4c83-d175-b051c525cd35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "933760\n",
            "Extracting features...\n",
            "Creating dataloader...\n",
            "Done data creation !\n",
            "-------------------start training-------------------\n",
            "epoch 0 batch 0 loss 2.796307325363159\n",
            "epoch 0 batch 1 loss 2.73392653465271\n",
            "epoch 0 batch 2 loss 2.667786121368408\n",
            "epoch 0 batch 3 loss 2.5993845462799072\n",
            "epoch 0 batch 4 loss 2.5216524600982666\n",
            "epoch 0 batch 5 loss 2.417729139328003\n",
            "epoch 0 batch 6 loss 2.2765519618988037\n",
            "epoch 0 batch 7 loss 2.0500404834747314\n",
            "epoch 0 batch 8 loss 1.5534729957580566\n",
            "epoch 0 batch 9 loss 0.8863129615783691\n",
            "epoch 0 batch 10 loss 1.0943711996078491\n",
            "epoch 1 batch 0 loss 1.1191633939743042\n",
            "epoch 1 batch 1 loss 0.9341135621070862\n",
            "epoch 1 batch 2 loss 0.8137668967247009\n",
            "epoch 1 batch 3 loss 0.8455584645271301\n",
            "epoch 1 batch 4 loss 0.8830914497375488\n",
            "epoch 1 batch 5 loss 0.8690981864929199\n",
            "epoch 1 batch 6 loss 0.8569626808166504\n",
            "epoch 1 batch 7 loss 0.8410628437995911\n",
            "epoch 1 batch 8 loss 0.8376221060752869\n",
            "epoch 1 batch 9 loss 0.7535815834999084\n",
            "epoch 1 batch 10 loss 0.8029239177703857\n",
            "epoch 2 batch 0 loss 0.845377504825592\n",
            "epoch 2 batch 1 loss 0.8026988506317139\n",
            "epoch 2 batch 2 loss 0.7498432397842407\n",
            "epoch 2 batch 3 loss 0.7794746160507202\n",
            "epoch 2 batch 4 loss 0.8034423589706421\n",
            "epoch 2 batch 5 loss 0.7779170870780945\n",
            "epoch 2 batch 6 loss 0.7649377584457397\n",
            "epoch 2 batch 7 loss 0.7587831616401672\n",
            "epoch 2 batch 8 loss 0.7696638703346252\n",
            "epoch 2 batch 9 loss 0.6989952921867371\n",
            "epoch 2 batch 10 loss 0.749107301235199\n",
            "epoch 3 batch 0 loss 0.7782688140869141\n",
            "epoch 3 batch 1 loss 0.7297986149787903\n",
            "epoch 3 batch 2 loss 0.6723737716674805\n",
            "epoch 3 batch 3 loss 0.6870930194854736\n",
            "epoch 3 batch 4 loss 0.6901087760925293\n",
            "epoch 3 batch 5 loss 0.6480274200439453\n",
            "epoch 3 batch 6 loss 0.6124850511550903\n",
            "epoch 3 batch 7 loss 0.5899708271026611\n",
            "epoch 3 batch 8 loss 0.5918554067611694\n",
            "epoch 3 batch 9 loss 0.5253909230232239\n",
            "epoch 3 batch 10 loss 0.581736147403717\n",
            "epoch 4 batch 0 loss 0.6342081427574158\n",
            "epoch 4 batch 1 loss 0.6359390020370483\n",
            "epoch 4 batch 2 loss 0.8593471050262451\n",
            "epoch 4 batch 3 loss 0.5759788155555725\n",
            "epoch 4 batch 4 loss 0.6180780529975891\n",
            "epoch 4 batch 5 loss 0.6530722975730896\n",
            "epoch 4 batch 6 loss 0.5708408355712891\n",
            "epoch 4 batch 7 loss 0.5648052096366882\n",
            "epoch 4 batch 8 loss 0.5808578729629517\n",
            "epoch 4 batch 9 loss 0.5224061012268066\n",
            "epoch 4 batch 10 loss 0.5790441632270813\n",
            "epoch 5 batch 0 loss 0.619414746761322\n",
            "epoch 5 batch 1 loss 0.5833536982536316\n",
            "epoch 5 batch 2 loss 0.542157769203186\n",
            "epoch 5 batch 3 loss 0.5652701258659363\n",
            "epoch 5 batch 4 loss 0.5861600041389465\n",
            "epoch 5 batch 5 loss 0.5668697953224182\n",
            "epoch 5 batch 6 loss 0.560273289680481\n",
            "epoch 5 batch 7 loss 0.5586514472961426\n",
            "epoch 5 batch 8 loss 0.5696715116500854\n",
            "epoch 5 batch 9 loss 0.5040788054466248\n",
            "epoch 5 batch 10 loss 0.5553269386291504\n",
            "epoch 6 batch 0 loss 0.5930605530738831\n",
            "epoch 6 batch 1 loss 0.5552725791931152\n",
            "epoch 6 batch 2 loss 0.5141410231590271\n",
            "epoch 6 batch 3 loss 0.5437117218971252\n",
            "epoch 6 batch 4 loss 0.5611292123794556\n",
            "epoch 6 batch 5 loss 0.5443812608718872\n",
            "epoch 6 batch 6 loss 0.5402557253837585\n",
            "epoch 6 batch 7 loss 0.5403078198432922\n",
            "epoch 6 batch 8 loss 0.5520707368850708\n",
            "epoch 6 batch 9 loss 0.48833465576171875\n",
            "epoch 6 batch 10 loss 0.540431022644043\n",
            "epoch 7 batch 0 loss 0.5793905258178711\n",
            "epoch 7 batch 1 loss 0.5429807305335999\n",
            "epoch 7 batch 2 loss 0.5025724172592163\n",
            "epoch 7 batch 3 loss 0.5275368094444275\n",
            "epoch 7 batch 4 loss 0.550127387046814\n",
            "epoch 7 batch 5 loss 0.533982515335083\n",
            "epoch 7 batch 6 loss 0.5299857258796692\n",
            "epoch 7 batch 7 loss 0.5298660397529602\n",
            "epoch 7 batch 8 loss 0.5421667695045471\n",
            "epoch 7 batch 9 loss 0.4794175326824188\n",
            "epoch 7 batch 10 loss 0.5314204096794128\n",
            "epoch 8 batch 0 loss 0.5700938701629639\n",
            "epoch 8 batch 1 loss 0.5339197516441345\n",
            "epoch 8 batch 2 loss 0.4941733181476593\n",
            "epoch 8 batch 3 loss 0.5192255973815918\n",
            "epoch 8 batch 4 loss 0.5417735576629639\n",
            "epoch 8 batch 5 loss 0.5256001949310303\n",
            "epoch 8 batch 6 loss 0.5216901898384094\n",
            "epoch 8 batch 7 loss 0.5215420722961426\n",
            "epoch 8 batch 8 loss 0.5339875817298889\n",
            "epoch 8 batch 9 loss 0.47193044424057007\n",
            "epoch 8 batch 10 loss 0.5234031081199646\n",
            "epoch 9 batch 0 loss 0.5616809129714966\n",
            "epoch 9 batch 1 loss 0.5255586504936218\n",
            "epoch 9 batch 2 loss 0.4862403869628906\n",
            "epoch 9 batch 3 loss 0.5109776854515076\n",
            "epoch 9 batch 4 loss 0.5332632064819336\n",
            "epoch 9 batch 5 loss 0.5170714259147644\n",
            "epoch 9 batch 6 loss 0.5131199955940247\n",
            "epoch 9 batch 7 loss 0.5127933025360107\n",
            "epoch 9 batch 8 loss 0.5250242948532104\n",
            "epoch 9 batch 9 loss 0.463737428188324\n",
            "epoch 9 batch 10 loss 0.5142908096313477\n",
            "epoch 10 batch 0 loss 0.5519355535507202\n",
            "epoch 10 batch 1 loss 0.5159710049629211\n",
            "epoch 10 batch 2 loss 0.47708994150161743\n",
            "epoch 10 batch 3 loss 0.5014156699180603\n",
            "epoch 10 batch 4 loss 0.5230879187583923\n",
            "epoch 10 batch 5 loss 0.5068758130073547\n",
            "epoch 10 batch 6 loss 0.502758264541626\n",
            "epoch 10 batch 7 loss 0.502138078212738\n",
            "epoch 10 batch 8 loss 0.5140321850776672\n",
            "epoch 10 batch 9 loss 0.4536188840866089\n",
            "epoch 10 batch 10 loss 0.502949595451355\n",
            "epoch 11 batch 0 loss 0.53960782289505\n",
            "epoch 11 batch 1 loss 0.503973662853241\n",
            "epoch 11 batch 2 loss 0.4656003713607788\n",
            "epoch 11 batch 3 loss 0.48932620882987976\n",
            "epoch 11 batch 4 loss 0.5100530982017517\n",
            "epoch 11 batch 5 loss 0.49400487542152405\n",
            "epoch 11 batch 6 loss 0.48966866731643677\n",
            "epoch 11 batch 7 loss 0.48863738775253296\n",
            "epoch 11 batch 8 loss 0.49992644786834717\n",
            "epoch 11 batch 9 loss 0.4406593441963196\n",
            "epoch 11 batch 10 loss 0.4883124530315399\n",
            "epoch 12 batch 0 loss 0.523776113986969\n",
            "epoch 12 batch 1 loss 0.4886162281036377\n",
            "epoch 12 batch 2 loss 0.45088014006614685\n",
            "epoch 12 batch 3 loss 0.47369205951690674\n",
            "epoch 12 batch 4 loss 0.4932997226715088\n",
            "epoch 12 batch 5 loss 0.4774337112903595\n",
            "epoch 12 batch 6 loss 0.47286534309387207\n",
            "epoch 12 batch 7 loss 0.4713888466358185\n",
            "epoch 12 batch 8 loss 0.4819205105304718\n",
            "epoch 12 batch 9 loss 0.4242834746837616\n",
            "epoch 12 batch 10 loss 0.46980413794517517\n",
            "epoch 13 batch 0 loss 0.5037322044372559\n",
            "epoch 13 batch 1 loss 0.46925410628318787\n",
            "epoch 13 batch 2 loss 0.43238699436187744\n",
            "epoch 13 batch 3 loss 0.4542170763015747\n",
            "epoch 13 batch 4 loss 0.47239455580711365\n",
            "epoch 13 batch 5 loss 0.4569040536880493\n",
            "epoch 13 batch 6 loss 0.45220398902893066\n",
            "epoch 13 batch 7 loss 0.4503132998943329\n",
            "epoch 13 batch 8 loss 0.46000799536705017\n",
            "epoch 13 batch 9 loss 0.4045121967792511\n",
            "epoch 13 batch 10 loss 0.4476638734340668\n",
            "epoch 14 batch 0 loss 0.4799725413322449\n",
            "epoch 14 batch 1 loss 0.446504682302475\n",
            "epoch 14 batch 2 loss 0.41094285249710083\n",
            "epoch 14 batch 3 loss 0.43207213282585144\n",
            "epoch 14 batch 4 loss 0.44885018467903137\n",
            "epoch 14 batch 5 loss 0.4340384006500244\n",
            "epoch 14 batch 6 loss 0.42964375019073486\n",
            "epoch 14 batch 7 loss 0.42767342925071716\n",
            "epoch 14 batch 8 loss 0.43680551648139954\n",
            "epoch 14 batch 9 loss 0.38393667340278625\n",
            "epoch 14 batch 10 loss 0.4248046875\n",
            "epoch 15 batch 0 loss 0.45590993762016296\n",
            "epoch 15 batch 1 loss 0.42374393343925476\n",
            "epoch 15 batch 2 loss 0.38976991176605225\n",
            "epoch 15 batch 3 loss 0.4105680286884308\n",
            "epoch 15 batch 4 loss 0.4260857105255127\n",
            "epoch 15 batch 5 loss 0.41206663846969604\n",
            "epoch 15 batch 6 loss 0.4081675708293915\n",
            "epoch 15 batch 7 loss 0.40606969594955444\n",
            "epoch 15 batch 8 loss 0.41512376070022583\n",
            "epoch 15 batch 9 loss 0.36442509293556213\n",
            "epoch 15 batch 10 loss 0.4033767282962799\n",
            "epoch 16 batch 0 loss 0.43322134017944336\n",
            "epoch 16 batch 1 loss 0.40220901370048523\n",
            "epoch 16 batch 2 loss 0.36965715885162354\n",
            "epoch 16 batch 3 loss 0.390012264251709\n",
            "epoch 16 batch 4 loss 0.4046202600002289\n",
            "epoch 16 batch 5 loss 0.3914814889431\n",
            "epoch 16 batch 6 loss 0.38780298829078674\n",
            "epoch 16 batch 7 loss 0.3858637511730194\n",
            "epoch 16 batch 8 loss 0.39480966329574585\n",
            "epoch 16 batch 9 loss 0.3464219272136688\n",
            "epoch 16 batch 10 loss 0.3835757374763489\n",
            "epoch 17 batch 0 loss 0.41252416372299194\n",
            "epoch 17 batch 1 loss 0.38262006640434265\n",
            "epoch 17 batch 2 loss 0.351736843585968\n",
            "epoch 17 batch 3 loss 0.37154749035835266\n",
            "epoch 17 batch 4 loss 0.3853740990161896\n",
            "epoch 17 batch 5 loss 0.3731294870376587\n",
            "epoch 17 batch 6 loss 0.3698110282421112\n",
            "epoch 17 batch 7 loss 0.36797863245010376\n",
            "epoch 17 batch 8 loss 0.37707632780075073\n",
            "epoch 17 batch 9 loss 0.33053115010261536\n",
            "epoch 17 batch 10 loss 0.3660930395126343\n",
            "epoch 18 batch 0 loss 0.39421820640563965\n",
            "epoch 18 batch 1 loss 0.36542433500289917\n",
            "epoch 18 batch 2 loss 0.335942804813385\n",
            "epoch 18 batch 3 loss 0.3551584482192993\n",
            "epoch 18 batch 4 loss 0.3685530126094818\n",
            "epoch 18 batch 5 loss 0.35677245259284973\n",
            "epoch 18 batch 6 loss 0.35403677821159363\n",
            "epoch 18 batch 7 loss 0.3524273633956909\n",
            "epoch 18 batch 8 loss 0.3611169159412384\n",
            "epoch 18 batch 9 loss 0.316898375749588\n",
            "epoch 18 batch 10 loss 0.35065922141075134\n",
            "epoch 19 batch 0 loss 0.37823235988616943\n",
            "epoch 19 batch 1 loss 0.3504762649536133\n",
            "epoch 19 batch 2 loss 0.3222566545009613\n",
            "epoch 19 batch 3 loss 0.34096500277519226\n",
            "epoch 19 batch 4 loss 0.3535955548286438\n",
            "epoch 19 batch 5 loss 0.3428642451763153\n",
            "epoch 19 batch 6 loss 0.34054744243621826\n",
            "epoch 19 batch 7 loss 0.33862513303756714\n",
            "epoch 19 batch 8 loss 0.34761136770248413\n",
            "epoch 19 batch 9 loss 0.30437761545181274\n",
            "epoch 19 batch 10 loss 0.33743298053741455\n",
            "-------------------start evaluating-------------------\n"
          ]
        }
      ]
    }
  ]
}