{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DV7fmwYwVX_",
        "outputId": "d331f997-1157-42d0-dff4-2d1fa59a01c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "u2nZg8-K24xf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import re\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "# import stanfordnlp\n",
        "\n",
        "# Download the Arabic models for the neural pipeline\n",
        "# stanfordnlp.download('ar', force=True)\n",
        "# Build a neural pipeline using the Arabic models\n",
        "# nlp = stanfordnlp.Pipeline(lang='ar')\n",
        "\n",
        "# def split_arabic_sentences_with_stanfordnlp(corpus_text):\n",
        "#     # Process the text\n",
        "#     doc = nlp(corpus_text)\n",
        "\n",
        "#     # Extract sentences from the doc\n",
        "#     sentences = [sentence.text for sentence in doc.sentences]\n",
        "\n",
        "#     return sentences\n",
        "\n",
        "max_len=600\n",
        "\n",
        "with open('/content/drive/MyDrive/NLP project/Arabic-text-to-diacritics/files/arabic_letters.pickle', 'rb') as file:\n",
        "            ARABIC_LETTERS_LIST = pkl.load(file)\n",
        "with open('/content/drive/MyDrive/NLP project/Arabic-text-to-diacritics/files/diacritics.pickle', 'rb') as file:\n",
        "            DIACRITICS_LIST = pkl.load(file)\n",
        "\n",
        "arabic_letters=[]\n",
        "for letter in ARABIC_LETTERS_LIST:\n",
        "    arabic_letters.append(letter[0])\n",
        "arabic_letters.append(\" \")\n",
        "\n",
        "dicritics=[]\n",
        "for letter in DIACRITICS_LIST:\n",
        "    dicritics.append(letter[0])\n",
        "\n",
        "classes = {\n",
        "    'َ': 0,\n",
        "    'ُ': 1,\n",
        "    'ِ': 2,\n",
        "    'ْ': 3,\n",
        "    'ّ': 4,\n",
        "    'ً': 5,\n",
        "    'ٌ': 6,\n",
        "    'ٍ': 7,\n",
        "    'َّ': 8,\n",
        "    'ُّ': 9,\n",
        "    'ِّ': 10,\n",
        "    'ًّ': 11,\n",
        "    'ٌّ': 12,\n",
        "    'ٍّ': 13,\n",
        "    'ّّ': 14,\n",
        "    \"\":15,\n",
        "}\n",
        "\n",
        "inverted_classes = {v: k for k, v in classes.items()}\n",
        "\n",
        "##################### to delete #####################\n",
        "def read_text(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        return file.read()\n",
        "\n",
        "def write_to_file_second(dirctory,file_path, text):\n",
        "    if not os.path.exists(dirctory):\n",
        "        os.makedirs(dirctory)\n",
        "\n",
        "    with open(dirctory+\"/\"+file_path, \"a\", encoding=\"utf-8\") as file:\n",
        "        lines=text\n",
        "        for l in lines:\n",
        "            file.write(l)\n",
        "        file.write('\\n')\n",
        "def write_to_file_labels(dirctory,file_path, text):\n",
        "    if not os.path.exists(dirctory):\n",
        "        os.makedirs(dirctory)\n",
        "\n",
        "    with open(dirctory+\"/\"+file_path, \"a\", encoding=\"utf-8\") as file:\n",
        "        lines=text\n",
        "        for l in lines:\n",
        "            file.write(inverted_classes[l])\n",
        "        file.write('\\n')\n",
        "\n",
        "def write_to_file_string(dirctory,file_path, text):\n",
        "    if not os.path.exists(dirctory):\n",
        "        os.makedirs(dirctory)\n",
        "\n",
        "    with open(dirctory+\"/\"+file_path, \"a\", encoding=\"utf-8\") as file:\n",
        "        lines=text\n",
        "        file.write(lines)\n",
        "        file.write('\\n')\n",
        "#####################################################\n",
        "\n",
        "def preprocess(text):\n",
        "         # Remove URLs\n",
        "        text = re.sub(r\"http[s|S]\\S+\", \"\", text,flags=re.MULTILINE)\n",
        "        text = re.sub(r\"www\\S+\", \"\", text,flags=re.MULTILINE)\n",
        "        # Remove English letters\n",
        "        text = re.sub(r\"[A-Za-z]+\", \"\", text,flags=re.MULTILINE)\n",
        "        # Remove Kashida Arabic character\n",
        "        text = re.sub(r\"\\u0640\", \"\", text,flags=re.MULTILINE)\n",
        "        # Add space before and after the numbers\n",
        "        text = re.sub(r\"(\\d+)\", r\" \\1 \", text,flags=re.MULTILINE)\n",
        "        # removes SHIFT+J Arabic character\n",
        "        text = re.sub(r\"\\u0691\", \"\", text,flags=re.MULTILINE)\n",
        "        # remove english numbers\n",
        "        text = re.sub(r\"[0-9]+\", \"\", text,flags=re.MULTILINE)\n",
        "        # remove arabic numbers\n",
        "        text = re.sub(r\"[٠-٩]+\", \"\", text,flags=re.MULTILINE)\n",
        "         # remove brackets\n",
        "        # text = re.sub(r\"\\[.*?\\]\", \"\", text)\n",
        "        # text = re.sub(r\"\\(.*?\\)\", \"\", text)\n",
        "        return text\n",
        "\n",
        "def split_text(text):\n",
        "    text=text.split('.')\n",
        "    # split text to sentences on all arabic sparatators\n",
        "\n",
        "    data=[]\n",
        "    for t in text:\n",
        "        if(len(t)==0): continue\n",
        "        if(len(t)<max_len):\n",
        "            while(len(t)<max_len):\n",
        "                 t+=\" \"\n",
        "            data.append(t)\n",
        "        if(len(t)>max_len):\n",
        "            data.append(t[:max_len])\n",
        "            supdata=t[max_len:]\n",
        "            while(len(supdata)>max_len):\n",
        "                data.append(supdata[:max_len])\n",
        "                supdata=supdata[max_len:]\n",
        "            if(len(supdata)<max_len):\n",
        "                while(len(supdata)<max_len):\n",
        "                    supdata+=\" \"\n",
        "                data.append(supdata)\n",
        "    return data\n",
        "\n",
        "def get_data_labels(text):\n",
        "    data=\"\"\n",
        "    labels=[]\n",
        "    for i in range(len(text)):\n",
        "            if(text[i] in arabic_letters and text[i]):\n",
        "                data+=(text[i])\n",
        "                if(i+1<len(text) and text[i+1] in dicritics):\n",
        "                    if(i+2<len(text) and classes[text[i+1]]==4 and text[i+2] in dicritics):\n",
        "                        labels.append(classes[text[i+1]+text[i+2]])\n",
        "                        i+=2\n",
        "                    else:\n",
        "                        labels.append(classes[text[i+1]])\n",
        "                        i+=1\n",
        "                else:\n",
        "                    labels.append(15)\n",
        "    return data,labels\n",
        "\n",
        "# def one_hot_encoding(text):\n",
        "#     onehot_encoded=[]\n",
        "#     for i in range(len(text)):\n",
        "#          if text[i] in arabic_letters:\n",
        "#             idx=arabic_letters.index(text[i])\n",
        "#             encode=np.zeros(len(arabic_letters))\n",
        "#             encode[idx]=1\n",
        "#             onehot_encoded.append(encode)\n",
        "#     onehot_encoded=torch.tensor(onehot_encoded)\n",
        "#     return onehot_encoded\n",
        "\n",
        "def encoding(text):\n",
        "    idx=arabic_letters.index(text)\n",
        "    encode=np.zeros(len(arabic_letters))\n",
        "    encode[idx]=1\n",
        "    return torch.tensor(encode,dtype=torch.float32).to(device)\n",
        "\n",
        "\n",
        "def get_dataloader(encoded_data, encoding_labels,batch_size=1):\n",
        "    # Create TensorDataset\n",
        "    dataset = TensorDataset(encoded_data, encoding_labels)\n",
        "    # Create DataLoader\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "def get_validation():\n",
        "    text=read_text('/content/drive/MyDrive/NLP project/Arabic-text-to-diacritics/Dataset/val.txt')\n",
        "    text=preprocess(text)\n",
        "    # size=int(0.02*len(text))\n",
        "    # text=text[:size]\n",
        "    # text=split_text(text)\n",
        "    # write_to_file_second(\"test\",\"data.txt\",text)\n",
        "    # text=split_text(text)\n",
        "    data=[]\n",
        "    labels=[]\n",
        "    for i in range (len(text)):\n",
        "        if(text[i] in arabic_letters):\n",
        "                data+=(text[i])\n",
        "                if(i+1<len(text) and text[i+1] in dicritics):\n",
        "                    if(i+2<len(text) and classes[text[i+1]]==4 and text[i+2] in dicritics):\n",
        "                        labels.append(classes[text[i+1]+text[i+2]])\n",
        "                        i+=2\n",
        "                    else:\n",
        "                        labels.append(classes[text[i+1]])\n",
        "                        i+=1\n",
        "                else:\n",
        "                    labels.append(15)\n",
        "        # else:\n",
        "        #     data.append(text[i])\n",
        "        #     labels.append(15)\n",
        "    encoded_data = torch.empty(0, len(arabic_letters),dtype=torch.float32).to(device)\n",
        "\n",
        "    for letter in data:\n",
        "        if letter in arabic_letters:\n",
        "            x = encoding(letter).unsqueeze(0)\n",
        "        else:\n",
        "            x=np.zeros((1,len(arabic_letters)))\n",
        "            x=torch.tensor(x,dtype=torch.float32).to(device)\n",
        "        encoded_data = torch.cat((encoded_data, x), 0)\n",
        "    labels=torch.tensor(labels,dtype=torch.long).to(device)\n",
        "    # print(encoded_data.shape)\n",
        "    # print(labels.shape)\n",
        "    dataloader=get_dataloader(encoded_data,labels)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "def get_data(path):\n",
        "    text=read_text(path)\n",
        "    text=preprocess(text)\n",
        "    # size=int(0.02*len(text))\n",
        "    # text=text[:size]\n",
        "    print(len(text))\n",
        "\n",
        "    text=split_text(text)\n",
        "\n",
        "    # write_to_file_second(\"test\",\"data.txt\",text)\n",
        "    # # get max length of sentence in text\n",
        "    # maxdata=text.split('\\n')\n",
        "\n",
        "    # max_len=0\n",
        "    # for t in maxdata:\n",
        "    #     if(len(t)>max_len):\n",
        "    #         max_len=len(t)\n",
        "    # print(max_len)\n",
        "\n",
        "    # split text to sentences on all arabic sparatators\n",
        "    # text = split_arabic_sentences_with_stanfordnlp(text)\n",
        "\n",
        "    # Filter out empty strings or whitespace-only sentences\n",
        "    # text = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "    data=[]\n",
        "    labels=[]\n",
        "    for t in text:\n",
        "        d=\"\"\n",
        "        l=[]\n",
        "        d,l=get_data_labels(t)\n",
        "        if(len(d)<max_len):\n",
        "            while(len(d)<max_len):\n",
        "                d+=(\" \")\n",
        "                l.append(15)\n",
        "        else:\n",
        "            d=d[:max_len]\n",
        "            l=l[:max_len]\n",
        "        data.append(d)\n",
        "        labels.append(l)\n",
        "    return data,labels\n",
        "\n",
        "def get_features(data,labels):\n",
        "    encoded_data = torch.empty(0, max_len, len(arabic_letters),dtype=torch.float32).to(device)\n",
        "    for d in data:\n",
        "        enc = torch.empty(0, len(arabic_letters),dtype=torch.float32).to(device)\n",
        "        for letter in d:\n",
        "            x = encoding(letter).unsqueeze(0)\n",
        "            enc = torch.cat((enc, x), 0)\n",
        "        encoded_data = torch.cat((encoded_data, enc.unsqueeze(0)), 0)\n",
        "    # print(encoded_data.shape)\n",
        "    encoding_labels=torch.tensor(labels,dtype=torch.long).to(device)\n",
        "    # print(encoding_labels.shape)\n",
        "    return encoded_data,encoding_labels\n",
        "\n",
        "class DataSet():\n",
        "\n",
        "    def __init__(self,path,batch_size=1) :\n",
        "        print(\"Loading data...\")\n",
        "        data,labels=get_data(path)\n",
        "        # now labels is list of list [[1,2,3,4,5,15,15,0],[1,2,3,4,5,15,15,0]]\n",
        "        # data is list of string ['احمد','محمد']\n",
        "        print(\"Extracting features...\")\n",
        "        data,labels=get_features(data,labels)\n",
        "        # now the data and labels are tensor\n",
        "        # data is tensor of shape (number of sentences,600,37)\n",
        "        # labels is tensor of shape (number of sentences,600)\n",
        "        print(\"Creating dataloader...\")\n",
        "        dataloader=get_dataloader(data,labels,batch_size)\n",
        "        self.x=data\n",
        "        self.y=labels\n",
        "        self.dataloader=dataloader\n",
        "        print(\"Done data creation !\")\n",
        "\n",
        "    def __len__(self):\n",
        "         return len(self.y)\n",
        "    def item(self,idx):\n",
        "         return encoding(self.x[idx]),self.y[idx]\n",
        "    def getdata(self):\n",
        "        return self.dataloader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "C5LvCIPR2eVx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, inp_vocab_size: int, hidden_dim: int = 256, seq_len: int = 600, num_classes: int = 16):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(inp_vocab_size, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # Output layer for 0 to 16 integers\n",
        "\n",
        "    def forward(self, input_sequence: torch.Tensor):\n",
        "        output, _ = self.lstm(input_sequence)\n",
        "        output = self.fc(output.reshape(-1, output.size(-1)))\n",
        "        return output\n",
        "\n",
        "def train(train_dl, model):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    for epoch in range(20):\n",
        "        for i, (inputs, targets) in enumerate(train_dl):\n",
        "            optimizer.zero_grad()\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            yhat = model(inputs)\n",
        "            yhat = yhat.view(inputs.size(0), inputs.size(1), -1)\n",
        "            yhat = yhat.view(-1, yhat.size(2))\n",
        "            targets = targets.view(-1)\n",
        "            loss = criterion(yhat, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            print(f'epoch {epoch} batch {i} loss {loss.item()}')\n",
        "    torch.save(model.state_dict(), 'model.pth')\n",
        "\n",
        "def calculate_DER(actual_labels, predicted_labels):\n",
        "    if not isinstance(actual_labels, torch.Tensor):\n",
        "        actual_labels = torch.tensor(actual_labels)\n",
        "    if not isinstance(predicted_labels, torch.Tensor):\n",
        "        predicted_labels = torch.tensor(predicted_labels)\n",
        "\n",
        "    if len(actual_labels) != len(predicted_labels):\n",
        "        raise ValueError(\"Lengths of actual and predicted labels should match.\")\n",
        "\n",
        "    total_errors = torch.sum(actual_labels != predicted_labels)\n",
        "    total_frames = len(actual_labels)\n",
        "    DER = (1-(total_errors / total_frames)) * 100.0\n",
        "    return DER.item()\n",
        "\n",
        "def evaluate_model(test_dl, model):\n",
        "    predictions, actuals = [], []\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    for i, (inputs, targets) in enumerate(test_dl):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        yhat = model(inputs)\n",
        "        yhat = yhat.detach().cpu().numpy()\n",
        "        yhat = yhat.reshape(-1, yhat.shape[-1])\n",
        "        predicted_classes = np.argmax(yhat, axis=1)\n",
        "        targets = targets.cpu().numpy().reshape(-1)\n",
        "        predictions.extend(predicted_classes.tolist())\n",
        "        actuals.extend(targets.tolist())\n",
        "    acc = calculate_DER(np.array(actuals), np.array(predictions))\n",
        "    return acc\n"
      ],
      "metadata": {
        "id": "qhy617nk2VPI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from data import DataSet, get_validation\n",
        "# from model import LSTM, train, evaluate_model\n",
        "\n",
        "Traindata = DataSet(\"/content/drive/MyDrive/NLP project/Arabic-text-to-diacritics/Dataset/val.txt\", batch_size=256)\n",
        "Traindataloader = Traindata.getdata()\n",
        "Validationdataloader=get_validation()\n",
        "\n",
        "inp_vocab_size = 37\n",
        "hidden_dim = 256\n",
        "seq_len = 600\n",
        "num_classes = 16\n",
        "\n",
        "model = LSTM(inp_vocab_size, hidden_dim, seq_len, num_classes).to(device)\n",
        "print(\"-------------------start training-------------------\")\n",
        "train(Traindataloader, model)\n",
        "\n",
        "# Validation=DataSet('Dataset/val.txt',batch_size=1)\n",
        "# Validationdataloader=Validation.getdata()\n",
        "print(\"-------------------start evaluating-------------------\")\n",
        "acc = evaluate_model(Validationdataloader, model)\n",
        "print(\"Accuracy: \", acc)\n",
        "\n",
        "# validation=get_validation()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqgXGly72RLu",
        "outputId": "1023343e-342e-4c83-d175-b051c525cd35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "933760\n",
            "Extracting features...\n",
            "Creating dataloader...\n",
            "Done data creation !\n"
          ]
        }
      ]
    }
  ]
}